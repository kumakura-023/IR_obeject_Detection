# unified_training_corrected.py - æœ€æ–°ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹æˆã«å¯¾å¿œ

import os
import gc
import math
import time
import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from typing import Dict, Optional, Tuple, List
import numpy as np
import torch.nn.functional as F
from collections import defaultdict

# PyTorch AMPã®äº’æ›æ€§è¨­å®š
if torch.__version__ >= '2.0':
    from torch.amp import autocast, GradScaler
    AMP_DEVICE = 'cuda'
else:
    from torch.cuda.amp import autocast, GradScaler
    AMP_DEVICE = None

# ===== ä¿®æ­£: æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«åã§import =====
from dataset import YoloInfraredDataset
from efficientnet_model import create_efficientnet_model
# â˜…â˜…â˜… ä¿®æ­£: `build_targets` ã®ã¿ã‚’ import ã™ã‚‹ã‚ˆã†ã«å¤‰æ›´ â˜…â˜…â˜…
from unified_targets import (
    build_targets,
    get_default_anchors,
    analyze_dataset_statistics,
    compare_anchor_sets,
    prepare_anchor_grid_info
)
from unified_loss import create_enhanced_loss


import datetime
import hashlib

class VersionTracker:
    """ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨ä¿®æ­£å±¥æ­´ã‚’è¿½è·¡"""
    
    def __init__(self, script_name, version="1.0.0"):
        self.script_name = script_name
        self.version = version
        self.load_time = datetime.datetime.now()
        self.modifications = []
        
    def add_modification(self, description, author="AI Assistant"):
        """ä¿®æ­£å±¥æ­´ã‚’è¿½åŠ """
        timestamp = datetime.datetime.now()
        self.modifications.append({
            'timestamp': timestamp,
            'description': description,
            'author': author
        })
        
    def get_file_hash(self, filepath):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—ï¼ˆå¤‰æ›´æ¤œå‡ºç”¨ï¼‰"""
        try:
            with open(filepath, 'rb') as f:
                content = f.read()
                return hashlib.md5(content).hexdigest()[:8]
        except:
            return "unknown"
    
    def print_version_info(self):
        """ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ã‚’è¡¨ç¤º"""
        print(f"\n{'='*60}")
        print(f"ğŸ“‹ {self.script_name} - Version {self.version}")
        print(f"â° Loaded: {self.load_time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        if hasattr(self, 'file_hash'):
            print(f"ğŸ”— File Hash: {self.file_hash}")
        
        if self.modifications:
            print(f"ğŸ“ Recent Modifications ({len(self.modifications)}):")
            for i, mod in enumerate(self.modifications[-3:], 1):  # æœ€æ–°3ä»¶
                print(f"   {i}. {mod['timestamp'].strftime('%H:%M:%S')} - {mod['description']}")
        
        print(f"{'='*60}\n")

# å„ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãƒˆãƒ©ãƒƒã‚«ãƒ¼ã‚’ä½œæˆ
def create_version_tracker(script_name, filepath=None):
    """ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãƒˆãƒ©ãƒƒã‚«ãƒ¼ã‚’ä½œæˆ"""
    tracker = VersionTracker(script_name)
    
    if filepath:
        tracker.file_hash = tracker.get_file_hash(filepath)
    
    return tracker

# ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
training_version = create_version_tracker("Unified Training System v2.6", "unified_training.py")
training_version.add_modification("ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’20â†’8ã«å‰Šæ¸›ï¼ˆãƒ¡ãƒ¢ãƒªä¸è¶³å¯¾ç­–ï¼‰")
training_version.add_modification("ç©æ¥µçš„ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—æ©Ÿèƒ½è¿½åŠ ")
training_version.add_modification("float32çµ±ä¸€ã§AMPå®Œå…¨ç„¡åŠ¹åŒ–")
training_version.add_modification("OOMã‚¨ãƒ©ãƒ¼è‡ªå‹•å›å¾©æ©Ÿèƒ½è¿½åŠ ")


# ===== ãƒ‡ãƒãƒƒã‚°ã‚·ã‚¹ãƒ†ãƒ  =====
class LossDebugTracker:
    """ãƒ­ã‚¹è¨ˆç®—ã®å…¨å·¥ç¨‹ã‚’è¿½è·¡ã™ã‚‹ãƒ‡ãƒãƒƒã‚°ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.step_history = []
        self.focal_loss_history = []
        self.target_history = []
        self.prediction_history = []
        self.weight_history = []
        
    def track_step(self, step, batch_idx, epoch, preds, targets, loss_dict, loss_fn):
        """å„ã‚¹ãƒ†ãƒƒãƒ—ã®è©³ç´°æƒ…å ±ã‚’è¨˜éŒ²"""
        
        debug_info = {
            'step': step,
            'batch_idx': batch_idx,
            'epoch': epoch,
            'timestamp': time.time()
        }
        
        # 1. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†æ
        target_obj = targets['objectness']
        debug_info['target_analysis'] = {
            'total_targets': target_obj.numel(),
            'positive_001': (target_obj > 0.001).sum().item(),
            'positive_01': (target_obj > 0.01).sum().item(),
            'positive_1': (target_obj > 0.1).sum().item(),
            'positive_5': (target_obj > 0.5).sum().item(),
            'max_objectness': target_obj.max().item(),
            'mean_positive': target_obj[target_obj > 0.01].mean().item() if (target_obj > 0.01).any() else 0.0,
            'min_positive': target_obj[target_obj > 0.01].min().item() if (target_obj > 0.01).any() else 0.0
        }
        
        # 2. äºˆæ¸¬åˆ†æ
        pred_obj = preds[..., 4]
        pred_obj_sigmoid = torch.sigmoid(pred_obj)
        debug_info['prediction_analysis'] = {
            'pred_obj_logits_mean': pred_obj.mean().item(),
            'pred_obj_logits_std': pred_obj.std().item(),
            'pred_obj_sigmoid_mean': pred_obj_sigmoid.mean().item(),
            'pred_obj_sigmoid_std': pred_obj_sigmoid.std().item(),
            'pred_obj_sigmoid_max': pred_obj_sigmoid.max().item(),
            'pred_obj_sigmoid_min': pred_obj_sigmoid.min().item()
        }
        
        # 3. ãƒ­ã‚¹é–¢æ•°å†…éƒ¨çŠ¶æ…‹ã®è¿½è·¡
        if hasattr(loss_fn, 'obj_loss_fn') and hasattr(loss_fn.obj_loss_fn, 'gamma'):
            debug_info['focal_loss_state'] = {
                'gamma': getattr(loss_fn.obj_loss_fn, 'gamma', 'N/A'),
                'alpha': getattr(loss_fn.obj_loss_fn, 'alpha', 'N/A'),
                'step_count': getattr(loss_fn.obj_loss_fn, 'step_count', 'N/A'),
                'loss_history_length': len(getattr(loss_fn.obj_loss_fn, 'loss_history', []))
            }
        
        # 4. å‹•çš„é‡ã¿
        if 'dynamic_weights' in loss_dict:
            debug_info['dynamic_weights'] = loss_dict['dynamic_weights']
        
        # 5. ãƒ­ã‚¹å€¤
        debug_info['loss_values'] = {
            'total': loss_dict['total'].item(),
            'box': loss_dict['box'].item(),
            'obj': loss_dict['obj'].item(),
            'cls': loss_dict['cls'].item(),
            'pos_samples': loss_dict['pos_samples']
        }
        
        # 6. æ‰‹å‹•ãƒ­ã‚¹è¨ˆç®—ï¼ˆæ¤œè¨¼ç”¨ï¼‰
        debug_info['manual_loss_check'] = self._manual_loss_calculation(preds, targets, loss_fn)
        
        self.step_history.append(debug_info)
        
        # ç•°å¸¸æ¤œå‡º
        if step > 5:  # æœ€åˆã®æ•°ã‚¹ãƒ†ãƒƒãƒ—ã¯ã‚¹ã‚­ãƒƒãƒ—
            self._detect_anomalies(debug_info, step)
    
    def _manual_loss_calculation(self, preds, targets, loss_fn):
        """æ‰‹å‹•ã§ãƒ­ã‚¹è¨ˆç®—ã‚’å®Ÿè¡Œã—ã¦æ¤œè¨¼"""
        try:
            device = preds.device
            pred_obj = preds[..., 4]
            target_obj = targets['objectness']
            
            # AdaptiveFocalLossã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
            if hasattr(loss_fn, 'obj_loss_fn'):
                obj_loss_fn = loss_fn.obj_loss_fn
                alpha = getattr(obj_loss_fn, 'alpha', 0.75)
                gamma = getattr(obj_loss_fn, 'gamma', 1.5)
            else:
                alpha, gamma = 0.75, 1.5
            
            # ãƒã‚¤ãƒŠãƒªã‚¿ãƒ¼ã‚²ãƒƒãƒˆä½œæˆï¼ˆloss_fn ã®å‡¦ç†ã‚’æ¨¡å€£ï¼‰
            obj_target_for_loss = target_obj.clone()
            obj_target_for_loss = torch.where(
                obj_target_for_loss > 0.01,
                obj_target_for_loss,
                torch.zeros_like(obj_target_for_loss)
            )
            obj_target_binary = (obj_target_for_loss > 0.3).float()
            
            # æ‰‹å‹•BCEè¨ˆç®—
            bce_loss = F.binary_cross_entropy_with_logits(pred_obj, obj_target_binary, reduction='none')
            
            # æ‰‹å‹•Focal Lossè¨ˆç®—
            probs = torch.sigmoid(pred_obj)
            pt = torch.where(obj_target_binary == 1, probs, 1 - probs)
            alpha_t = torch.where(obj_target_binary == 1, alpha, 1 - alpha)
            focal_loss = alpha_t * (1 - pt) ** gamma * bce_loss
            
            return {
                'binary_targets_sum': obj_target_binary.sum().item(),
                'manual_bce_mean': bce_loss.mean().item(),
                'manual_focal_mean': focal_loss.mean().item(),
                'pt_mean': pt.mean().item(),
                'alpha_t_mean': alpha_t.mean().item(),
                'gamma_used': gamma,
                'alpha_used': alpha
            }
        except Exception as e:
            return {'error': str(e)}
    
    def _detect_anomalies(self, current_info, step):
        """ç•°å¸¸å€¤ã‚’æ¤œå‡ºã—ã¦ã‚¢ãƒ©ãƒ¼ãƒˆ"""
        current_obj_loss = current_info['loss_values']['obj']
        
        # éå»5ã‚¹ãƒ†ãƒƒãƒ—ã®å¹³å‡ã¨æ¯”è¼ƒ
        if len(self.step_history) >= 5:
            recent_obj_losses = [info['loss_values']['obj'] for info in self.step_history[-5:]]
            avg_recent = sum(recent_obj_losses) / len(recent_obj_losses)
            
            # æ€¥æ¿€ãªæ¸›å°‘ã‚’æ¤œå‡º
            if current_obj_loss < avg_recent * 0.1 and current_obj_loss < 0.01:
                print(f"\nğŸš¨ ANOMALY DETECTED at Step {step}!")
                print(f"   Obj Loss dropped from {avg_recent:.6f} to {current_obj_loss:.6f}")
                print(f"   Investigating cause...")
                
                self._investigate_cause(current_info, step)
    
    def _investigate_cause(self, current_info, step):
        """ç•°å¸¸ã®åŸå› ã‚’èª¿æŸ»"""
        print(f"\nğŸ” INVESTIGATING ANOMALY at Step {step}:")
        
        # 1. Focal Loss ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒã‚§ãƒƒã‚¯
        if 'focal_loss_state' in current_info:
            focal_state = current_info['focal_loss_state']
            print(f"   Focal Loss State:")
            print(f"     Gamma: {focal_state.get('gamma', 'N/A')}")
            print(f"     Alpha: {focal_state.get('alpha', 'N/A')}")
            print(f"     Step Count: {focal_state.get('step_count', 'N/A')}")
        
        # 2. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰åŒ–ãƒã‚§ãƒƒã‚¯
        target_analysis = current_info['target_analysis']
        print(f"   Target Analysis:")
        print(f"     Positive targets (>0.01): {target_analysis['positive_01']}")
        print(f"     Mean positive objectness: {target_analysis['mean_positive']:.6f}")
        
        # 3. æ‰‹å‹•è¨ˆç®—ã¨ã®æ¯”è¼ƒ
        if 'manual_loss_check' in current_info:
            manual = current_info['manual_loss_check']
            if 'error' not in manual:
                actual_obj_loss = current_info['loss_values']['obj']
                manual_focal_loss = manual['manual_focal_mean']
                print(f"   Manual vs Actual Loss:")
                print(f"     Manual Focal: {manual_focal_loss:.6f}")
                print(f"     Actual Obj: {actual_obj_loss:.6f}")
                print(f"     Difference: {abs(manual_focal_loss - actual_obj_loss):.6f}")
                print(f"     Binary targets: {manual['binary_targets_sum']}")
                print(f"     Gamma used: {manual['gamma_used']}")
        
        # 4. å‹•çš„é‡ã¿ãƒã‚§ãƒƒã‚¯
        if 'dynamic_weights' in current_info:
            weights = current_info['dynamic_weights']
            print(f"   Dynamic Weights:")
            print(f"     Box: {weights['box']:.2f}, Obj: {weights['obj']:.2f}, Cls: {weights['cls']:.2f}")
    
    def save_debug_log(self, filename="loss_debug_log.json"):
        """ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ã‚’ä¿å­˜"""
        try:
            # NumPy/Tensorã‚’é€šå¸¸ã®Pythonå‹ã«å¤‰æ›
            json_safe_log = []
            for entry in self.step_history:
                safe_entry = {}
                for key, value in entry.items():
                    if isinstance(value, dict):
                        safe_entry[key] = {k: float(v) if isinstance(v, (torch.Tensor, float)) else v 
                                         for k, v in value.items()}
                    else:
                        safe_entry[key] = float(value) if isinstance(value, (torch.Tensor, float)) else value
                json_safe_log.append(safe_entry)
            
            with open(filename, 'w') as f:
                json.dump(json_safe_log, f, indent=2)
            print(f"ğŸ“ Debug log saved to {filename}")
        except Exception as e:
            print(f"âš ï¸ Failed to save debug log: {e}")
    
    def print_summary(self):
        """ãƒ‡ãƒãƒƒã‚°ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
        if len(self.step_history) == 0:
            return
        
        print(f"\nğŸ“Š LOSS DEBUG SUMMARY ({len(self.step_history)} steps tracked)")
        
        # Obj Lossã®æ¨ç§»
        obj_losses = [info['loss_values']['obj'] for info in self.step_history]
        print(f"   Obj Loss: {obj_losses[0]:.6f} â†’ {obj_losses[-1]:.6f}")
        
        # æœ€å¤§å€¤ã¨æœ€å°å€¤
        max_obj = max(obj_losses)
        min_obj = min(obj_losses)
        print(f"   Range: {min_obj:.6f} - {max_obj:.6f}")
        
        # æ€¥æ¿€ãªå¤‰åŒ–ã‚’æ¤œå‡º
        for i in range(1, len(obj_losses)):
            if obj_losses[i] < obj_losses[i-1] * 0.1 and obj_losses[i] < 0.01:
                print(f"   ğŸš¨ Sharp drop detected at step {self.step_history[i]['step']}")
                break

def check_for_nan_gradients(model, step):
    """ãƒ¢ãƒ‡ãƒ«å…¨ä½“ã®å‹¾é…ã«NaNãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ã™ã‚‹"""
    nan_found = False
    for name, param in model.named_parameters():
        if param.grad is not None and torch.isnan(param.grad).any():
            print(f"ğŸš¨ğŸš¨ğŸš¨ NaN GRADIENT DETECTED IN ==> {name} at step {step}")
            nan_found = True
    if nan_found:
        # NaNãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã€ãƒ‡ãƒãƒƒã‚¬ã‚’èµ·å‹•ã™ã‚‹ï¼ˆColabã§æœ‰åŠ¹ï¼‰
        import pdb; pdb.set_trace()
    return nan_found

class RootCauseInvestigator:
    """äºˆæ¸¬å€¤å´©å£Šã®æ ¹æœ¬åŸå› ã‚’èª¿æŸ»"""
    
    def __init__(self):
        self.weight_history = []
        self.gradient_history = []
        self.activation_history = []
        self.loss_component_history = []
        
    def investigate_prediction_collapse(self, model, preds, targets, loss_dict, 
                                      optimizer, step, batch_idx):
        """äºˆæ¸¬å€¤å´©å£Šã®æ ¹æœ¬åŸå› ã‚’èª¿æŸ»"""
        
        print(f"\nğŸ”¬ [ROOT CAUSE INVESTIGATION] Step {step}, Batch {batch_idx}")
        
        # 1. ãƒ¢ãƒ‡ãƒ«é‡ã¿ã®ç•°å¸¸èª¿æŸ»
        self._investigate_model_weights(model, step)
        
        # 2. å‹¾é…ã®ç•°å¸¸èª¿æŸ»
        self._investigate_gradients(model, step)
        
        # 3. ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³èª¿æŸ»
        self._investigate_activations(model, preds, step)
        
        # 4. ãƒ­ã‚¹æˆåˆ†ã®å¯„ä¸åº¦èª¿æŸ»
        self._investigate_loss_components(preds, targets, loss_dict, step)
    
    def _investigate_model_weights(self, model, step):
        """ãƒ¢ãƒ‡ãƒ«é‡ã¿ã®ç•°å¸¸ã‚’èª¿æŸ»"""
        print("   ğŸ” Model Weights Analysis:")
        
        # Detection Headã®é‡ã¿ï¼ˆæœ€ã‚‚å½±éŸ¿ãŒå¤§ãã„ï¼‰
        detection_head = model.head.detection_head
        weight_data = detection_head.weight.data
        bias_data = detection_head.bias.data if detection_head.bias is not None else None
        
        # Objectnesså‡ºåŠ›ã«å¯¾å¿œã™ã‚‹é‡ã¿ï¼ˆchannel 4ï¼‰
        # detection_head: [num_anchors * (5+num_classes), in_channels, 1, 1]
        num_anchors = 3
        num_classes = 15
        outputs_per_anchor = 5 + num_classes  # 20
        
        obj_weight_indices = []
        for anchor_idx in range(num_anchors):
            obj_idx = anchor_idx * outputs_per_anchor + 4  # objectnessä½ç½®
            obj_weight_indices.append(obj_idx)
        
        obj_weights = weight_data[obj_weight_indices]  # [3, 256, 1, 1]
        obj_biases = bias_data[obj_weight_indices] if bias_data is not None else None
        
        weight_stats = {
            'obj_weight_mean': obj_weights.mean().item(),
            'obj_weight_std': obj_weights.std().item(),
            'obj_weight_max': obj_weights.max().item(),
            'obj_weight_min': obj_weights.min().item(),
            'obj_bias_mean': obj_biases.mean().item() if obj_biases is not None else 0.0,
            'obj_bias_std': obj_biases.std().item() if obj_biases is not None else 0.0
        }
        
        self.weight_history.append({'step': step, **weight_stats})
        
        print(f"     Obj weights: mean={weight_stats['obj_weight_mean']:.6f}, "
              f"std={weight_stats['obj_weight_std']:.6f}")
        print(f"     Obj biases: mean={weight_stats['obj_bias_mean']:.6f}, "
              f"std={weight_stats['obj_bias_std']:.6f}")
        
        # ç•°å¸¸æ¤œå‡º
        if abs(weight_stats['obj_weight_mean']) > 10.0:
            print(f"     ğŸš¨ WEIGHT EXPLOSION: obj_weight_mean = {weight_stats['obj_weight_mean']:.6f}")
        elif abs(weight_stats['obj_weight_mean']) < 1e-6:
            print(f"     ğŸš¨ WEIGHT VANISHING: obj_weight_mean = {weight_stats['obj_weight_mean']:.6f}")
        
        if abs(weight_stats['obj_bias_mean']) > 50.0:
            print(f"     ğŸš¨ BIAS EXPLOSION: obj_bias_mean = {weight_stats['obj_bias_mean']:.6f}")
        elif abs(weight_stats['obj_bias_mean']) < -50.0:
            print(f"     ğŸš¨ LARGE NEGATIVE BIAS: obj_bias_mean = {weight_stats['obj_bias_mean']:.6f}")
            print(f"       â†’ This could cause sigmoid collapse!")
    
    def _investigate_gradients(self, model, step):
        """å‹¾é…ã®ç•°å¸¸ã‚’èª¿æŸ»"""
        print("   ğŸ” Gradient Analysis:")
        
        detection_head = model.head.detection_head
        if detection_head.weight.grad is not None:
            grad_data = detection_head.weight.grad.data
            
            # Objectnesså‹¾é…
            num_anchors = 3
            num_classes = 15
            outputs_per_anchor = 5 + num_classes
            
            obj_grad_indices = []
            for anchor_idx in range(num_anchors):
                obj_idx = anchor_idx * outputs_per_anchor + 4
                obj_grad_indices.append(obj_idx)
            
            obj_grads = grad_data[obj_grad_indices]
            
            grad_stats = {
                'obj_grad_mean': obj_grads.mean().item(),
                'obj_grad_std': obj_grads.std().item(),
                'obj_grad_max': obj_grads.max().item(),
                'obj_grad_min': obj_grads.min().item(),
                'obj_grad_norm': obj_grads.norm().item()
            }
            
            self.gradient_history.append({'step': step, **grad_stats})
            
            print(f"     Obj gradients: mean={grad_stats['obj_grad_mean']:.6f}, "
                  f"norm={grad_stats['obj_grad_norm']:.6f}")
            
            # å‹¾é…ç•°å¸¸æ¤œå‡º
            if grad_stats['obj_grad_norm'] > 100.0:
                print(f"     ğŸš¨ GRADIENT EXPLOSION: norm = {grad_stats['obj_grad_norm']:.6f}")
            elif grad_stats['obj_grad_norm'] < 1e-8:
                print(f"     ğŸš¨ GRADIENT VANISHING: norm = {grad_stats['obj_grad_norm']:.6f}")
        else:
            print("     âš ï¸ No gradients found (may be due to gradient accumulation)")
    
    def _investigate_activations(self, model, preds, step):
        """ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³èª¿æŸ»"""
        print("   ğŸ” Activation Analysis:")
        
        # Objectness logits (sigmoidé©ç”¨å‰)
        obj_logits = preds[..., 4]  # [B, N]
        
        activation_stats = {
            'obj_logits_mean': obj_logits.mean().item(),
            'obj_logits_std': obj_logits.std().item(),
            'obj_logits_max': obj_logits.max().item(),
            'obj_logits_min': obj_logits.min().item(),
            'obj_logits_median': obj_logits.median().item()
        }
        
        self.activation_history.append({'step': step, **activation_stats})
        
        print(f"     Obj logits: mean={activation_stats['obj_logits_mean']:.6f}, "
              f"min={activation_stats['obj_logits_min']:.6f}, "
              f"max={activation_stats['obj_logits_max']:.6f}")
        
        # Sigmoidå´©å£Šã®åˆ¤å®š
        if activation_stats['obj_logits_max'] < -10.0:
            print(f"     ğŸš¨ SIGMOID SATURATION (negative): max_logit = {activation_stats['obj_logits_max']:.6f}")
            print(f"       â†’ sigmoid({activation_stats['obj_logits_max']:.2f}) â‰ˆ {torch.sigmoid(torch.tensor(activation_stats['obj_logits_max'])).item():.8f}")
        elif activation_stats['obj_logits_min'] > 10.0:
            print(f"     ğŸš¨ SIGMOID SATURATION (positive): min_logit = {activation_stats['obj_logits_min']:.6f}")
    
    def _investigate_loss_components(self, preds, targets, loss_dict, step):
        """ãƒ­ã‚¹æˆåˆ†ã®å¯„ä¸åº¦èª¿æŸ»"""
        print("   ğŸ” Loss Component Analysis:")
        
        # å„ãƒ­ã‚¹æˆåˆ†ã®å¤§ãã•
        total_loss = loss_dict['total'].item()
        box_loss = loss_dict['box'].item()
        obj_loss = loss_dict['obj'].item()
        cls_loss = loss_dict['cls'].item()
        
        # é‡ã¿ä»˜ãå¯„ä¸åº¦
        if 'dynamic_weights' in loss_dict:
            weights = loss_dict['dynamic_weights']
            w_box, w_obj, w_cls = weights['box'], weights['obj'], weights['cls']
            
            weighted_box = w_box * box_loss
            weighted_obj = w_obj * obj_loss
            weighted_cls = w_cls * cls_loss
            
            total_weighted = weighted_box + weighted_obj + weighted_cls
            
            if total_weighted > 0:
                box_contribution = weighted_box / total_weighted
                obj_contribution = weighted_obj / total_weighted
                cls_contribution = weighted_cls / total_weighted
                
                print(f"     Loss contributions:")
                print(f"       Box: {box_contribution:.3f} ({weighted_box:.6f})")
                print(f"       Obj: {obj_contribution:.3f} ({weighted_obj:.6f})")
                print(f"       Cls: {cls_contribution:.3f} ({weighted_cls:.6f})")
                
                # ç•°å¸¸å¯„ä¸åº¦æ¤œå‡º
                if obj_contribution > 0.8:
                    print(f"     ğŸš¨ OBJ LOSS DOMINANCE: {obj_contribution:.3f}")
                    print(f"       â†’ Objectness loss is overwhelming other components")
                elif obj_contribution < 0.05:
                    print(f"     ğŸš¨ OBJ LOSS TOO WEAK: {obj_contribution:.3f}")


# ===== ä¿®æ­£ã•ã‚ŒãŸãƒ‡ãƒãƒƒã‚°é–¢æ•° =====
def debug_loss_calculation_comprehensive(preds, targets, loss_fn, model, optimizer, 
                                       batch_idx, epoch, tracker: LossDebugTracker, 
                                       step_count, root_investigator: RootCauseInvestigator):
    """åŒ…æ‹¬çš„ãƒ­ã‚¹è¨ˆç®—ãƒ‡ãƒãƒƒã‚° - ä¿®æ­£ç‰ˆ"""
    
    # 1. é€šå¸¸ã®ãƒ­ã‚¹è¨ˆç®—ã‚’å®Ÿè¡Œ
    loss_dict = loss_fn(preds, targets)
    
    # 2. ãƒˆãƒ©ãƒƒã‚«ãƒ¼ã«è¨˜éŒ²
    tracker.track_step(step_count, batch_idx, epoch, preds, targets, loss_dict, loss_fn)
    
    # 3. äºˆæ¸¬å€¤ç•°å¸¸æ™‚ã«æ ¹æœ¬åŸå› èª¿æŸ»
    pred_obj_sigmoid = torch.sigmoid(preds[..., 4])
    sigmoid_mean = pred_obj_sigmoid.mean().item()
    
    if sigmoid_mean < 0.001 or step_count in [0, 1, 2, 10, 20]:  # ç•°å¸¸æ™‚ + ç‰¹å®šã‚¹ãƒ†ãƒƒãƒ—
        root_investigator.investigate_prediction_collapse(
            model, preds, targets, loss_dict, optimizer, step_count, batch_idx
        )
    
    # 4. æœ€åˆã®æ•°ã‚¹ãƒ†ãƒƒãƒ—ã¨ãƒ”ãƒ³ãƒã‚¤ãƒ³ãƒˆã§ã®è©³ç´°å‡ºåŠ›
    if step_count <= 3 or (batch_idx in [10, 20, 30] and epoch == 0):
        print(f"\nğŸ”¬ [COMPREHENSIVE DEBUG] Step {step_count}, Batch {batch_idx}")
        
        # ç›´å‰ã®è¨˜éŒ²ã‚’è¡¨ç¤º
        if tracker.step_history:
            latest = tracker.step_history[-1]
            
            print(f"   Target Summary:")
            target_info = latest['target_analysis']
            print(f"     Positive (>0.01): {target_info['positive_01']}")
            print(f"     Positive (>0.1): {target_info['positive_1']}")
            print(f"     Mean positive: {target_info['mean_positive']:.6f}")
            
            print(f"   Prediction Summary:")
            pred_info = latest['prediction_analysis']
            print(f"     Sigmoid mean: {pred_info['pred_obj_sigmoid_mean']:.6f}")
            print(f"     Sigmoid std: {pred_info['pred_obj_sigmoid_std']:.6f}")
            
            if 'focal_loss_state' in latest:
                focal_info = latest['focal_loss_state']
                print(f"   Focal Loss State:")
                print(f"     Gamma: {focal_info['gamma']}")
                print(f"     Alpha: {focal_info['alpha']}")
                print(f"     Internal step: {focal_info['step_count']}")
            
            print(f"   Loss Values:")
            loss_info = latest['loss_values']
            print(f"     Total: {loss_info['total']:.6f}")
            print(f"     Box: {loss_info['box']:.6f}")
            print(f"     Obj: {loss_info['obj']:.6f}")
            print(f"     Cls: {loss_info['cls']:.6f}")
            
            if 'manual_loss_check' in latest and 'error' not in latest['manual_loss_check']:
                manual_info = latest['manual_loss_check']
                print(f"   Manual Check:")
                print(f"     Manual focal: {manual_info['manual_focal_mean']:.6f}")
                print(f"     Difference: {abs(manual_info['manual_focal_mean'] - loss_info['obj']):.6f}")
    
    return loss_dict


# ===== æ—¢å­˜ã®ã‚¯ãƒ©ã‚¹ã¨é–¢æ•°ï¼ˆunified_training_gm.pyã‹ã‚‰ï¼‰ =====
class TrainingConfig:
    """å­¦ç¿’è¨­å®šã‚’ä¸€å…ƒç®¡ç†"""
    def __init__(self):
        # åŸºæœ¬è¨­å®š
        self.batch_size = 8  # â˜…â˜…â˜… 20 â†’ 8 ã«å‰Šæ¸› â˜…â˜…â˜…
        self.num_classes = 15
        self.epochs = 30
        self.input_size = (640, 512)  # (W, H)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # å­¦ç¿’ç‡è¨­å®š
        self.initial_lr = 1e-3
        self.max_lr = 1e-3
        self.min_lr = 1e-6
        self.warmup_steps = 500

        # Lossé‡ã¿ï¼ˆåˆæœŸå€¤ï¼‰
        self.loss_weights = {
            'box': 5.0,
            'obj': 2.0,
            'cls': 1.0
        }
        
        # Gradient Accumulationï¼ˆãƒãƒƒãƒã‚µã‚¤ã‚ºå‰Šæ¸›ã®è£œå„Ÿï¼‰
        self.accumulation_steps = 8  # â˜…â˜…â˜… 4 â†’ 8 ã«å¢—åŠ  â˜…â˜…â˜…
        self.effective_batch_size = self.batch_size * self.accumulation_steps  # 8 Ã— 8 = 64
        
        # ãƒ‘ã‚¹è¨­å®š
        self.project_root = "/content/drive/MyDrive/EfficientNet_Project"
        self.train_img_dir = "/content/FLIR_YOLO_local/images/train"
        self.train_label_dir = "/content/FLIR_YOLO_local/labels/train"
        self.model_save_path = os.path.join(self.project_root, "saved_models")
        os.makedirs(self.model_save_path, exist_ok=True)
        
        # é€²æ—è¡¨ç¤ºè¨­å®š
        self.progress_interval = 10
        self.memory_check_interval = 10  # â˜…â˜…â˜… 50 â†’ 10 ã«çŸ­ç¸® â˜…â˜…â˜…
        
        # ã‚¢ãƒ³ã‚«ãƒ¼è¨­å®š
        self.anchor_threshold = 0.2  # IoUé–¾å€¤
        self.num_anchors = 9  # ç·ã‚¢ãƒ³ã‚«ãƒ¼æ•°

        print(f"ğŸ”§ Memory-optimized config:")
        print(f"   Batch size: {self.batch_size}")
        print(f"   Accumulation steps: {self.accumulation_steps}")
        print(f"   Effective batch size: {self.effective_batch_size}")
        print(f"   Memory check interval: {self.memory_check_interval}")


class WarmRestartScheduler:
    """Cosine Annealing with Warm Restarts"""
    def __init__(self, optimizer, T_0=10, T_mult=2, eta_min=1e-6, verbose=True):
        self.optimizer = optimizer
        self.T_0 = T_0
        self.T_mult = T_mult
        self.eta_min = eta_min
        self.eta_max = optimizer.param_groups[0]['lr']
        self.verbose = verbose
        
        self.T_cur = 0
        self.T_i = T_0
        self.restart_count = 0
        self.total_steps = 0
        
        self._set_lr(self.eta_max)
    
    def _set_lr(self, lr):
        """ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã®å­¦ç¿’ç‡ã‚’è¨­å®š"""
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr
    
    def get_lr(self):
        lr = self.eta_min + (self.eta_max - self.eta_min) * \
             (1 + math.cos(math.pi * self.T_cur / self.T_i)) / 2
        return lr
    
    def step(self):
        self.total_steps += 1
        self.T_cur += 1
        
        new_lr = self.get_lr()
        self._set_lr(new_lr)
        
        if self.T_cur >= self.T_i:
            self.restart_count += 1
            if self.verbose:
                print(f"\nğŸ”„ Learning rate warm restart #{self.restart_count}")
                print(f"   Next cycle: {int(self.T_i * self.T_mult)} steps")
            
            self.T_cur = 0
            self.T_i = int(self.T_i * self.T_mult)
        
        return new_lr


def get_memory_usage() -> Dict[str, float]:
    """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—"""
    memory_info = {}
    
    if torch.cuda.is_available():
        memory_info['gpu_allocated'] = torch.cuda.memory_allocated() / 1024**3
        memory_info['gpu_reserved'] = torch.cuda.memory_reserved() / 1024**3
    
    memory_info['cpu_percent'] = 0.0  # psutilãªã—ã§ã‚·ãƒ³ãƒ—ãƒ«ã«
    
    return memory_info

# â˜…â˜…â˜… ç©æ¥µçš„ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—é–¢æ•° â˜…â˜…â˜…
def aggressive_memory_cleanup():
    """ç©æ¥µçš„ãªãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    import gc
    
    # Python ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
    collected = gc.collect()
    
    # CUDA ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    
    # ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3
        reserved = torch.cuda.memory_reserved() / 1024**3
        print(f"ğŸ§¹ Memory cleanup: {collected} objects | GPU: {allocated:.1f}GB alloc, {reserved:.1f}GB reserved")
    
    return allocated if torch.cuda.is_available() else 0

def progressive_unfreezing(model: nn.Module, epoch: int) -> str:
    """æ®µéšçš„ãƒ¬ã‚¤ãƒ¤ãƒ¼è§£å‡"""
    total_layers = len(list(model.backbone.parameters()))
    
    if epoch < 3:
        # Stage 1: Backboneå®Œå…¨å‡çµ
        for param in model.backbone.parameters():
            param.requires_grad = False
        stage = "Head+Neck Only"
    elif epoch < 8:
        # Stage 2: å¾ŒåŠ50%ã‚’è§£å‡
        for i, param in enumerate(model.backbone.parameters()):
            param.requires_grad = i >= total_layers // 2
        stage = "Backbone 50% + Head+Neck"
    elif epoch < 15:
        # Stage 3: å¾ŒåŠ75%ã‚’è§£å‡
        for i, param in enumerate(model.backbone.parameters()):
            param.requires_grad = i >= total_layers // 4
        stage = "Backbone 75% + Head+Neck"
    else:
        # Stage 4: å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼è§£å‡
        for param in model.backbone.parameters():
            param.requires_grad = True
        stage = "Full Model"
    
    # Neck/Headã¯å¸¸ã«å­¦ç¿’å¯èƒ½
    for param in model.neck.parameters():
        param.requires_grad = True
    for param in model.head.parameters():
        param.requires_grad = True
    
    return stage


def collate_fn(batch):
    """DataLoaderç”¨ã®collateé–¢æ•°"""
    return tuple(zip(*batch))


# ===== ãƒ¡ã‚¤ãƒ³å­¦ç¿’é–¢æ•°ï¼ˆä¿®æ­£ç‰ˆï¼‰ =====
def main():

    # æœ€åˆã«ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ã‚’è¡¨ç¤º
    training_version.print_version_info()

    # è¨­å®šåˆæœŸåŒ–
    config = TrainingConfig()
    print("ğŸš€ Starting Unified EfficientNet Training with Comprehensive Debug")
    print(f"ğŸ“± Device: {config.device}")
    print(f"ğŸ¯ Target: {config.num_classes} classes")
    print(f"ğŸ“ Input size: {config.input_size}")
    print("ver 1.5-gemini - Corrected for Latest Project Structure")
    
    # ãƒ†ã‚¹ãƒˆç”¨ã‚³ãƒ¼ãƒ‰
    from efficientnet_model import test_model_creation

    model = test_model_creation()
    if model:
        print("âœ… ãƒ¢ãƒ‡ãƒ«ä½œæˆæˆåŠŸï¼")
    else:
        print("âŒ ãƒ¢ãƒ‡ãƒ«ä½œæˆå¤±æ•—")

    # === ãƒ‡ãƒãƒƒã‚°ãƒˆãƒ©ãƒƒã‚«ãƒ¼åˆæœŸåŒ– ===
    debug_tracker = LossDebugTracker()
    root_investigator = RootCauseInvestigator()
    step_counter = 0
    
    # === AMPè¨­å®šã‚’å®Œå…¨ç„¡åŠ¹åŒ– ===
    print("ğŸ”§ AMPï¼ˆè‡ªå‹•æ··åˆç²¾åº¦ï¼‰ã‚’å®Œå…¨ç„¡åŠ¹åŒ–ã—ã¾ã™")
    use_amp = False
    scaler = None

    # ãƒ¢ãƒ‡ãƒ«ã‚’æ˜ç¤ºçš„ã«float32ã«è¨­å®š
    model = create_efficientnet_model(num_classes=config.num_classes, pretrained=True).to(config.device)
    model = model.float()  # æ˜ç¤ºçš„ã«float32ã«è¨­å®š
    print(f"ğŸ”§ Model dtype: {next(model.parameters()).dtype}")

    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™
    print("\n=== ğŸ“š Dataset Loading ===")
    train_dataset = YoloInfraredDataset(
        image_dir=config.train_img_dir,
        label_dir=config.train_label_dir,
        input_size=config.input_size,
        num_classes=config.num_classes
    )
    
    print(f"âœ… Dataset: {len(train_dataset)} samples")
    
    # ã‚¢ãƒ³ã‚«ãƒ¼æœ€é©åŒ–
    print("\n=== ğŸ¯ Anchor Optimization ===")
    
    default_anchors = get_default_anchors()
    
    try:
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ†æã¨ã‚¢ãƒ³ã‚«ãƒ¼æœ€é©åŒ–
        optimized_anchors_pixel, class_dist, stats = analyze_dataset_statistics(
            train_dataset,
            num_samples=1000,
            input_size=config.input_size
        )
        
        if optimized_anchors_pixel is None:
            print("âš ï¸ Using default anchors")
            optimized_anchors_pixel = default_anchors
        else:
            print("âœ… Anchor optimization completed")
            
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¨ã®æ¯”è¼ƒ
            print("\n=== ğŸ“Š Anchor Comparison ===")
            comparison = compare_anchor_sets(
                train_dataset,
                default_anchors,
                optimized_anchors_pixel,
                num_samples=500,
                input_size=config.input_size
            )
            
    except Exception as e:
        print(f"âš ï¸ Anchor optimization failed: {e}")
        print("   Using default anchors")
        optimized_anchors_pixel = default_anchors
    
    # DataLoader
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        collate_fn=collate_fn,
        num_workers=2,
        pin_memory=True
    )
    
    print(f"ğŸ“¦ Batches per epoch: {len(train_loader)}")
    
    # é€²æ—è¡¨ç¤ºé–“éš”ã®èª¿æ•´
    if len(train_loader) < config.progress_interval:
        print(f"âš ï¸ Only {len(train_loader)} batches, adjusting progress interval to 1")
        config.progress_interval = 1
    else:
        print(f"ğŸ“Š Progress will be shown every {config.progress_interval} batches")
    
        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    print("\n=== ğŸ¤– Model Initialization ===")
    model = create_efficientnet_model(num_classes=config.num_classes, pretrained=True).to(config.device)

    # === ğŸ“ Anchor & Grid Setup === ã®éƒ¨åˆ†
    print("\n=== ğŸ“ Anchor & Grid Setup ===")
    model.eval()
    with torch.no_grad():
        test_input = torch.randn(1, 1, *config.input_size[::-1]).to(config.device)
        # å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ã‚°ãƒªãƒƒãƒ‰ã‚µã‚¤ã‚ºã‚’å–å¾—
        feat1, feat2, feat3 = model.backbone(test_input)
        p1, p2, p3 = model.neck(feat1, feat2, feat3)
        grid_sizes = [(p.shape[2], p.shape[3]) for p in [p1, p2, p3]]
        strides = [config.input_size[1] // gs[0] for gs in grid_sizes]
        
        # ã‚¢ãƒ³ã‚«ãƒ¼ãƒã‚¤ãƒ³ãƒˆã¨ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ã‚’ãƒ†ãƒ³ã‚½ãƒ«å½¢å¼ã§çµ±ä¸€
        anchor_points_list = []
        strides_list = []
        for i, stride_val in enumerate(strides):
            h, w = grid_sizes[i]
            grid_y, grid_x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing='ij')
            grid = torch.stack((grid_x, grid_y), 2).view(-1, 2)
            anchor_points = (grid.float() + 0.5)
            anchor_points_list.append(anchor_points.to(config.device))
            strides_list.append(torch.full((h * w, 1), stride_val).to(config.device))

        # ãƒ•ãƒ©ãƒƒãƒˆåŒ–ã—ãŸãƒ†ãƒ³ã‚½ãƒ«ã‚‚ä½œæˆ
        anchor_points_flat = torch.cat(anchor_points_list, dim=0)  # [total_anchors, 2]
        strides_flat = torch.cat(strides_list, dim=0)             # [total_anchors, 1]

    # â˜…â˜…â˜… é‡è¦ãªä¿®æ­£: lossé–¢æ•°ã®ä½œæˆéƒ¨åˆ†ã‚‚ä¿®æ­£ â˜…â˜…â˜…
    anchor_info = {
        'anchor_points': anchor_points_list,      # ãƒ¬ãƒ™ãƒ«åˆ¥ãƒªã‚¹ãƒˆï¼ˆå¾Œæ–¹äº’æ›ç”¨ï¼‰
        'strides_list': strides_list,            # ãƒ¬ãƒ™ãƒ«åˆ¥ãƒªã‚¹ãƒˆï¼ˆå¾Œæ–¹äº’æ›ç”¨ï¼‰
        'anchor_points_flat': anchor_points_flat, # ãƒ•ãƒ©ãƒƒãƒˆåŒ–ãƒ†ãƒ³ã‚½ãƒ«
        'strides': strides_flat,                 # ãƒ†ãƒ³ã‚½ãƒ«å½¢å¼
        'grid_sizes': grid_sizes,
        'input_size': config.input_size
    }

    # ã‚¢ãƒ³ã‚«ãƒ¼ãƒ»ã‚°ãƒªãƒƒãƒ‰æƒ…å ±æº–å‚™ï¼ˆLossé–¢æ•°ç”¨ã«åˆ¥é€”ä½œæˆï¼‰
    anchor_grid_info = prepare_anchor_grid_info(
        anchors_pixel_per_level=optimized_anchors_pixel,
        model_grid_sizes=grid_sizes,
        input_size_wh=config.input_size
    )

    # â˜…â˜…â˜… é‡è¦: Lossé–¢æ•°ã«ã¯anchor_infoã‚’æ¸¡ã™ï¼ˆanchor_grid_infoã§ã¯ãªã„ï¼‰ â˜…â˜…â˜…
    print("\n=== ğŸ¯ Loss Function Setup ===")
    loss_fn = create_enhanced_loss(
        num_classes=config.num_classes,
        anchor_info=anchor_info,  # â† anchor_grid_info ã§ã¯ãªã anchor_info ã‚’ä½¿ã†ï¼
        loss_strategy='balanced'
    )

    model.train()
    print(f"âœ… Anchor and grid info prepared - Points: {anchor_points_flat.shape}, Strides: {strides_flat.shape}")
    
    # ã‚¢ãƒ³ã‚«ãƒ¼ãƒ»ã‚°ãƒªãƒƒãƒ‰æƒ…å ±æº–å‚™
    anchor_grid_info = prepare_anchor_grid_info(
        anchors_pixel_per_level=optimized_anchors_pixel,
        model_grid_sizes=grid_sizes,
        input_size_wh=config.input_size
    )
    
    # Lossé–¢æ•°åˆæœŸåŒ–
    print("\n=== ğŸ¯ Loss Function Setup ===")
    loss_fn = create_enhanced_loss(
        num_classes=config.num_classes,
        loss_strategy='balanced',  # unified_loss.pyã®æˆ¦ç•¥ã‚’ä½¿ç”¨
        anchor_info=anchor_grid_info
    )
    
    # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©
    print("\n=== âš™ï¸ Optimizer Setup ===")
    optimizer = optim.AdamW(
        model.parameters(),
        lr=config.initial_lr,
        weight_decay=0.01,
        betas=(0.9, 0.999)
    )
    
    batches_per_optimizer_step = len(train_loader) // config.accumulation_steps
    scheduler = WarmRestartScheduler(
        optimizer=optimizer,
        T_0=batches_per_optimizer_step * 2,  # 2ã‚¨ãƒãƒƒã‚¯ç›¸å½“
        T_mult=1.5,
        eta_min=config.min_lr,
        verbose=True
    )
    
    # unified_training.py ã®å­¦ç¿’ãƒ«ãƒ¼ãƒ—éƒ¨åˆ†ã‚’ä»¥ä¸‹ã«ç½®ãæ›ãˆ

    # å­¦ç¿’ãƒ«ãƒ¼ãƒ—ï¼ˆå®Œå…¨ä¿®æ­£ç‰ˆï¼‰
    print(f"\nğŸ¬ Training Started with Bug Fixes!")
    best_box_loss = float('inf')
    best_obj_loss = 0.0
    
    # â˜…â˜…â˜… ä¿®æ­£1: step_counterã‚’é–¢æ•°ãƒ¬ãƒ™ãƒ«ã§åˆæœŸåŒ– â˜…â˜…â˜…
    global_step_counter = 0
    
    for epoch in range(config.epochs):
        epoch_start_time = time.time()
        epoch_losses = {"total": [], "box": [], "obj": [], "cls": []}
        
        # æ®µéšçš„è§£å‡
        unfreezing_stage = progressive_unfreezing(model, epoch)
        
        print(f"\n{'='*80}")
        print(f"ğŸ¯ Epoch {epoch+1}/{config.epochs} | Stage: {unfreezing_stage}")
        print(f"ğŸ“¦ Total batches: {len(train_loader)} | Batch size: {config.batch_size}")
        print(f"âš™ï¸ Accumulation steps: {config.accumulation_steps}")
        print(f"{'='*80}")
        
        # è©³ç´°ãªé€²æ—ãƒˆãƒ©ãƒƒã‚«ãƒ¼
        successful_batches = 0
        oom_count = 0
        error_count = 0
        batch_times = []
        skipped_batches = 0
        
        # â˜…â˜…â˜… ä¿®æ­£2: current_lrã‚’äº‹å‰ã«åˆæœŸåŒ– â˜…â˜…â˜…
        current_lr = config.initial_lr
        
        # Gradient Accumulationç”¨
        accumulated_loss = 0.0
        optimizer.zero_grad()
        
        for batch_idx, (images, targets) in enumerate(train_loader):
            batch_start_time = time.time()
            
            if len(images) == 0:
                skipped_batches += 1
                if batch_idx % 100 == 0:  # ãŸã¾ã«å ±å‘Š
                    print(f"   âš ï¸ Batch {batch_idx}: Empty batch (total skipped: {skipped_batches})")
                continue
        
            try:
                # â˜…â˜…â˜… ä¿®æ­£3: ãƒ¡ãƒ¢ãƒªæƒ…å ±ã‚’å®‰å…¨ã«å–å¾— â˜…â˜…â˜…
                current_memory = 0.0
                if batch_idx % 10 == 0:  # é »åº¦ã‚’ä¸‹ã’ã‚‹
                    try:
                        current_memory = aggressive_memory_cleanup()
                        if current_memory > 13.0:
                            print(f"   ğŸš¨ Batch {batch_idx}: High memory: {current_memory:.1f}GB")
                    except Exception:
                        current_memory = 0.0
                
                # ãƒãƒƒãƒæº–å‚™
                images = torch.stack(images).to(config.device, non_blocking=True).float()
                targets = [t.to(config.device, non_blocking=True).float() for t in targets]
                
                # Forward pass
                preds = model(images)
                preds = preds.float()
                
                # â˜…â˜…â˜… ä¿®æ­£4: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ§‹ç¯‰ã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ– â˜…â˜…â˜…
                try:
                    target_dict = build_targets(
                        predictions=preds.detach(),
                        targets=targets,
                        anchor_info=anchor_info,
                        num_classes=config.num_classes
                    )
                except Exception as target_error:
                    print(f"   âŒ build_targets failed at batch {batch_idx}: {type(target_error).__name__}")
                    error_count += 1
                    continue
                
                # â˜…â˜…â˜… ä¿®æ­£5: æå¤±è¨ˆç®—ã®è©³ç´°ãƒ‡ãƒãƒƒã‚° â˜…â˜…â˜…
                try:
                    loss_dict = loss_fn(preds, target_dict)
                    
                    # ç•°å¸¸ãªæå¤±å€¤ã®æ¤œå‡ºã¨ä¿®æ­£
                    if loss_dict["obj"].item() > 1000.0:
                        print(f"   ğŸš¨ ABNORMAL OBJ LOSS DETECTED: {loss_dict['obj'].item():.2f}")
                        print(f"      Box: {loss_dict['box'].item():.4f}")
                        print(f"      Cls: {loss_dict['cls'].item():.4f}")
                        print(f"      Positive samples: {loss_dict.get('pos_samples', 'N/A')}")
                        
                        # ç•°å¸¸å€¤ã®å ´åˆã¯å­¦ç¿’ç‡ã‚’1/10ã«ä¸‹ã’ã¦ç¶šè¡Œ
                        if current_lr > 1e-5:
                            current_lr = current_lr / 10.0
                            for g in optimizer.param_groups:
                                g['lr'] = current_lr
                            print(f"      â†’ Emergency LR reduction to {current_lr:.2e}")
                        
                        # ãã‚Œã§ã‚‚ç•°å¸¸ãªå ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                        if loss_dict["obj"].item() > 10000.0:
                            print(f"      â†’ Skipping this batch (too abnormal)")
                            error_count += 1
                            continue
                    
                except Exception as loss_error:
                    print(f"   âŒ Loss calculation failed at batch {batch_idx}: {type(loss_error).__name__}")
                    error_count += 1
                    continue
                
                # â˜…â˜…â˜… ä¿®æ­£6: å®‰å…¨ãªæå¤±å€¤ã®å–å¾— â˜…â˜…â˜…
                try:
                    loss = loss_dict["total"] / config.accumulation_steps
                    
                    # NaN/Inf ãƒã‚§ãƒƒã‚¯
                    if torch.isnan(loss) or torch.isinf(loss):
                        print(f"   ğŸš¨ NaN/Inf loss detected at batch {batch_idx}, skipping")
                        error_count += 1
                        continue
                    
                    # ç•°å¸¸ã«å¤§ãã„æå¤±å€¤ã®ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
                    if loss.item() > 1000.0:
                        loss = torch.clamp(loss, max=100.0)
                        print(f"   âš ï¸ Loss clipped to {loss.item():.2f}")
                    
                    accumulated_loss += loss.item()
                    
                except Exception as e:
                    print(f"   âŒ Loss processing failed: {e}")
                    error_count += 1
                    continue
                
                # â˜…â˜…â˜… ä¿®æ­£7: å®‰å…¨ãªBackward pass â˜…â˜…â˜…
                try:
                    loss.backward()
                    
                    # NaNå‹¾é…ã®å³åº§ãƒã‚§ãƒƒã‚¯
                    has_nan_grad = False
                    for name, param in model.named_parameters():
                        if param.grad is not None and torch.isnan(param.grad).any():
                            print(f"   ğŸš¨ NaN gradient in {name} at batch {batch_idx}")
                            has_nan_grad = True
                            break
                    
                    if has_nan_grad:
                        optimizer.zero_grad()  # å‹¾é…ã‚’ã‚¯ãƒªã‚¢
                        error_count += 1
                        continue
                        
                except Exception as backward_error:
                    print(f"   âŒ Backward pass failed: {backward_error}")
                    error_count += 1
                    continue
                
                # â˜…â˜…â˜… ä¿®æ­£8: Gradient Stepã®å®‰å…¨ãªå®Ÿè¡Œ â˜…â˜…â˜…
                if (batch_idx + 1) % config.accumulation_steps == 0:
                    try:
                        # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                        
                        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚¹ãƒ†ãƒƒãƒ—
                        optimizer.step()
                        global_step_counter += 1

                        # â˜…â˜…â˜… ä¿®æ­£9: å­¦ç¿’ç‡æ›´æ–°ã®å®‰å…¨ãªå®Ÿè¡Œ â˜…â˜…â˜…
                        if global_step_counter < config.warmup_steps:
                            lr_scale = (global_step_counter + 1) / config.warmup_steps
                            current_lr = config.initial_lr * lr_scale
                            for g in optimizer.param_groups:
                                g['lr'] = current_lr
                        else:
                            try:
                                current_lr = scheduler.step()
                            except Exception:
                                current_lr = optimizer.param_groups[0]['lr']

                        optimizer.zero_grad()
                        accumulated_loss = 0.0
                        
                    except Exception as step_error:
                        print(f"   âŒ Optimizer step failed: {step_error}")
                        optimizer.zero_grad()
                        error_count += 1
                        continue
                
                # â˜…â˜…â˜… ä¿®æ­£10: å®‰å…¨ãªé€²æ—è¡¨ç¤º â˜…â˜…â˜…
                if batch_idx % config.progress_interval == 0:
                    try:
                        progress_pct = ((batch_idx + 1) / len(train_loader)) * 100
                        elapsed = time.time() - epoch_start_time
                        eta_seconds = elapsed / (batch_idx + 1) * (len(train_loader) - batch_idx - 1)
                        eta_minutes = eta_seconds / 60
                        
                        print(f"""   ğŸ“Š Batch {batch_idx+1:4d}/{len(train_loader)} ({progress_pct:5.1f}%)
                        ğŸ“‰ Loss: T={loss_dict['total']:.4f} B={loss_dict['box']:.4f} O={loss_dict['obj']:.4f} C={loss_dict['cls']:.4f}
                        âš™ï¸ LR: {current_lr:.2e} | ETA: {eta_minutes:.1f}min | Mem: {current_memory:.1f}GB
                        âœ… Success: {successful_batches} | âŒ Errors: {error_count} | ğŸš¨ OOM: {oom_count}""")
                        
                    except Exception:
                        print(f"   ğŸ“Š Batch {batch_idx+1}/{len(train_loader)} - Progress display error")
                
                # æå¤±å±¥æ­´ã«è¨˜éŒ²
                try:
                    epoch_losses["total"].append(loss_dict["total"].item())
                    epoch_losses["box"].append(loss_dict["box"].item())
                    epoch_losses["obj"].append(loss_dict["obj"].item())
                    epoch_losses["cls"].append(loss_dict["cls"].item())
                except Exception:
                    pass  # è¨˜éŒ²å¤±æ•—ã¯ç„¡è¦–
                
                successful_batches += 1
                batch_times.append(time.time() - batch_start_time)
                
                # ä¸­é–“ãƒ†ãƒ³ã‚½ãƒ«ã‚’æ˜ç¤ºçš„ã«å‰Šé™¤
                try:
                    del preds, target_dict, loss_dict, loss
                except Exception:
                    pass
                
                # å®šæœŸçš„ãªãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                if batch_idx % config.memory_check_interval == 0:
                    try:
                        aggressive_memory_cleanup()
                    except Exception:
                        pass
                    
            except torch.cuda.OutOfMemoryError as e:
                oom_count += 1
                print(f"   ğŸš¨ OOM at batch {batch_idx}/{len(train_loader)} (#{oom_count})")
                try:
                    aggressive_memory_cleanup()
                    optimizer.zero_grad()
                    accumulated_loss = 0.0
                except Exception:
                    pass
                continue
                
            except Exception as e:
                error_count += 1
                print(f"   âš ï¸ Unexpected error at batch {batch_idx}: {type(e).__name__}: {str(e)[:100]}")
                try:
                    aggressive_memory_cleanup()
                    optimizer.zero_grad()
                except Exception:
                    pass
                continue
        
        # â˜…â˜…â˜… ä¿®æ­£11: ã‚¨ãƒãƒƒã‚¯çµ‚äº†æ™‚ã®è©³ç´°ã‚µãƒãƒªãƒ¼ â˜…â˜…â˜…
        epoch_time = time.time() - epoch_start_time
        
        print(f"\n{'='*80}")
        print(f"ğŸ“Š Epoch {epoch+1} Summary | Time: {epoch_time:.1f}s ({epoch_time/60:.1f}min)")
        print(f"âœ… Successful: {successful_batches}/{len(train_loader)} ({successful_batches/len(train_loader)*100:.1f}%)")
        print(f"âš ï¸ Skipped: {skipped_batches} | âŒ Errors: {error_count} | ğŸš¨ OOM: {oom_count}")
        
        if batch_times:
            avg_batch_time = np.mean(batch_times)
            estimated_full_time = avg_batch_time * len(train_loader) / 60
            print(f"â±ï¸ Avg batch: {avg_batch_time:.2f}s | Est. full epoch: {estimated_full_time:.1f}min")
        
        # â˜…â˜…â˜… ä¿®æ­£12: æå¤±ã‚µãƒãƒªãƒ¼ã®å®‰å…¨ãªè¨ˆç®— â˜…â˜…â˜…
        if epoch_losses["total"] and len(epoch_losses["total"]) > 0:
            try:
                # ç•°å¸¸å€¤ã‚’é™¤å¤–ã—ã¦ã‹ã‚‰å¹³å‡è¨ˆç®—
                total_losses = [l for l in epoch_losses['total'] if l < 1000.0]
                box_losses = [l for l in epoch_losses['box'] if l < 100.0]
                obj_losses = [l for l in epoch_losses['obj'] if l < 1000.0]
                cls_losses = [l for l in epoch_losses['cls'] if l < 100.0]
                
                if total_losses:
                    avg_metrics = {
                        'total': np.mean(total_losses),
                        'box': np.mean(box_losses) if box_losses else 0.0,
                        'obj': np.mean(obj_losses) if obj_losses else 0.0,
                        'cls': np.mean(cls_losses) if cls_losses else 0.0
                    }
                    
                    print(f"ğŸ“‰ Average Losses (outliers removed):")
                    print(f"   Total: {avg_metrics['total']:.4f} | Box: {avg_metrics['box']:.4f}")
                    print(f"   Obj: {avg_metrics['obj']:.4f} | Cls: {avg_metrics['cls']:.4f}")
                    print(f"   Valid samples: {len(total_losses)}/{len(epoch_losses['total'])}")
                    
                    # â˜…â˜…â˜… å•é¡Œè¨ºæ–­ â˜…â˜…â˜…
                    if successful_batches < len(train_loader) * 0.3:
                        print(f"\nğŸš¨ CRITICAL: Success rate < 30%!")
                        print(f"   â†’ Reduce batch_size to {max(1, config.batch_size//2)}")
                        print(f"   â†’ Increase accumulation_steps to {config.accumulation_steps*2}")
                    
                    if len(total_losses) < len(epoch_losses['total']) * 0.5:
                        print(f"\nğŸš¨ WARNING: 50%+ outlier losses detected!")
                        print(f"   â†’ Loss function may be unstable")
                        print(f"   â†’ Consider reducing learning rate to {current_lr/10:.2e}")
                    
                    if avg_metrics['obj'] < 0.001:
                        print(f"\nğŸš¨ WARNING: Objectness loss collapsed!")
                        print(f"   â†’ Model predictions may have saturated")
                        print(f"   â†’ Consider restarting with lower LR")
                    
                    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜æ¡ä»¶
                    save_model = False
                    
                    if avg_metrics['box'] < best_box_loss and avg_metrics['box'] > 0.01:
                        best_box_loss = avg_metrics['box']
                        save_model = True
                        print(f"ğŸ‰ New best Box Loss: {best_box_loss:.4f}")
                    
                    if avg_metrics['obj'] > best_obj_loss and avg_metrics['obj'] > 0.01:
                        best_obj_loss = avg_metrics['obj']
                        save_model = True
                        print(f"ğŸ‰ Obj Loss improved: {best_obj_loss:.4f}")
                    
                    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
                    if save_model and successful_batches > len(train_loader) * 0.1:
                        try:
                            checkpoint = {
                                'epoch': epoch,
                                'model_state_dict': model.state_dict(),
                                'optimizer_state_dict': optimizer.state_dict(),
                                'best_box_loss': best_box_loss,
                                'best_obj_loss': best_obj_loss,
                                'avg_metrics': avg_metrics,
                                'training_stats': {
                                    'successful_batches': successful_batches,
                                    'total_batches': len(train_loader),
                                    'oom_count': oom_count,
                                    'error_count': error_count,
                                    'epoch_time': epoch_time,
                                    'final_lr': current_lr
                                }
                            }
                            
                            save_path = os.path.join(
                                config.model_save_path,
                                f"fixed_model_epoch_{epoch+1}_box_{avg_metrics['box']:.4f}_success_{successful_batches}.pth"
                            )
                            torch.save(checkpoint, save_path)
                            print(f"ğŸ’¾ Model saved: {os.path.basename(save_path)}")
                        except Exception as save_error:
                            print(f"âš ï¸ Model save failed: {save_error}")
                    
                else:
                    print(f"âŒ No valid losses recorded (all outliers)")
                    
            except Exception as summary_error:
                print(f"âŒ Summary calculation failed: {summary_error}")
        else:
            print(f"âŒ No losses recorded in this epoch")
            print(f"   â†’ Check data loading and model forward pass")
        
        print(f"{'='*80}")
        
        # æ—©æœŸçµ‚äº†æ¡ä»¶
        if successful_batches == 0:
            print(f"ğŸ’¥ TRAINING HALT: No successful batches in epoch {epoch+1}")
            print(f"   â†’ Check your data, model, or reduce batch size")
            break
        
        if epoch_losses["total"] and len([l for l in epoch_losses['total'] if l < 1000.0]) > 0:
            valid_losses = [l for l in epoch_losses['total'] if l < 1000.0]
            avg_loss = np.mean(valid_losses)
            if avg_loss < 0.5:
                print(f"ğŸ¯ TARGET ACHIEVED! Average Loss < 0.5")
                break
    
    # å­¦ç¿’çµ‚äº†å‡¦ç†
    print(f"\nğŸŠ Training Complete!")
    print(f"ğŸ† Best Box Loss: {best_box_loss:.4f}")
    print(f"ğŸ‘ï¸ Best Obj Loss: {best_obj_loss:.4f}")
    
    return model, best_box_loss, best_obj_loss


if __name__ == "__main__":
    try:
        model, best_box_loss, best_obj_loss = main()
        print(f"\nâœ… Training completed successfully!")
        print(f"ğŸ¯ Final best Box Loss: {best_box_loss:.4f}")
        print(f"ğŸ‘ï¸ Final best Obj Loss: {best_obj_loss:.4f}")
    except KeyboardInterrupt:
        print("\nâš ï¸ Training interrupted by user")
    except Exception as e:
        print(f"\nâŒ Training failed: {e}")
        import traceback
        traceback.print_exc()