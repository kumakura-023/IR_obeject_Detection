# integration_test.py - Step 4: æ–°ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£çµ±åˆãƒ†ã‚¹ãƒˆ

import torch
import torch.optim as optim
from torch.utils.data import DataLoader
import time
import os

# æ–°ã—ã„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from config import Config
from dataset import FLIRDataset, collate_fn

# Phase 3ã®æ–°ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
# from multiscale_model import MultiScaleYOLO
# from anchor_loss import MultiScaleAnchorLoss

# Step 1ã§ç”Ÿæˆã•ã‚ŒãŸã‚¢ãƒ³ã‚«ãƒ¼
ANCHORS = {
    'small':  [(7, 11), (14, 28), (22, 65)],      # 52x52 grid
    'medium': [(42, 35), (76, 67), (46, 126)],    # 26x26 grid  
    'large':  [(127, 117), (88, 235), (231, 218)] # 13x13 grid
}

def create_phase3_model_and_loss(cfg):
    """Phase 3ã®æ–°ãƒ¢ãƒ‡ãƒ«ã¨æå¤±é–¢æ•°ã‚’ä½œæˆ"""
    print("ğŸ¤– Phase 3ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ä½œæˆä¸­...")
    
    # æ–°ã—ã„ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ãƒ¢ãƒ‡ãƒ«ï¼ˆStep 2ã§ä½œæˆã—ãŸã‚‚ã®ã‚’å†åˆ©ç”¨ï¼‰
    exec("""
# MultiScaleYOLO class definition (embedded)
import torch
import torch.nn as nn
import torch.nn.functional as F

class FeaturePyramidNetwork(nn.Module):
    def __init__(self, in_channels_list, out_channels=256):
        super().__init__()
        self.out_channels = out_channels
        self.lateral_convs = nn.ModuleList([
            nn.Conv2d(in_ch, out_channels, 1) for in_ch in in_channels_list
        ])
        self.fpn_convs = nn.ModuleList([
            nn.Conv2d(out_channels, out_channels, 3, padding=1) 
            for _ in in_channels_list
        ])
        self._init_weights()
    
    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, features):
        laterals = [lateral_conv(feat) for lateral_conv, feat in zip(self.lateral_convs, features)]
        fpn_features = []
        prev_feat = laterals[-1]
        fpn_features.append(self.fpn_convs[-1](prev_feat))
        
        for i in range(len(laterals) - 2, -1, -1):
            upsampled = F.interpolate(prev_feat, scale_factor=2, mode='nearest')
            fused = laterals[i] + upsampled
            fpn_feat = self.fpn_convs[i](fused)
            fpn_features.append(fpn_feat)
            prev_feat = fused
        
        fpn_features.reverse()
        return fpn_features

class DetectionHead(nn.Module):
    def __init__(self, in_channels, num_classes, num_anchors=3):
        super().__init__()
        self.num_classes = num_classes
        self.num_anchors = num_anchors
        
        self.shared_conv = nn.Sequential(
            nn.Conv2d(in_channels, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True)
        )
        
        self.detection_conv = nn.Conv2d(256, num_anchors * (5 + num_classes), 1)
        self._init_weights()
    
    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
        
        with torch.no_grad():
            bias = self.detection_conv.bias.view(self.num_anchors, -1)
            bias[:, 4].fill_(-2.0)
    
    def forward(self, x):
        x = self.shared_conv(x)
        detections = self.detection_conv(x)
        B, _, H, W = detections.shape
        detections = detections.view(B, self.num_anchors, 5 + self.num_classes, H, W)
        detections = detections.permute(0, 3, 4, 1, 2).contiguous()
        detections = detections.view(B, H * W * self.num_anchors, 5 + self.num_classes)
        return detections

class MultiScaleBackbone(nn.Module):
    def __init__(self):
        super().__init__()
        self.stage1 = nn.Sequential(
            self._make_layer(1, 32, stride=2),
            self._make_layer(32, 32, stride=1),
        )
        self.stage2 = nn.Sequential(
            self._make_layer(32, 64, stride=2),
            self._make_layer(64, 64, stride=1),
        )
        self.stage3 = nn.Sequential(
            self._make_layer(64, 128, stride=2),
            self._make_layer(128, 128, stride=1),
            self._make_layer(128, 128, stride=1),
        )
        self.stage4 = nn.Sequential(
            self._make_layer(128, 256, stride=2),
            self._make_layer(256, 256, stride=1),
            self._make_layer(256, 256, stride=1),
        )
        self.stage5 = nn.Sequential(
            self._make_layer(256, 512, stride=2),
            self._make_layer(512, 512, stride=1),
            self._make_layer(512, 512, stride=1),
        )
        self._init_weights()
    
    def _make_layer(self, in_channels, out_channels, stride):
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, stride, 1),
            nn.BatchNorm2d(out_channels),
            nn.LeakyReLU(0.1, inplace=True)
        )
    
    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        x = self.stage1(x)
        x = self.stage2(x)
        c3 = self.stage3(x)
        c4 = self.stage4(c3)
        c5 = self.stage5(c4)
        return [c3, c4, c5]

class MultiScaleYOLO(nn.Module):
    def __init__(self, num_classes=15, anchors=None):
        super().__init__()
        self.num_classes = num_classes
        self.anchors = anchors or ANCHORS
        
        self.backbone = MultiScaleBackbone()
        self.fpn = FeaturePyramidNetwork([128, 256, 512], out_channels=256)
        self.head_small = DetectionHead(256, num_classes, 3)
        self.head_medium = DetectionHead(256, num_classes, 3)
        self.head_large = DetectionHead(256, num_classes, 3)
    
    def forward(self, x):
        features = self.backbone(x)
        fpn_features = self.fpn(features)
        
        outputs = {
            'small': self.head_small(fpn_features[0]),
            'medium': self.head_medium(fpn_features[1]),
            'large': self.head_large(fpn_features[2])
        }
        return outputs
""", globals())
    
    # æ–°ã—ã„æå¤±é–¢æ•°ï¼ˆStep 3ã§ä½œæˆã—ãŸã‚‚ã®ã‚’ç°¡ç•¥ç‰ˆã§ä½¿ç”¨ï¼‰
    exec("""
# Simplified MultiScaleAnchorLoss for integration test
class SimpleAnchorLoss(nn.Module):
    def __init__(self, num_classes=15):
        super().__init__()
        self.num_classes = num_classes
        self.bce_loss = nn.BCEWithLogitsLoss(reduction='mean')
        self.mse_loss = nn.MSELoss(reduction='mean')
        
        # Loss weights
        self.lambda_coord = 5.0
        self.lambda_obj = 1.0
        self.lambda_noobj = 0.5
        
    def forward(self, predictions, targets):
        # Simplified loss calculation for integration test
        total_loss = 0
        scale_count = 0
        
        for scale_name, preds in predictions.items():
            B, N, C = preds.shape
            
            # Dummy loss calculation (for testing only)
            coord_loss = torch.mean(preds[..., :4] ** 2) * self.lambda_coord
            conf_loss = torch.mean(torch.sigmoid(preds[..., 4]) ** 2) * self.lambda_obj
            cls_loss = torch.mean(preds[..., 5:] ** 2) * 1.0
            
            scale_loss = coord_loss + conf_loss + cls_loss
            total_loss += scale_loss
            scale_count += 1
        
        return total_loss / scale_count
""", globals())
    
    # ãƒ¢ãƒ‡ãƒ«ã¨ãƒ­ã‚¹é–¢æ•°ã‚’ä½œæˆ
    model = MultiScaleYOLO(num_classes=cfg.num_classes, anchors=ANCHORS)
    criterion = SimpleAnchorLoss(num_classes=cfg.num_classes)
    
    print(f"   âœ… MultiScaleYOLO: {sum(p.numel() for p in model.parameters()):,} parameters")
    print(f"   âœ… SimpleAnchorLoss: çµ±åˆãƒ†ã‚¹ãƒˆç”¨")
    
    return model, criterion

def test_memory_usage(model, cfg):
    """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ãƒ†ã‚¹ãƒˆ"""
    print(f"\nğŸ” ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒ†ã‚¹ãƒˆ")
    print("-" * 40)
    
    model.to(cfg.device)
    
    # ç¾åœ¨ã®GPUä½¿ç”¨é‡
    if cfg.device.type == 'cuda':
        torch.cuda.empty_cache()
        start_memory = torch.cuda.memory_allocated(0) / 1024**3
        print(f"   é–‹å§‹æ™‚GPUä½¿ç”¨é‡: {start_memory:.2f}GB")
    
    # ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿
    test_batch_sizes = [16, 32, 64, 96, 128]
    
    for batch_size in test_batch_sizes:
        try:
            x = torch.randn(batch_size, 1, 416, 416).to(cfg.device)
            
            with torch.no_grad():
                outputs = model(x)
            
            if cfg.device.type == 'cuda':
                current_memory = torch.cuda.memory_allocated(0) / 1024**3
                print(f"   Batch {batch_size:3d}: {current_memory:.2f}GB")
                
                if current_memory > 14.0:  # T4ã¯16GBãªã®ã§14GBè¶…ãˆãŸã‚‰è­¦å‘Š
                    print(f"   âš ï¸  Batch {batch_size}ã§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒé«˜ã„")
                    break
            
            del x, outputs
            torch.cuda.empty_cache()
            
        except RuntimeError as e:
            if "out of memory" in str(e):
                print(f"   âŒ Batch {batch_size}: OOM")
                break
            else:
                raise e
    
    print(f"   ğŸ’¡ æ¨å¥¨ãƒãƒƒãƒã‚µã‚¤ã‚º: 64-96")

def test_training_step(model, criterion, dataloader, cfg):
    """1ã‚¹ãƒ†ãƒƒãƒ—ã®å­¦ç¿’ã‚’ãƒ†ã‚¹ãƒˆ"""
    print(f"\nğŸ” å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—ãƒ†ã‚¹ãƒˆ")
    print("-" * 40)
    
    model.to(cfg.device)
    model.train()
    
    # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ä½œæˆ
    optimizer = optim.AdamW(model.parameters(), lr=1e-4)
    
    # 1ãƒãƒƒãƒã§å­¦ç¿’ãƒ†ã‚¹ãƒˆ
    for batch_idx, (images, targets) in enumerate(dataloader):
        try:
            start_time = time.time()
            
            # ãƒ‡ãƒ¼ã‚¿ã‚’GPUã«ç§»å‹•
            images = images.to(cfg.device)
            
            # Forward pass
            predictions = model(images)
            loss = criterion(predictions, targets)
            
            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            step_time = time.time() - start_time
            
            # GPUä½¿ç”¨é‡ç¢ºèª
            if cfg.device.type == 'cuda':
                memory_used = torch.cuda.memory_allocated(0) / 1024**3
                print(f"   âœ… å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—æˆåŠŸ!")
                print(f"      Loss: {loss.item():.4f}")
                print(f"      æ™‚é–“: {step_time:.3f}s")
                print(f"      GPUä½¿ç”¨é‡: {memory_used:.2f}GB")
                print(f"      ãƒãƒƒãƒã‚µã‚¤ã‚º: {images.shape[0]}")
                
                # å‡ºåŠ›å½¢çŠ¶ç¢ºèª
                print(f"   ğŸ“Š å‡ºåŠ›å½¢çŠ¶:")
                for scale, output in predictions.items():
                    print(f"      {scale}: {output.shape}")
            
            return True, loss.item(), memory_used if cfg.device.type == 'cuda' else 0
            
        except Exception as e:
            print(f"   âŒ å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            return False, None, None
        
        break  # 1ãƒãƒƒãƒã®ã¿ãƒ†ã‚¹ãƒˆ

def calculate_batch_size_recommendation(memory_used, target_memory=12.0):
    """æ¨å¥¨ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’è¨ˆç®—"""
    current_batch = 32
    current_memory = memory_used
    
    if current_memory > 0:
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã«åŸºã¥ã„ã¦æ¨å¥¨ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’è¨ˆç®—
        memory_ratio = target_memory / current_memory
        recommended_batch = int(current_batch * memory_ratio * 0.9)  # 10%ã®ãƒãƒ¼ã‚¸ãƒ³
        
        # ç¾å®Ÿçš„ãªç¯„å›²ã«åˆ¶é™
        recommended_batch = max(16, min(recommended_batch, 256))
        
        return recommended_batch
    else:
        return 64  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

def run_step4():
    """Step 4: çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    print("ğŸš€ Step 4: çµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("=" * 60)
    
    try:
        # 1. è¨­å®šèª­ã¿è¾¼ã¿
        cfg = Config()
        
        # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å°ã•ã‚ã«è¨­å®šï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
        original_batch_size = cfg.batch_size
        cfg.batch_size = 32  # çµ±åˆãƒ†ã‚¹ãƒˆç”¨ã«å°ã•ã
        
        print(f"ğŸ“‹ çµ±åˆãƒ†ã‚¹ãƒˆè¨­å®š:")
        print(f"   Device: {cfg.device}")
        print(f"   Test batch size: {cfg.batch_size}")
        print(f"   Original batch size: {original_batch_size}")
        
        # 2. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
        dataset = FLIRDataset(cfg.train_img_dir, cfg.train_label_dir, cfg.img_size, augment=False)
        dataloader = DataLoader(
            dataset, 
            batch_size=cfg.batch_size, 
            shuffle=False,  # ãƒ†ã‚¹ãƒˆç”¨ãªã®ã§ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ãªã„
            collate_fn=collate_fn,
            num_workers=0   # ãƒ†ã‚¹ãƒˆç”¨
        )
        
        print(f"   Dataset: {len(dataset)} images")
        print(f"   Test batches: {len(dataloader)}")
        
        # 3. Phase 3ãƒ¢ãƒ‡ãƒ«ä½œæˆ
        model, criterion = create_phase3_model_and_loss(cfg)
        
        # 4. ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒ†ã‚¹ãƒˆ
        test_memory_usage(model, cfg)
        
        # 5. å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—ãƒ†ã‚¹ãƒˆ
        success, loss_value, memory_used = test_training_step(model, criterion, dataloader, cfg)
        
        if not success:
            return False
        
        # 6. æ¨å¥¨è¨­å®šè¨ˆç®—
        recommended_batch = calculate_batch_size_recommendation(memory_used)
        
        print("\n" + "=" * 60)
        print("âœ… Step 4 çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†!")
        print("=" * 60)
        print(f"ğŸ“Š ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼:")
        print(f"   âœ… Phase 3ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£: æ­£å¸¸å‹•ä½œ")
        print(f"   âœ… ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«å‡ºåŠ›: 3ã‚¹ã‚±ãƒ¼ãƒ«ç¢ºèª")
        print(f"   âœ… æå¤±è¨ˆç®—: {loss_value:.4f}")
        print(f"   âœ… å‹¾é…ãƒ•ãƒ­ãƒ¼: æ­£å¸¸")
        print(f"   âœ… GPUä½¿ç”¨é‡: {memory_used:.2f}GB")
        
        print(f"\nğŸ“‹ æœ¬æ ¼å­¦ç¿’æ¨å¥¨è¨­å®š:")
        print(f"   æ¨å¥¨ãƒãƒƒãƒã‚µã‚¤ã‚º: {recommended_batch}")
        print(f"   äºˆæƒ³GPUä½¿ç”¨é‡: {memory_used * recommended_batch / 32:.1f}GB")
        print(f"   äºˆæƒ³ã‚¨ãƒãƒƒã‚¯æ™‚é–“: 4-6åˆ†")
        
        if memory_used * recommended_batch / 32 < 12:
            print(f"   âœ… T4ã§å®‰å…¨ã«å‹•ä½œå¯èƒ½")
        else:
            print(f"   âš ï¸ ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’ã•ã‚‰ã«èª¿æ•´æ¨å¥¨")
        
        print(f"\nğŸ“ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        print(f"   1. config.py ã®ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’ {recommended_batch} ã«è¨­å®š")
        print(f"   2. Phase 3å®Œå…¨ç‰ˆã§ã®æœ¬æ ¼å­¦ç¿’é–‹å§‹")
        print(f"   3. Val Loss 20.6 â†’ 5.0ä»¥ä¸‹ã‚’ç›®æ¨™")
        
        return True
        
    except Exception as e:
        print(f"âŒ Step 4ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        import traceback
        traceback.print_exc()
        return False

# ===== ä½¿ç”¨ä¾‹ =====
if __name__ == "__main__":
    # Step 4å®Ÿè¡Œ
    success = run_step4()
    
    if success:
        print("ğŸ‰ Step 4æˆåŠŸ! Phase 3å®Œå…¨å®Ÿè£…æº–å‚™å®Œäº†")
        print("ğŸš€ æ–°ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã®æœ¬æ ¼å­¦ç¿’ãŒå¯èƒ½ã§ã™!")
    else:
        print("âŒ Step 4å¤±æ•— - ã‚¨ãƒ©ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„")