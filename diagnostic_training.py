# diagnostic_training.py - è¨ºæ–­çš„å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ 

import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from collections import defaultdict
import os
from datetime import datetime
import json

class DiagnosticTrainer:
    """å­¦ç¿’éç¨‹ã‚’è©³ç´°ã«åˆ†æãƒ»è¨ºæ–­ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, save_dir="diagnostic_logs"):
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)
        
        # çµ±è¨ˆæƒ…å ±åé›†
        self.detection_stats = {
            'epoch_stats': [],
            'loss_components': defaultdict(list),
            'confidence_distribution': [],
            'class_performance': defaultdict(list),
            'anchor_usage': defaultdict(list),
            'size_distribution': defaultdict(list)
        }
        
        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¨ºæ–­ç”¨
        self.current_epoch_detections = []
        self.problematic_samples = []
        
        print(f"ğŸ”¬ DiagnosticTraineråˆæœŸåŒ–å®Œäº†")
        print(f"   ä¿å­˜å…ˆ: {save_dir}")
        print(f"   è¨ºæ–­é …ç›®: æ¤œå‡ºçµ±è¨ˆã€ä¿¡é ¼åº¦åˆ†å¸ƒã€ã‚¯ãƒ©ã‚¹æ€§èƒ½ã€ã‚¢ãƒ³ã‚«ãƒ¼ä½¿ç”¨ç‡")
    
    def start_epoch_diagnosis(self, epoch):
        """ã‚¨ãƒãƒƒã‚¯é–‹å§‹æ™‚ã®è¨ºæ–­æº–å‚™"""
        self.current_epoch = epoch
        self.current_epoch_detections = []
        self.current_epoch_losses = []
        
        print(f"\nğŸ” Epoch {epoch} è¨ºæ–­é–‹å§‹")
    
    def log_batch_diagnosis(self, batch_idx, images, targets, predictions, loss_components=None):
        """ãƒãƒƒãƒã”ã¨ã®è©³ç´°è¨ºæ–­"""
        
        # 1. æ¤œå‡ºæ•°çµ±è¨ˆ
        batch_detections = self._analyze_detections(predictions, targets)
        self.current_epoch_detections.extend(batch_detections)
        
        # 2. æå¤±æˆåˆ†è¨˜éŒ²
        if loss_components:
            for key, value in loss_components.items():
                self.detection_stats['loss_components'][key].append(value)
        
        # 3. å•é¡Œã‚µãƒ³ãƒ—ãƒ«ç‰¹å®šï¼ˆä¿¡é ¼åº¦ãŒç•°å¸¸ã«ä½ã„ï¼‰
        problematic = self._identify_problematic_samples(
            images, targets, predictions, batch_idx
        )
        self.problematic_samples.extend(problematic)
        
        # 4. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è­¦å‘Š
        if batch_idx % 20 == 0:
            self._print_realtime_diagnosis(batch_idx, batch_detections)
    
    def _analyze_detections(self, predictions, targets):
        """æ¤œå‡ºçµæœã®è©³ç´°åˆ†æ"""
        detections = []
        
        # ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«äºˆæ¸¬ã®å ´åˆ
        if isinstance(predictions, dict):
            all_preds = []
            for scale_name, scale_preds in predictions.items():
                # [B, N, 20] -> [B*N, 20]
                B, N, C = scale_preds.shape
                flat_preds = scale_preds.view(-1, C)
                all_preds.append(flat_preds)
            
            # å…¨äºˆæ¸¬ã‚’çµåˆ
            combined_preds = torch.cat(all_preds, dim=0)
        else:
            combined_preds = predictions.view(-1, predictions.shape[-1])
        
        # ä¿¡é ¼åº¦æŠ½å‡ºï¼ˆå‹¾é…ãƒ‡ã‚¿ãƒƒãƒï¼‰
        confidences = torch.sigmoid(combined_preds[:, 4]).detach().cpu().numpy()
        
        # ã‚¯ãƒ©ã‚¹äºˆæ¸¬æŠ½å‡ºï¼ˆå‹¾é…ãƒ‡ã‚¿ãƒƒãƒï¼‰
        class_probs = torch.softmax(combined_preds[:, 5:], dim=-1).detach().cpu().numpy()
        class_ids = np.argmax(class_probs, axis=-1)
        
        # æ¤œå‡ºçµ±è¨ˆ
        detection_info = {
            'total_predictions': len(confidences),
            'conf_mean': np.mean(confidences),
            'conf_std': np.std(confidences),
            'conf_max': np.max(confidences),
            'conf_above_05': np.sum(confidences > 0.5),
            'conf_above_07': np.sum(confidences > 0.7),
            'conf_above_09': np.sum(confidences > 0.9),
            'conf_distribution': np.histogram(confidences, bins=10, range=(0, 1))[0],
            'class_distribution': np.bincount(class_ids, minlength=15)
        }
        
        detections.append(detection_info)
        return detections
    
    def _identify_problematic_samples(self, images, targets, predictions, batch_idx):
        """å•é¡Œã®ã‚ã‚‹ã‚µãƒ³ãƒ—ãƒ«ã‚’ç‰¹å®š"""
        problematic = []
        
        # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å–å¾—
        B = images.shape[0]
        
        for i in range(B):
            # ã“ã®ç”»åƒã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ•°
            target_count = len(targets[i]) if len(targets) > i else 0
            
            # ã“ã®ç”»åƒã®æœ€å¤§ä¿¡é ¼åº¦ï¼ˆå‹¾é…ãƒ‡ã‚¿ãƒƒãƒï¼‰
            if isinstance(predictions, dict):
                max_conf = 0
                for scale_preds in predictions.values():
                    scale_conf = torch.sigmoid(scale_preds[i, :, 4]).detach().max().item()
                    max_conf = max(max_conf, scale_conf)
            else:
                max_conf = torch.sigmoid(predictions[i, :, 4]).detach().max().item()
            
            # å•é¡Œåˆ¤å®š
            is_problematic = False
            problem_type = []
            
            if target_count > 0 and max_conf < 0.1:
                is_problematic = True
                problem_type.append("low_confidence_with_targets")
            
            if target_count == 0 and max_conf > 0.8:
                is_problematic = True
                problem_type.append("high_confidence_without_targets")
            
            if target_count > 3 and max_conf < 0.3:
                is_problematic = True
                problem_type.append("many_targets_low_confidence")
            
            if is_problematic:
                problematic.append({
                    'batch_idx': batch_idx,
                    'sample_idx': i,
                    'target_count': target_count,
                    'max_confidence': max_conf,
                    'problem_types': problem_type
                })
        
        return problematic
    
    def _print_realtime_diagnosis(self, batch_idx, batch_detections):
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¨ºæ–­æƒ…å ±ã‚’è¡¨ç¤º"""
        if not batch_detections:
            return
        
        latest = batch_detections[-1]
        
        print(f"ğŸ“Š Batch {batch_idx} è¨ºæ–­:")
        print(f"   äºˆæ¸¬æ•°: {latest['total_predictions']:,}")
        print(f"   å¹³å‡ä¿¡é ¼åº¦: {latest['conf_mean']:.3f}")
        print(f"   æœ€å¤§ä¿¡é ¼åº¦: {latest['conf_max']:.3f}")
        print(f"   é«˜ä¿¡é ¼åº¦(>0.5): {latest['conf_above_05']}")
        print(f"   è¶…é«˜ä¿¡é ¼åº¦(>0.7): {latest['conf_above_07']}")
        
        # è­¦å‘Š
        if latest['conf_max'] < 0.1:
            print(f"   âš ï¸ è­¦å‘Š: æœ€å¤§ä¿¡é ¼åº¦ãŒç•°å¸¸ã«ä½ã„ ({latest['conf_max']:.3f})")
        if latest['conf_above_05'] == 0:
            print(f"   âš ï¸ è­¦å‘Š: ä¿¡é ¼åº¦0.5ä»¥ä¸Šã®æ¤œå‡ºãŒã‚¼ãƒ­")
    
    def end_epoch_diagnosis(self, epoch, val_loss, model=None):
        """ã‚¨ãƒãƒƒã‚¯çµ‚äº†æ™‚ã®ç·åˆè¨ºæ–­"""
        print(f"\nğŸ“‹ Epoch {epoch} ç·åˆè¨ºæ–­")
        print("=" * 60)
        
        # 1. æ¤œå‡ºçµ±è¨ˆã‚µãƒãƒªãƒ¼
        if self.current_epoch_detections:
            self._summarize_detection_stats(epoch)
        
        # 2. å•é¡Œã‚µãƒ³ãƒ—ãƒ«åˆ†æ
        if self.problematic_samples:
            self._analyze_problematic_samples()
        
        # 3. æ”¹å–„ææ¡ˆ
        suggestions = self._generate_improvement_suggestions(val_loss)
        
        # 4. çµ±è¨ˆä¿å­˜
        self._save_epoch_statistics(epoch, val_loss)
        
        # 5. å¯è¦–åŒ–
        if epoch % 5 == 0:  # 5ã‚¨ãƒãƒƒã‚¯ã”ã¨
            self._create_diagnostic_plots(epoch)
        
        return suggestions
    
    def _summarize_detection_stats(self, epoch):
        """æ¤œå‡ºçµ±è¨ˆã®ã‚µãƒãƒªãƒ¼"""
        detections = self.current_epoch_detections
        
        # å…¨ãƒãƒƒãƒã®çµ±è¨ˆã‚’é›†ç´„
        total_preds = sum(d['total_predictions'] for d in detections)
        avg_conf = np.mean([d['conf_mean'] for d in detections])
        max_conf = max(d['conf_max'] for d in detections)
        total_high_conf = sum(d['conf_above_05'] for d in detections)
        total_super_conf = sum(d['conf_above_07'] for d in detections)
        
        print(f"ğŸ¯ æ¤œå‡ºçµ±è¨ˆã‚µãƒãƒªãƒ¼:")
        print(f"   ç·äºˆæ¸¬æ•°: {total_preds:,}")
        print(f"   å¹³å‡ä¿¡é ¼åº¦: {avg_conf:.3f}")
        print(f"   æœ€å¤§ä¿¡é ¼åº¦: {max_conf:.3f}")
        print(f"   é«˜ä¿¡é ¼åº¦æ¤œå‡º: {total_high_conf} ({100*total_high_conf/total_preds:.2f}%)")
        print(f"   è¶…é«˜ä¿¡é ¼åº¦æ¤œå‡º: {total_super_conf} ({100*total_super_conf/total_preds:.2f}%)")
        
        # ã‚¨ãƒãƒƒã‚¯çµ±è¨ˆã¨ã—ã¦è¨˜éŒ²
        epoch_stats = {
            'epoch': epoch,
            'total_predictions': total_preds,
            'avg_confidence': avg_conf,
            'max_confidence': max_conf,
            'high_conf_detections': total_high_conf,
            'super_conf_detections': total_super_conf,
            'high_conf_rate': total_high_conf / total_preds if total_preds > 0 else 0
        }
        
        self.detection_stats['epoch_stats'].append(epoch_stats)
    
    def _analyze_problematic_samples(self):
        """å•é¡Œã‚µãƒ³ãƒ—ãƒ«ã®åˆ†æ"""
        if not self.problematic_samples:
            print(f"âœ… å•é¡Œã‚µãƒ³ãƒ—ãƒ«: ãªã—")
            return
        
        print(f"âš ï¸ å•é¡Œã‚µãƒ³ãƒ—ãƒ«åˆ†æ: {len(self.problematic_samples)}ä»¶")
        
        # å•é¡Œã‚¿ã‚¤ãƒ—åˆ¥é›†è¨ˆ
        problem_types = defaultdict(int)
        for sample in self.problematic_samples:
            for prob_type in sample['problem_types']:
                problem_types[prob_type] += 1
        
        print(f"   å•é¡Œã‚¿ã‚¤ãƒ—åˆ¥:")
        for prob_type, count in problem_types.items():
            print(f"     {prob_type}: {count}ä»¶")
        
        # æœ€ã‚‚å•é¡Œã®å¤§ããªã‚µãƒ³ãƒ—ãƒ«
        worst_samples = sorted(
            self.problematic_samples, 
            key=lambda x: x['max_confidence'] if 'low_confidence' in str(x['problem_types']) else -x['max_confidence']
        )[:3]
        
        print(f"   æœ€é‡è¦å•é¡Œã‚µãƒ³ãƒ—ãƒ«:")
        for i, sample in enumerate(worst_samples, 1):
            print(f"     {i}. Batch {sample['batch_idx']}, Sample {sample['sample_idx']}")
            print(f"        Targetæ•°: {sample['target_count']}, æœ€å¤§ä¿¡é ¼åº¦: {sample['max_confidence']:.3f}")
            print(f"        å•é¡Œ: {', '.join(sample['problem_types'])}")
    
    def _generate_improvement_suggestions(self, val_loss):
        """æ”¹å–„ææ¡ˆã‚’ç”Ÿæˆ"""
        suggestions = []
        
        # æœ€æ–°çµ±è¨ˆ
        if self.detection_stats['epoch_stats']:
            latest = self.detection_stats['epoch_stats'][-1]
            
            # ä¿¡é ¼åº¦ãŒä½ã™ãã‚‹
            if latest['max_confidence'] < 0.3:
                suggestions.append({
                    'type': 'critical',
                    'issue': 'æœ€å¤§ä¿¡é ¼åº¦ãŒç•°å¸¸ã«ä½ã„',
                    'suggestion': 'å­¦ç¿’ç‡ã‚’2å€ã«å¢—åŠ ã€ã¾ãŸã¯ã‚¢ãƒ³ã‚«ãƒ¼ã‚µã‚¤ã‚ºè¦‹ç›´ã—'
                })
            
            # é«˜ä¿¡é ¼åº¦æ¤œå‡ºãŒã»ã¼ãªã„
            if latest['high_conf_rate'] < 0.01:
                suggestions.append({
                    'type': 'important',
                    'issue': 'é«˜ä¿¡é ¼åº¦æ¤œå‡ºãŒ1%æœªæº€',
                    'suggestion': 'lossé‡ã¿ã®objé …ã‚’å¢—åŠ ã€ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã‚’è»½æ¸›'
                })
            
            # Val LossãŒé«˜æ­¢ã¾ã‚Š
            if val_loss > 40:
                suggestions.append({
                    'type': 'important',
                    'issue': 'Val Lossé«˜æ­¢ã¾ã‚Š',
                    'suggestion': 'å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°è¦‹ç›´ã—ã€æ­£å‰‡åŒ–è»½æ¸›'
                })
        
        # å•é¡Œã‚µãƒ³ãƒ—ãƒ«ãŒå¤šã„
        if len(self.problematic_samples) > 20:
            suggestions.append({
                'type': 'warning',
                'issue': 'å•é¡Œã‚µãƒ³ãƒ—ãƒ«ãŒå¤šã™ãã‚‹',
                'suggestion': 'ãƒ‡ãƒ¼ã‚¿å“è³ªç¢ºèªã€ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è¦‹ç›´ã—'
            })
        
        return suggestions
    
    def _save_epoch_statistics(self, epoch, val_loss):
        """çµ±è¨ˆæƒ…å ±ã‚’JSONã§ä¿å­˜"""
        stats_file = os.path.join(self.save_dir, f"epoch_{epoch}_stats.json")
        
        save_data = {
            'epoch': epoch,
            'val_loss': val_loss,
            'timestamp': datetime.now().isoformat(),
            'detection_stats': self.detection_stats['epoch_stats'][-1] if self.detection_stats['epoch_stats'] else {},
            'problematic_samples_count': len(self.problematic_samples),
            'loss_components': {k: v[-10:] for k, v in self.detection_stats['loss_components'].items()}  # æœ€æ–°10ä»¶
        }
        
        with open(stats_file, 'w') as f:
            json.dump(save_data, f, indent=2)
    
    def _create_diagnostic_plots(self, epoch):
        """è¨ºæ–­ç”¨ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ"""
        if not self.detection_stats['epoch_stats']:
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle(f'Diagnostic Plots - Epoch {epoch}', fontsize=16)
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        epochs = [s['epoch'] for s in self.detection_stats['epoch_stats']]
        avg_confs = [s['avg_confidence'] for s in self.detection_stats['epoch_stats']]
        max_confs = [s['max_confidence'] for s in self.detection_stats['epoch_stats']]
        high_conf_rates = [s['high_conf_rate'] for s in self.detection_stats['epoch_stats']]
        
        # 1. ä¿¡é ¼åº¦æ¨ç§»
        axes[0, 0].plot(epochs, avg_confs, 'b-', label='Average', linewidth=2)
        axes[0, 0].plot(epochs, max_confs, 'r-', label='Maximum', linewidth=2)
        axes[0, 0].axhline(y=0.5, color='green', linestyle='--', alpha=0.7, label='Target (0.5)')
        axes[0, 0].set_title('Confidence Trends')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Confidence')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. é«˜ä¿¡é ¼åº¦æ¤œå‡ºç‡
        axes[0, 1].plot(epochs, [r*100 for r in high_conf_rates], 'g-', linewidth=2)
        axes[0, 1].set_title('High Confidence Detection Rate')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Rate (%)')
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. æœ€æ–°ã®ä¿¡é ¼åº¦åˆ†å¸ƒ
        if self.current_epoch_detections:
            latest_dist = self.current_epoch_detections[-1]['conf_distribution']
            bin_edges = np.linspace(0, 1, 11)
            bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
            axes[1, 0].bar(bin_centers, latest_dist, width=0.08, alpha=0.7)
            axes[1, 0].set_title('Current Confidence Distribution')
            axes[1, 0].set_xlabel('Confidence')
            axes[1, 0].set_ylabel('Count')
            axes[1, 0].grid(True, alpha=0.3)
        
        # 4. æå¤±æˆåˆ†æ¨ç§»
        if self.detection_stats['loss_components']:
            for loss_type, values in self.detection_stats['loss_components'].items():
                if values:
                    axes[1, 1].plot(values[-20:], label=loss_type, linewidth=2)  # æœ€æ–°20ä»¶
            axes[1, 1].set_title('Loss Components')
            axes[1, 1].set_xlabel('Recent Batches')
            axes[1, 1].set_ylabel('Loss Value')
            axes[1, 1].legend()
            axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜
        plot_file = os.path.join(self.save_dir, f"diagnostic_plots_epoch_{epoch}.png")
        plt.savefig(plot_file, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š è¨ºæ–­ã‚°ãƒ©ãƒ•ä¿å­˜: {plot_file}")
    
    def generate_final_report(self):
        """æœ€çµ‚è¨ºæ–­ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        print(f"\n{'='*80}")
        print(f"ğŸ“‹ æœ€çµ‚è¨ºæ–­ãƒ¬ãƒãƒ¼ãƒˆ")
        print(f"{'='*80}")
        
        if not self.detection_stats['epoch_stats']:
            print("âš ï¸ çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
            return
        
        # å…¨ä½“çš„ãªå‚¾å‘
        first_epoch = self.detection_stats['epoch_stats'][0]
        last_epoch = self.detection_stats['epoch_stats'][-1]
        
        print(f"ğŸ” å­¦ç¿’é€²æ—åˆ†æ:")
        print(f"   ã‚¨ãƒãƒƒã‚¯æ•°: {first_epoch['epoch']} â†’ {last_epoch['epoch']}")
        print(f"   å¹³å‡ä¿¡é ¼åº¦: {first_epoch['avg_confidence']:.3f} â†’ {last_epoch['avg_confidence']:.3f}")
        print(f"   æœ€å¤§ä¿¡é ¼åº¦: {first_epoch['max_confidence']:.3f} â†’ {last_epoch['max_confidence']:.3f}")
        print(f"   é«˜ä¿¡é ¼åº¦ç‡: {first_epoch['high_conf_rate']:.1%} â†’ {last_epoch['high_conf_rate']:.1%}")
        
        # æ”¹å–„åº¦è©•ä¾¡
        conf_improvement = last_epoch['max_confidence'] - first_epoch['max_confidence']
        rate_improvement = last_epoch['high_conf_rate'] - first_epoch['high_conf_rate']
        
        print(f"\nğŸ“ˆ æ”¹å–„åº¦è©•ä¾¡:")
        if conf_improvement > 0.1:
            print(f"   âœ… ä¿¡é ¼åº¦æ”¹å–„: +{conf_improvement:.3f} (è‰¯å¥½)")
        elif conf_improvement > 0.05:
            print(f"   ğŸŸ¡ ä¿¡é ¼åº¦æ”¹å–„: +{conf_improvement:.3f} (æ™®é€š)")
        else:
            print(f"   âŒ ä¿¡é ¼åº¦æ”¹å–„: +{conf_improvement:.3f} (è¦æ”¹å–„)")
        
        if rate_improvement > 0.01:
            print(f"   âœ… æ¤œå‡ºç‡æ”¹å–„: +{rate_improvement:.1%} (è‰¯å¥½)")
        elif rate_improvement > 0.005:
            print(f"   ğŸŸ¡ æ¤œå‡ºç‡æ”¹å–„: +{rate_improvement:.1%} (æ™®é€š)")
        else:
            print(f"   âŒ æ¤œå‡ºç‡æ”¹å–„: +{rate_improvement:.1%} (è¦æ”¹å–„)")
        
        # æœ€çµ‚æ¨å¥¨äº‹é …
        print(f"\nğŸ¯ æœ€çµ‚æ¨å¥¨äº‹é …:")
        if last_epoch['max_confidence'] < 0.3:
            print(f"   ğŸš¨ ç·Šæ€¥: å­¦ç¿’ç‡ã‚’2-3å€ã«å¢—åŠ ")
            print(f"   ğŸš¨ ç·Šæ€¥: ã‚¢ãƒ³ã‚«ãƒ¼ã‚µã‚¤ã‚ºã®å…¨é¢è¦‹ç›´ã—")
        elif last_epoch['max_confidence'] < 0.5:
            print(f"   âš ï¸ é‡è¦: æå¤±é–¢æ•°ã®é‡ã¿èª¿æ•´")
            print(f"   âš ï¸ é‡è¦: ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã®è»½æ¸›")
        else:
            print(f"   âœ… é †èª¿: ç¾åœ¨ã®è¨­å®šã‚’ç¶™ç¶š")
            print(f"   ğŸ’¡ ææ¡ˆ: ã‚ˆã‚Šé•·æœŸçš„ãªå­¦ç¿’ã‚’æ¤œè¨")
        
        print(f"{'='*80}\n")


# çµ±åˆç”¨ã®é–¢æ•°
def integrate_diagnostic_training(original_training_function):
    """æ—¢å­˜ã®å­¦ç¿’é–¢æ•°ã«è¨ºæ–­æ©Ÿèƒ½ã‚’çµ±åˆ"""
    
    def enhanced_training_function(model, train_dataloader, val_dataloader, criterion, cfg, architecture_type):
        """è¨ºæ–­æ©Ÿèƒ½ä»˜ãå­¦ç¿’é–¢æ•°"""
        
        # è¨ºæ–­å™¨åˆæœŸåŒ–
        diagnostics = DiagnosticTrainer(
            save_dir=os.path.join(cfg.save_dir, "diagnostics")
        )
        
        print(f"ğŸ”¬ è¨ºæ–­æ©Ÿèƒ½çµ±åˆç‰ˆå­¦ç¿’é–‹å§‹")
        print(f"   è¨ºæ–­ãƒ­ã‚°: {diagnostics.save_dir}")
        
        # å…ƒã®å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã‚’ãƒ©ãƒƒãƒ—
        return original_training_function(
            model, train_dataloader, val_dataloader, criterion, cfg, architecture_type,
            diagnostics=diagnostics  # è¨ºæ–­å™¨ã‚’æ¸¡ã™
        )
    
    return enhanced_training_function


# ãƒ†ã‚¹ãƒˆç”¨è¨ºæ–­é–¢æ•°
def quick_diagnosis_test():
    """è¨ºæ–­ã‚·ã‚¹ãƒ†ãƒ ã®å‹•ä½œç¢ºèª"""
    print("ğŸ§ª è¨ºæ–­ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã§è¨ºæ–­æ©Ÿèƒ½ã‚’ãƒ†ã‚¹ãƒˆ
    diagnostics = DiagnosticTrainer("test_diagnostic_logs")
    
    # ãƒ€ãƒŸãƒ¼äºˆæ¸¬ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
    dummy_predictions = {
        'small': torch.randn(2, 100, 20),
        'medium': torch.randn(2, 50, 20), 
        'large': torch.randn(2, 25, 20)
    }
    dummy_targets = [
        torch.tensor([[0, 0.5, 0.5, 0.1, 0.1]]),
        torch.tensor([[1, 0.3, 0.7, 0.2, 0.3]])
    ]
    dummy_images = torch.randn(2, 1, 416, 416)
    
    # ã‚¨ãƒãƒƒã‚¯1ã®ãƒ†ã‚¹ãƒˆ
    diagnostics.start_epoch_diagnosis(1)
    
    for batch_idx in range(5):
        diagnostics.log_batch_diagnosis(
            batch_idx, dummy_images, dummy_targets, dummy_predictions
        )
    
    suggestions = diagnostics.end_epoch_diagnosis(1, 45.0)
    
    print(f"âœ… è¨ºæ–­ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆå®Œäº†")
    print(f"   ææ¡ˆæ•°: {len(suggestions)}")
    for suggestion in suggestions:
        print(f"   {suggestion['type']}: {suggestion['issue']}")
    
    return True


# ç°¡æ˜“ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
def calculate_simple_metrics(model, val_loader, max_batches=10):
    """ç°¡æ˜“çš„ãªæ€§èƒ½ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—"""
    model.eval()
    
    metrics = {
        'val_loss': [],
        'detection_rate': [],
        'confidence_stats': {
            'mean': [], 'max': [], 'above_0.5': [], 'above_0.7': []
        },
        'per_image_detections': []
    }
    
    print(f"ğŸ“Š ç°¡æ˜“ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—é–‹å§‹ (æœ€å¤§{max_batches}ãƒãƒƒãƒ)")
    
    with torch.no_grad():
        for batch_idx, (images, targets) in enumerate(val_loader):
            if batch_idx >= max_batches:
                break
            
            # äºˆæ¸¬
            predictions = model(images)
            
            # ä¿¡é ¼åº¦çµ±è¨ˆ
            if isinstance(predictions, dict):
                all_confs = []
                for scale_preds in predictions.values():
                    confs = torch.sigmoid(scale_preds[..., 4]).cpu().numpy().flatten()
                    all_confs.extend(confs)
            else:
                all_confs = torch.sigmoid(predictions[..., 4]).cpu().numpy().flatten()
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
            metrics['confidence_stats']['mean'].append(np.mean(all_confs))
            metrics['confidence_stats']['max'].append(np.max(all_confs))
            metrics['confidence_stats']['above_0.5'].append(np.sum(np.array(all_confs) > 0.5))
            metrics['confidence_stats']['above_0.7'].append(np.sum(np.array(all_confs) > 0.7))
            
            # ç”»åƒã‚ãŸã‚Šã®é«˜ä¿¡é ¼åº¦æ¤œå‡ºæ•°
            high_conf_per_image = np.sum(np.array(all_confs) > 0.5) / images.shape[0]
            metrics['per_image_detections'].append(high_conf_per_image)
            
            if batch_idx % 3 == 0:
                print(f"   Batch {batch_idx}: å¹³å‡ä¿¡é ¼åº¦={np.mean(all_confs):.3f}, "
                      f"é«˜ä¿¡é ¼åº¦æ¤œå‡º={np.sum(np.array(all_confs) > 0.5)}")
    
    # ã‚µãƒãƒªãƒ¼çµ±è¨ˆ
    summary = {
        'avg_confidence': np.mean(metrics['confidence_stats']['mean']),
        'max_confidence': np.max(metrics['confidence_stats']['max']),
        'total_high_conf': np.sum(metrics['confidence_stats']['above_0.5']),
        'total_super_conf': np.sum(metrics['confidence_stats']['above_0.7']),
        'avg_detections_per_image': np.mean(metrics['per_image_detections'])
    }
    
    print(f"\nğŸ“‹ ç°¡æ˜“ãƒ¡ãƒˆãƒªã‚¯ã‚¹çµæœ:")
    print(f"   å¹³å‡ä¿¡é ¼åº¦: {summary['avg_confidence']:.3f}")
    print(f"   æœ€å¤§ä¿¡é ¼åº¦: {summary['max_confidence']:.3f}")
    print(f"   é«˜ä¿¡é ¼åº¦æ¤œå‡º: {summary['total_high_conf']}")
    print(f"   è¶…é«˜ä¿¡é ¼åº¦æ¤œå‡º: {summary['total_super_conf']}")
    print(f"   å¹³å‡æ¤œå‡ºæ•°/ç”»åƒ: {summary['avg_detections_per_image']:.1f}")
    
    return summary, metrics


if __name__ == "__main__":
    # è¨ºæ–­ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    success = quick_diagnosis_test()
    
    if success:
        print("ğŸ‰ è¨ºæ–­ã‚·ã‚¹ãƒ†ãƒ æº–å‚™å®Œäº†!")
        print("   æ¬¡: train.pyã«çµ±åˆã—ã¦è©³ç´°è¨ºæ–­ã‚’é–‹å§‹")
    else:
        print("âŒ è¨ºæ–­ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ - ä¿®æ­£ãŒå¿…è¦")