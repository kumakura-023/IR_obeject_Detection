# diagnostic_training.py - Fixed JSON serialization + enhanced diagnosis

import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from collections import defaultdict
import os
from datetime import datetime
import json

class DiagnosticTrainer:
    """å­¦ç¿’éç¨‹ã‚’è©³ç´°ã«åˆ†æãƒ»è¨ºæ–­ã™ã‚‹ã‚¯ãƒ©ã‚¹ï¼ˆä¿®æ­£ç‰ˆï¼‰"""
    
    def __init__(self, save_dir="diagnostic_logs"):
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)
        
        # çµ±è¨ˆæƒ…å ±åé›†
        self.detection_stats = {
            'epoch_stats': [],
            'loss_components': defaultdict(list),
            'confidence_distribution': [],
            'class_performance': defaultdict(list),
            'anchor_usage': defaultdict(list),
            'size_distribution': defaultdict(list)
        }
        
        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¨ºæ–­ç”¨
        self.current_epoch_detections = []
        self.problematic_samples = []
        
        print(f"ğŸ”¬ DiagnosticTraineråˆæœŸåŒ–å®Œäº†ï¼ˆä¿®æ­£ç‰ˆï¼‰")
        print(f"   ä¿å­˜å…ˆ: {save_dir}")
        print(f"   è¨ºæ–­é …ç›®: æ¤œå‡ºçµ±è¨ˆã€ä¿¡é ¼åº¦åˆ†å¸ƒã€ã‚¯ãƒ©ã‚¹æ€§èƒ½ã€ã‚¢ãƒ³ã‚«ãƒ¼ä½¿ç”¨ç‡")
        print(f"   ä¿®æ­£ç‚¹: JSON serialization, å½æ¤œå‡ºå¯¾ç­–")
    
    def start_epoch_diagnosis(self, epoch):
        """ã‚¨ãƒãƒƒã‚¯é–‹å§‹æ™‚ã®è¨ºæ–­æº–å‚™"""
        self.current_epoch = epoch
        self.current_epoch_detections = []
        self.current_epoch_losses = []
        
        print(f"\nğŸ” Epoch {epoch} è¨ºæ–­é–‹å§‹")
    
    def log_batch_diagnosis(self, batch_idx, images, targets, predictions, loss_components=None):
        """ãƒãƒƒãƒã”ã¨ã®è©³ç´°è¨ºæ–­ï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        
        try:
            # 1. æ¤œå‡ºæ•°çµ±è¨ˆ
            batch_detections = self._analyze_detections(predictions, targets)
            self.current_epoch_detections.extend(batch_detections)
            
            # 2. æå¤±æˆåˆ†è¨˜éŒ²ï¼ˆJSONå¯¾å¿œï¼‰
            if loss_components:
                json_safe_components = self._make_json_safe(loss_components)
                for key, value in json_safe_components.items():
                    self.detection_stats['loss_components'][key].append(value)
            
            # 3. å•é¡Œã‚µãƒ³ãƒ—ãƒ«ç‰¹å®šï¼ˆæ”¹è‰¯ç‰ˆï¼‰
            problematic = self._identify_problematic_samples(
                images, targets, predictions, batch_idx
            )
            self.problematic_samples.extend(problematic)
            
            # 4. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è­¦å‘Šï¼ˆé »åº¦èª¿æ•´ï¼‰
            if batch_idx % 50 == 0:  # 20 â†’ 50ï¼ˆãƒ­ã‚°é »åº¦å‰Šæ¸›ï¼‰
                self._print_realtime_diagnosis(batch_idx, batch_detections)
                
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–
            if batch_idx % 100 == 0:
                print(f"   âš ï¸ è¨ºæ–­ã‚¨ãƒ©ãƒ¼ (batch {batch_idx}): {str(e)[:100]}...")
    
    def _make_json_safe(self, obj):
        """ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’JSONå®‰å…¨ãªå½¢å¼ã«å¤‰æ›"""
        if isinstance(obj, dict):
            return {k: self._make_json_safe(v) for k, v in obj.items()}
        elif isinstance(obj, (list, tuple)):
            return [self._make_json_safe(item) for item in obj]
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (np.float32, np.float64)):
            return float(obj)
        elif isinstance(obj, (np.int32, np.int64)):
            return int(obj)
        elif torch.is_tensor(obj):
            return float(obj.detach().cpu().item()) if obj.numel() == 1 else obj.detach().cpu().tolist()
        else:
            return obj
    
    def _analyze_detections(self, predictions, targets):
        """æ¤œå‡ºçµæœã®è©³ç´°åˆ†æï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""

        print(f"DEBUG: _analyze_detections ã«æ¸¡ã•ã‚ŒãŸ predictions ã®ã‚¿ã‚¤ãƒ—: {type(predictions)}")
        if torch.is_tensor(predictions):
            print(f"DEBUG: _analyze_detections ã«æ¸¡ã•ã‚ŒãŸ predictions ã®å½¢çŠ¶: {predictions.shape}")
        elif isinstance(predictions, dict):
            print(f"DEBUG: _analyze_detections ã«æ¸¡ã•ã‚ŒãŸ predictions (dict) ã®ã‚­ãƒ¼ã¨å½¢çŠ¶:")
            for k, v in predictions.items():
                print(f"  - {k}: {v.shape}")
        detections = []
        
        try:
            # ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£åˆ¤å®šã‚’è¿½åŠ 
            if isinstance(predictions, dict):
                # ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ã®å ´åˆï¼š1ã¤ã®ã‚¹ã‚±ãƒ¼ãƒ«ã ã‘ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                # å…¨ã‚¹ã‚±ãƒ¼ãƒ«ã‚’åˆè¨ˆã™ã‚‹ã¨äºˆæ¸¬æ•°ãŒç•°å¸¸ã«ãªã‚‹ã‹ã‚‰
                sample_scale = 'medium'  # ä»£è¡¨ã¨ã—ã¦ä¸­ã‚µã‚¤ã‚ºã‚’ä½¿ç”¨
                if sample_scale in predictions:
                    scale_preds = predictions[sample_scale]
                    B, N, C = scale_preds.shape
                    # ãƒãƒƒãƒã®1æšç›®ã®ã¿
                    combined_preds = scale_preds[0].view(-1, C)  # [N, C]
                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    first_scale = list(predictions.keys())[0]
                    scale_preds = predictions[first_scale]
                    combined_preds = scale_preds[0].view(-1, scale_preds.shape[-1])
            else:
                # ã‚·ãƒ³ã‚°ãƒ«ã‚¹ã‚±ãƒ¼ãƒ«ã®å ´åˆ
                combined_preds = predictions[0].view(-1, predictions.shape[-1])
            
            # å‹¾é…ãƒ‡ã‚¿ãƒƒãƒã—ã¦å®‰å…¨ã«å‡¦ç†
            with torch.no_grad():
                # ä¿¡é ¼åº¦æŠ½å‡º
                confidences = torch.sigmoid(combined_preds[:, 4]).cpu().numpy()
                
                # ã‚¯ãƒ©ã‚¹äºˆæ¸¬æŠ½å‡º
                class_probs = torch.softmax(combined_preds[:, 5:], dim=-1).cpu().numpy()
                class_ids = np.argmax(class_probs, axis=-1)
                
                # ç•°å¸¸å€¤æ¤œå‡ºï¼ˆä¿¡é ¼åº¦1.0ã®æ¤œå‡ºï¼‰
                perfect_conf_count = np.sum(confidences >= 0.999)
                
                 # äºˆæ¸¬æ•°ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
                total_preds = len(confidences)
                expected_max = 2000  # ãƒãƒƒãƒã‚µã‚¤ã‚º32 Ã— 13Ã—13ã‚°ãƒªãƒƒãƒ‰ç¨‹åº¦ãŒå¦¥å½“
                
                if total_preds > expected_max:
                    print(f"âš ï¸ ç•°å¸¸ãªäºˆæ¸¬æ•°æ¤œå‡º: {total_preds} (æœŸå¾…å€¤: < {expected_max})")
                    print(f"   â†’ è¨ºæ–­ã‚·ã‚¹ãƒ†ãƒ ã®è¨­å®šãƒŸã‚¹ã€ã¾ãŸã¯ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­å®šãƒŸã‚¹ã®å¯èƒ½æ€§")

                # æ¤œå‡ºçµ±è¨ˆ
                detection_info = {
                    'total_predictions': min(total_preds, expected_max),  # ä¸Šé™è¨­å®š
                    'conf_mean': float(np.mean(confidences)),
                    'conf_std': float(np.std(confidences)),
                    'conf_max': float(np.max(confidences)),
                    'conf_above_05': int(np.sum(confidences > 0.5)),
                    'conf_above_07': int(np.sum(confidences > 0.7)),
                    'conf_above_09': int(np.sum(confidences > 0.9)),
                    'perfect_conf_count': int(perfect_conf_count),  # è¿½åŠ 
                    'conf_distribution': np.histogram(confidences, bins=10, range=(0, 1))[0].tolist(),
                    'class_distribution': np.bincount(class_ids, minlength=15).tolist()
                }
            
            detections.append(detection_info)
            
        except Exception as e:
            print(f"   âš ï¸ æ¤œå‡ºåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯çµ±è¨ˆ
            detections.append({
                'total_predictions': 0,
                'conf_mean': 0.0,
                'conf_std': 0.0,
                'conf_max': 0.0,
                'conf_above_05': 0,
                'conf_above_07': 0,
                'conf_above_09': 0,
                'perfect_conf_count': 0,
                'conf_distribution': [0] * 10,
                'class_distribution': [0] * 15
            })
        
        return detections
    
    def _identify_problematic_samples(self, images, targets, predictions, batch_idx):
        """å•é¡Œã®ã‚ã‚‹ã‚µãƒ³ãƒ—ãƒ«ã‚’ç‰¹å®šï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        problematic = []
        
        try:
            # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å–å¾—
            B = images.shape[0]
            
            for i in range(B):
                # ã“ã®ç”»åƒã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ•°
                target_count = len(targets[i]) if len(targets) > i else 0
                
                # ã“ã®ç”»åƒã®æœ€å¤§ä¿¡é ¼åº¦ï¼ˆå‹¾é…ãƒ‡ã‚¿ãƒƒãƒï¼‰
                with torch.no_grad():
                    if isinstance(predictions, dict):
                        max_conf = 0
                        perfect_conf_count = 0
                        for scale_preds in predictions.values():
                            if i < scale_preds.shape[0]:
                                scale_conf = torch.sigmoid(scale_preds[i, :, 4]).cpu()
                                max_conf = max(max_conf, scale_conf.max().item())
                                perfect_conf_count += (scale_conf >= 0.999).sum().item()
                    else:
                        if i < predictions.shape[0]:
                            scale_conf = torch.sigmoid(predictions[i, :, 4]).cpu()
                            max_conf = scale_conf.max().item()
                            perfect_conf_count = (scale_conf >= 0.999).sum().item()
                        else:
                            max_conf = 0
                            perfect_conf_count = 0
                
                # å•é¡Œåˆ¤å®šï¼ˆæ”¹è‰¯ç‰ˆï¼‰
                is_problematic = False
                problem_type = []
                
                # æ·±åˆ»ãªå•é¡Œ: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãªã—ã§å®Œç’§ãªä¿¡é ¼åº¦
                if target_count == 0 and max_conf >= 0.999:
                    is_problematic = True
                    problem_type.append("perfect_confidence_without_targets")
                # ä¸­ç¨‹åº¦ã®å•é¡Œ: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãªã—ã§é«˜ä¿¡é ¼åº¦
                elif target_count == 0 and max_conf > 0.8:
                    is_problematic = True
                    problem_type.append("high_confidence_without_targets")
                # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚ã‚Šã§ä½ä¿¡é ¼åº¦
                elif target_count > 0 and max_conf < 0.1:
                    is_problematic = True
                    problem_type.append("low_confidence_with_targets")
                # å¤šã‚¿ãƒ¼ã‚²ãƒƒãƒˆã§ä½ä¿¡é ¼åº¦
                elif target_count > 3 and max_conf < 0.3:
                    is_problematic = True
                    problem_type.append("many_targets_low_confidence")
                
                if is_problematic:
                    problematic.append({
                        'batch_idx': int(batch_idx),
                        'sample_idx': int(i),
                        'target_count': int(target_count),
                        'max_confidence': float(max_conf),
                        'perfect_conf_count': int(perfect_conf_count),  # è¿½åŠ 
                        'problem_types': problem_type
                    })
                    
        except Exception as e:
            print(f"   âš ï¸ å•é¡Œã‚µãƒ³ãƒ—ãƒ«åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        
        return problematic
    
    def _print_realtime_diagnosis(self, batch_idx, batch_detections):
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¨ºæ–­æƒ…å ±ã‚’è¡¨ç¤ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        if not batch_detections:
            return
        
        latest = batch_detections[-1]
        
        print(f"ğŸ“Š Batch {batch_idx} è¨ºæ–­:")
        print(f"   äºˆæ¸¬æ•°: {latest['total_predictions']:,}")
        print(f"   å¹³å‡ä¿¡é ¼åº¦: {latest['conf_mean']:.3f}")
        print(f"   æœ€å¤§ä¿¡é ¼åº¦: {latest['conf_max']:.3f}")
        print(f"   é«˜ä¿¡é ¼åº¦(>0.5): {latest['conf_above_05']}")
        print(f"   è¶…é«˜ä¿¡é ¼åº¦(>0.7): {latest['conf_above_07']}")
        
        # æ–°ã—ã„è­¦å‘Š: å®Œç’§ãªä¿¡é ¼åº¦æ¤œå‡º
        if latest['perfect_conf_count'] > 0:
            print(f"   ğŸš¨ å®Œç’§ä¿¡é ¼åº¦æ¤œå‡º: {latest['perfect_conf_count']} (è¦æ³¨æ„)")
        
        # è­¦å‘Š
        if latest['conf_max'] < 0.1:
            print(f"   âš ï¸ è­¦å‘Š: æœ€å¤§ä¿¡é ¼åº¦ãŒç•°å¸¸ã«ä½ã„ ({latest['conf_max']:.3f})")
        if latest['conf_above_05'] == 0:
            print(f"   âš ï¸ è­¦å‘Š: ä¿¡é ¼åº¦0.5ä»¥ä¸Šã®æ¤œå‡ºãŒã‚¼ãƒ­")
        if latest['perfect_conf_count'] > 100:
            print(f"   ğŸš¨ é‡è¦è­¦å‘Š: å®Œç’§ä¿¡é ¼åº¦æ¤œå‡ºãŒå¤šã™ãã‚‹ (å½æ¤œå‡ºã®å¯èƒ½æ€§)")
    
    def end_epoch_diagnosis(self, epoch, val_loss, model=None):
        """ã‚¨ãƒãƒƒã‚¯çµ‚äº†æ™‚ã®ç·åˆè¨ºæ–­ï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        print(f"\nğŸ“‹ Epoch {epoch} ç·åˆè¨ºæ–­")
        print("=" * 60)
        
        # 1. æ¤œå‡ºçµ±è¨ˆã‚µãƒãƒªãƒ¼
        if self.current_epoch_detections:
            self._summarize_detection_stats(epoch)
        
        # 2. å•é¡Œã‚µãƒ³ãƒ—ãƒ«åˆ†æ
        if self.problematic_samples:
            self._analyze_problematic_samples()
        
        # 3. æ”¹å–„ææ¡ˆ
        suggestions = self._generate_improvement_suggestions(val_loss)
        
        # 4. çµ±è¨ˆä¿å­˜ï¼ˆJSONå®‰å…¨ç‰ˆï¼‰
        try:
            self._save_epoch_statistics(epoch, val_loss)
        except Exception as e:
            print(f"   âš ï¸ çµ±è¨ˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        
        # 5. å¯è¦–åŒ–ï¼ˆã‚¨ãƒ©ãƒ¼è€æ€§ï¼‰
        if epoch % 5 == 0:  # 5ã‚¨ãƒãƒƒã‚¯ã”ã¨
            try:
                self._create_diagnostic_plots(epoch)
            except Exception as e:
                print(f"   âš ï¸ å¯è¦–åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        
        return suggestions
    
    def _summarize_detection_stats(self, epoch):
        """æ¤œå‡ºçµ±è¨ˆã®ã‚µãƒãƒªãƒ¼ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        detections = self.current_epoch_detections
        
        # å…¨ãƒãƒƒãƒã®çµ±è¨ˆã‚’é›†ç´„
        total_preds = sum(d['total_predictions'] for d in detections)
        avg_conf = np.mean([d['conf_mean'] for d in detections])
        max_conf = max(d['conf_max'] for d in detections)
        total_high_conf = sum(d['conf_above_05'] for d in detections)
        total_super_conf = sum(d['conf_above_07'] for d in detections)
        total_perfect_conf = sum(d['perfect_conf_count'] for d in detections)  # è¿½åŠ 
        
        print(f"ğŸ¯ æ¤œå‡ºçµ±è¨ˆã‚µãƒãƒªãƒ¼:")
        print(f"   ç·äºˆæ¸¬æ•°: {total_preds:,}")
        print(f"   å¹³å‡ä¿¡é ¼åº¦: {avg_conf:.3f}")
        print(f"   æœ€å¤§ä¿¡é ¼åº¦: {max_conf:.3f}")
        print(f"   é«˜ä¿¡é ¼åº¦æ¤œå‡º: {total_high_conf} ({100*total_high_conf/total_preds:.2f}%)")
        print(f"   è¶…é«˜ä¿¡é ¼åº¦æ¤œå‡º: {total_super_conf} ({100*total_super_conf/total_preds:.2f}%)")
        
        # æ–°ã—ã„çµ±è¨ˆ: å®Œç’§ä¿¡é ¼åº¦
        if total_perfect_conf > 0:
            print(f"   ğŸš¨ å®Œç’§ä¿¡é ¼åº¦æ¤œå‡º: {total_perfect_conf} ({100*total_perfect_conf/total_preds:.2f}%)")
        
        # ã‚¨ãƒãƒƒã‚¯çµ±è¨ˆã¨ã—ã¦è¨˜éŒ²ï¼ˆJSONå®‰å…¨ï¼‰
        epoch_stats = {
            'epoch': int(epoch),
            'total_predictions': int(total_preds),
            'avg_confidence': float(avg_conf),
            'max_confidence': float(max_conf),
            'high_conf_detections': int(total_high_conf),
            'super_conf_detections': int(total_super_conf),
            'perfect_conf_detections': int(total_perfect_conf),  # è¿½åŠ 
            'high_conf_rate': float(total_high_conf / total_preds) if total_preds > 0 else 0.0
        }
        
        self.detection_stats['epoch_stats'].append(epoch_stats)
    
    def _analyze_problematic_samples(self):
        """å•é¡Œã‚µãƒ³ãƒ—ãƒ«ã®åˆ†æï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        if not self.problematic_samples:
            print(f"âœ… å•é¡Œã‚µãƒ³ãƒ—ãƒ«: ãªã—")
            return
        
        print(f"âš ï¸ å•é¡Œã‚µãƒ³ãƒ—ãƒ«åˆ†æ: {len(self.problematic_samples)}ä»¶")
        
        # å•é¡Œã‚¿ã‚¤ãƒ—åˆ¥é›†è¨ˆ
        problem_types = defaultdict(int)
        for sample in self.problematic_samples:
            for prob_type in sample['problem_types']:
                problem_types[prob_type] += 1
        
        print(f"   å•é¡Œã‚¿ã‚¤ãƒ—åˆ¥:")
        for prob_type, count in problem_types.items():
            print(f"     {prob_type}: {count}ä»¶")
        
        # æœ€ã‚‚æ·±åˆ»ãªå•é¡Œï¼ˆå®Œç’§ä¿¡é ¼åº¦ï¼‰
        perfect_conf_problems = [
            s for s in self.problematic_samples 
            if 'perfect_confidence_without_targets' in s['problem_types']
        ]
        
        if perfect_conf_problems:
            print(f"   ğŸš¨ é‡å¤§å•é¡Œï¼ˆå®Œç’§ä¿¡é ¼åº¦ï¼‰: {len(perfect_conf_problems)}ä»¶")
            print(f"      â†’ æå¤±é–¢æ•°ã®é‡ã¿èª¿æ•´ãŒå¿…è¦")
        
        # æœ€ã‚‚å•é¡Œã®å¤§ããªã‚µãƒ³ãƒ—ãƒ«
        worst_samples = sorted(
            self.problematic_samples, 
            key=lambda x: x['max_confidence'] if 'low_confidence' in str(x['problem_types']) else -x['max_confidence']
        )[:3]
        
        print(f"   æœ€é‡è¦å•é¡Œã‚µãƒ³ãƒ—ãƒ«:")
        for i, sample in enumerate(worst_samples, 1):
            print(f"     {i}. Batch {sample['batch_idx']}, Sample {sample['sample_idx']}")
            print(f"        Targetæ•°: {sample['target_count']}, æœ€å¤§ä¿¡é ¼åº¦: {sample['max_confidence']:.3f}")
            if 'perfect_conf_count' in sample:
                print(f"        å®Œç’§ä¿¡é ¼åº¦æ¤œå‡º: {sample['perfect_conf_count']}")
            print(f"        å•é¡Œ: {', '.join(sample['problem_types'])}")
    
    def _generate_improvement_suggestions(self, val_loss):
        """æ”¹å–„ææ¡ˆã‚’ç”Ÿæˆï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        suggestions = []
        
        # æœ€æ–°çµ±è¨ˆ
        if self.detection_stats['epoch_stats']:
            latest = self.detection_stats['epoch_stats'][-1]
            
            # å®Œç’§ä¿¡é ¼åº¦å•é¡Œï¼ˆæœ€å„ªå…ˆï¼‰
            if latest.get('perfect_conf_detections', 0) > 1000:
                suggestions.append({
                    'type': 'critical',
                    'issue': 'å®Œç’§ä¿¡é ¼åº¦æ¤œå‡ºãŒç•°å¸¸ã«å¤šã„ï¼ˆå½æ¤œå‡ºï¼‰',
                    'suggestion': 'lambda_noobj ã‚’2å€ã«å¢—åŠ ã€ä¿¡é ¼åº¦æå¤±ã®é‡ã¿å¼·åŒ–'
                })
            
            # ä¿¡é ¼åº¦ãŒä½ã™ãã‚‹
            if latest['max_confidence'] < 0.3:
                suggestions.append({
                    'type': 'critical',
                    'issue': 'æœ€å¤§ä¿¡é ¼åº¦ãŒç•°å¸¸ã«ä½ã„',
                    'suggestion': 'å­¦ç¿’ç‡ã‚’2å€ã«å¢—åŠ ã€ã¾ãŸã¯ã‚¢ãƒ³ã‚«ãƒ¼ã‚µã‚¤ã‚ºè¦‹ç›´ã—'
                })
            
            # é«˜ä¿¡é ¼åº¦æ¤œå‡ºãŒã»ã¼ãªã„
            if latest['high_conf_rate'] < 0.01:
                suggestions.append({
                    'type': 'important',
                    'issue': 'é«˜ä¿¡é ¼åº¦æ¤œå‡ºãŒ1%æœªæº€',
                    'suggestion': 'lossé‡ã¿ã®objé …ã‚’å¢—åŠ ã€ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã‚’è»½æ¸›'
                })
            
            # Val LossãŒé«˜æ­¢ã¾ã‚Š
            if val_loss > 40:
                suggestions.append({
                    'type': 'important',
                    'issue': 'Val Lossé«˜æ­¢ã¾ã‚Š',
                    'suggestion': 'å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°è¦‹ç›´ã—ã€æ­£å‰‡åŒ–è»½æ¸›'
                })
        
        # å•é¡Œã‚µãƒ³ãƒ—ãƒ«ãŒå¤šã„
        if len(self.problematic_samples) > 20:
            suggestions.append({
                'type': 'warning',
                'issue': 'å•é¡Œã‚µãƒ³ãƒ—ãƒ«ãŒå¤šã™ãã‚‹',
                'suggestion': 'ãƒ‡ãƒ¼ã‚¿å“è³ªç¢ºèªã€ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è¦‹ç›´ã—'
            })
        
        return suggestions
    
    def _save_epoch_statistics(self, epoch, val_loss):
        """çµ±è¨ˆæƒ…å ±ã‚’JSONã§ä¿å­˜ï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        stats_file = os.path.join(self.save_dir, f"epoch_{epoch}_stats.json")
        
        # JSONå®‰å…¨ãªãƒ‡ãƒ¼ã‚¿ä½œæˆ
        save_data = {
            'epoch': int(epoch),
            'val_loss': float(val_loss),
            'timestamp': datetime.now().isoformat(),
            'detection_stats': self.detection_stats['epoch_stats'][-1] if self.detection_stats['epoch_stats'] else {},
            'problematic_samples_count': len(self.problematic_samples),
            'loss_components': {}
        }
        
        # æå¤±æˆåˆ†ï¼ˆæœ€æ–°10ä»¶ã€JSONå®‰å…¨åŒ–ï¼‰
        for k, v in self.detection_stats['loss_components'].items():
            if v:
                safe_values = [self._make_json_safe(val) for val in v[-10:]]
                save_data['loss_components'][k] = safe_values
        
        try:
            with open(stats_file, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, indent=2, ensure_ascii=False)
            print(f"   ğŸ’¾ çµ±è¨ˆä¿å­˜æˆåŠŸ: {stats_file}")
        except Exception as e:
            print(f"   âŒ çµ±è¨ˆä¿å­˜å¤±æ•—: {e}")
    
    def _create_diagnostic_plots(self, epoch):
        """è¨ºæ–­ç”¨ã‚°ãƒ©ãƒ•ã‚’ä½œæˆï¼ˆã‚¨ãƒ©ãƒ¼è€æ€§ç‰ˆï¼‰"""
        if not self.detection_stats['epoch_stats']:
            return
        
        try:
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle(f'Diagnostic Plots - Epoch {epoch}', fontsize=16)
            
            # ãƒ‡ãƒ¼ã‚¿æº–å‚™
            epochs = [s['epoch'] for s in self.detection_stats['epoch_stats']]
            avg_confs = [s['avg_confidence'] for s in self.detection_stats['epoch_stats']]
            max_confs = [s['max_confidence'] for s in self.detection_stats['epoch_stats']]
            high_conf_rates = [s['high_conf_rate'] for s in self.detection_stats['epoch_stats']]
            
            # 1. ä¿¡é ¼åº¦æ¨ç§»
            axes[0, 0].plot(epochs, avg_confs, 'b-', label='Average', linewidth=2)
            axes[0, 0].plot(epochs, max_confs, 'r-', label='Maximum', linewidth=2)
            axes[0, 0].axhline(y=0.5, color='green', linestyle='--', alpha=0.7, label='Target (0.5)')
            axes[0, 0].set_title('Confidence Trends')
            axes[0, 0].set_xlabel('Epoch')
            axes[0, 0].set_ylabel('Confidence')
            axes[0, 0].legend()
            axes[0, 0].grid(True, alpha=0.3)
            
            # 2. é«˜ä¿¡é ¼åº¦æ¤œå‡ºç‡
            axes[0, 1].plot(epochs, [r*100 for r in high_conf_rates], 'g-', linewidth=2)
            axes[0, 1].set_title('High Confidence Detection Rate')
            axes[0, 1].set_xlabel('Epoch')
            axes[0, 1].set_ylabel('Rate (%)')
            axes[0, 1].grid(True, alpha=0.3)
            
            # 3. å®Œç’§ä¿¡é ¼åº¦æ¤œå‡ºï¼ˆæ–°è¦è¿½åŠ ï¼‰
            perfect_confs = [s.get('perfect_conf_detections', 0) for s in self.detection_stats['epoch_stats']]
            if any(perfect_confs):
                axes[1, 0].plot(epochs, perfect_confs, 'r-', linewidth=2)
                axes[1, 0].set_title('Perfect Confidence Detections (WARNING)')
                axes[1, 0].set_xlabel('Epoch')
                axes[1, 0].set_ylabel('Count')
                axes[1, 0].grid(True, alpha=0.3)
            
            # 4. æå¤±æˆåˆ†æ¨ç§»
            if self.detection_stats['loss_components']:
                for loss_type, values in self.detection_stats['loss_components'].items():
                    if values and len(values) > 1:
                        safe_values = [float(v) for v in values[-20:] if v is not None]
                        if safe_values:
                            axes[1, 1].plot(safe_values, label=loss_type, linewidth=2)
                axes[1, 1].set_title('Loss Components')
                axes[1, 1].set_xlabel('Recent Batches')
                axes[1, 1].set_ylabel('Loss Value')
                axes[1, 1].legend()
                axes[1, 1].grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            # ä¿å­˜
            plot_file = os.path.join(self.save_dir, f"diagnostic_plots_epoch_{epoch}.png")
            plt.savefig(plot_file, dpi=150, bbox_inches='tight')
            plt.close()
            
            print(f"ğŸ“Š è¨ºæ–­ã‚°ãƒ©ãƒ•ä¿å­˜: {plot_file}")
            
        except Exception as e:
            print(f"   âš ï¸ å¯è¦–åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
    
    def generate_final_report(self):
        """æœ€çµ‚è¨ºæ–­ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        print(f"\n{'='*80}")
        print(f"ğŸ“‹ æœ€çµ‚è¨ºæ–­ãƒ¬ãƒãƒ¼ãƒˆ")
        print(f"{'='*80}")
        
        if not self.detection_stats['epoch_stats']:
            print("âš ï¸ çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
            return
        
        # å…¨ä½“çš„ãªå‚¾å‘
        first_epoch = self.detection_stats['epoch_stats'][0]
        last_epoch = self.detection_stats['epoch_stats'][-1]
        
        print(f"ğŸ” å­¦ç¿’é€²æ—åˆ†æ:")
        print(f"   ã‚¨ãƒãƒƒã‚¯æ•°: {first_epoch['epoch']} â†’ {last_epoch['epoch']}")
        print(f"   å¹³å‡ä¿¡é ¼åº¦: {first_epoch['avg_confidence']:.3f} â†’ {last_epoch['avg_confidence']:.3f}")
        print(f"   æœ€å¤§ä¿¡é ¼åº¦: {first_epoch['max_confidence']:.3f} â†’ {last_epoch['max_confidence']:.3f}")
        print(f"   é«˜ä¿¡é ¼åº¦ç‡: {first_epoch['high_conf_rate']:.1%} â†’ {last_epoch['high_conf_rate']:.1%}")
        
        # å½æ¤œå‡ºã®è­¦å‘Š
        perfect_conf = last_epoch.get('perfect_conf_detections', 0)
        if perfect_conf > 0:
            print(f"   ğŸš¨ å®Œç’§ä¿¡é ¼åº¦æ¤œå‡º: {perfect_conf} (å½æ¤œå‡ºã®å¯èƒ½æ€§)")
        
        # æ”¹å–„åº¦è©•ä¾¡
        conf_improvement = last_epoch['max_confidence'] - first_epoch['max_confidence']
        rate_improvement = last_epoch['high_conf_rate'] - first_epoch['high_conf_rate']
        
        print(f"\nğŸ“ˆ æ”¹å–„åº¦è©•ä¾¡:")
        if conf_improvement > 0.1:
            print(f"   âœ… ä¿¡é ¼åº¦æ”¹å–„: +{conf_improvement:.3f} (è‰¯å¥½)")
        elif conf_improvement > 0.05:
            print(f"   ğŸŸ¡ ä¿¡é ¼åº¦æ”¹å–„: +{conf_improvement:.3f} (æ™®é€š)")
        else:
            print(f"   âŒ ä¿¡é ¼åº¦æ”¹å–„: +{conf_improvement:.3f} (è¦æ”¹å–„)")
        
        if rate_improvement > 0.01:
            print(f"   âœ… æ¤œå‡ºç‡æ”¹å–„: +{rate_improvement:.1%} (è‰¯å¥½)")
        elif rate_improvement > 0.005:
            print(f"   ğŸŸ¡ æ¤œå‡ºç‡æ”¹å–„: +{rate_improvement:.1%} (æ™®é€š)")
        else:
            print(f"   âŒ æ¤œå‡ºç‡æ”¹å–„: +{rate_improvement:.1%} (è¦æ”¹å–„)")
        
        # æœ€çµ‚æ¨å¥¨äº‹é …
        print(f"\nğŸ¯ æœ€çµ‚æ¨å¥¨äº‹é …:")
        if perfect_conf > 1000:
            print(f"   ğŸš¨ ç·Šæ€¥: å½æ¤œå‡ºå¯¾ç­–ï¼ˆlambda_noobjå¢—åŠ ï¼‰")
        if last_epoch['max_confidence'] < 0.3:
            print(f"   ğŸš¨ ç·Šæ€¥: å­¦ç¿’ç‡ã‚’2-3å€ã«å¢—åŠ ")
            print(f"   ğŸš¨ ç·Šæ€¥: ã‚¢ãƒ³ã‚«ãƒ¼ã‚µã‚¤ã‚ºã®å…¨é¢è¦‹ç›´ã—")
        elif last_epoch['max_confidence'] < 0.5:
            print(f"   âš ï¸ é‡è¦: æå¤±é–¢æ•°ã®é‡ã¿èª¿æ•´")
            print(f"   âš ï¸ é‡è¦: ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã®è»½æ¸›")
        else:
            print(f"   âœ… é †èª¿: ç¾åœ¨ã®è¨­å®šã‚’ç¶™ç¶š")
            print(f"   ğŸ’¡ ææ¡ˆ: ã‚ˆã‚Šé•·æœŸçš„ãªå­¦ç¿’ã‚’æ¤œè¨")
        
        print(f"{'='*80}\n")


# ===== çµ±åˆç”¨ã®é–¢æ•°ï¼ˆä¿®æ­£ç‰ˆï¼‰ =====

def integrate_diagnostic_training(original_training_function):
    """æ—¢å­˜ã®å­¦ç¿’é–¢æ•°ã«è¨ºæ–­æ©Ÿèƒ½ã‚’çµ±åˆï¼ˆä¿®æ­£ç‰ˆï¼‰"""
    
    def enhanced_training_function(model, train_dataloader, val_dataloader, criterion, cfg, architecture_type):
        """è¨ºæ–­æ©Ÿèƒ½ä»˜ãå­¦ç¿’é–¢æ•°ï¼ˆã‚¨ãƒ©ãƒ¼è€æ€§å¼·åŒ–ï¼‰"""
        
        # è¨ºæ–­å™¨åˆæœŸåŒ–
        try:
            diagnostics = DiagnosticTrainer(
                save_dir=os.path.join(cfg.save_dir, "diagnostics")
            )
            
            print(f"ğŸ”¬ è¨ºæ–­æ©Ÿèƒ½çµ±åˆç‰ˆå­¦ç¿’é–‹å§‹ï¼ˆä¿®æ­£ç‰ˆï¼‰")
            print(f"   è¨ºæ–­ãƒ­ã‚°: {diagnostics.save_dir}")
            print(f"   ä¿®æ­£ç‚¹: JSON serialization, å½æ¤œå‡ºå¯¾ç­–")
            
            # å…ƒã®å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã‚’ãƒ©ãƒƒãƒ—
            return original_training_function(
                model, train_dataloader, val_dataloader, criterion, cfg, architecture_type,
                diagnostics=diagnostics  # è¨ºæ–­å™¨ã‚’æ¸¡ã™
            )
            
        except Exception as e:
            print(f"âŒ è¨ºæ–­ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            print("   â†’ è¨ºæ–­æ©Ÿèƒ½ãªã—ã§å­¦ç¿’ç¶™ç¶š")
            return original_training_function(
                model, train_dataloader, val_dataloader, criterion, cfg, architecture_type
            )
    
    return enhanced_training_function


# ===== å½æ¤œå‡ºå¯¾ç­–ç”¨è¨­å®šèª¿æ•´é–¢æ•° =====

def suggest_config_fixes(diagnostic_results):
    """è¨ºæ–­çµæœã«åŸºã¥ãè¨­å®šä¿®æ­£ææ¡ˆ"""
    
    suggestions = {
        'config_changes': {},
        'priority': 'medium',
        'explanation': ''
    }
    
    if not diagnostic_results:
        return suggestions
    
    latest_stats = diagnostic_results.get('epoch_stats', [])
    if not latest_stats:
        return suggestions
    
    latest = latest_stats[-1]
    
    # å½æ¤œå‡ºï¼ˆå®Œç’§ä¿¡é ¼åº¦ï¼‰å¯¾ç­–
    perfect_conf = latest.get('perfect_conf_detections', 0)
    total_preds = latest.get('total_predictions', 1)
    perfect_rate = perfect_conf / total_preds if total_preds > 0 else 0
    
    if perfect_rate > 0.01:  # 1%ä»¥ä¸ŠãŒå®Œç’§ä¿¡é ¼åº¦
        suggestions['config_changes'].update({
            'lambda_noobj': 1.0,  # 0.3 â†’ 1.0 (èƒŒæ™¯ã¸ã®ç½°å‰‡å¼·åŒ–)
            'lambda_obj': 1.5,    # 2.0 â†’ 1.5 (ç‰©ä½“ä¿¡é ¼åº¦ã‚’ç›¸å¯¾çš„ã«ä¸‹ã’ã‚‹)
            'learning_rate': 3e-4,  # 5e-4 â†’ 3e-4 (å­¦ç¿’ã‚’å®‰å®šåŒ–)
        })
        suggestions['priority'] = 'critical'
        suggestions['explanation'] = f'å½æ¤œå‡ºç‡{perfect_rate:.1%}ã¯å±é™ºãƒ¬ãƒ™ãƒ«ã€‚èƒŒæ™¯ç½°å‰‡ã‚’å¼·åŒ–ã€‚'
    
    # ä½ä¿¡é ¼åº¦å¯¾ç­–
    max_conf = latest.get('max_confidence', 0)
    if max_conf < 0.3:
        suggestions['config_changes'].update({
            'learning_rate': 8e-4,  # ã‚ˆã‚Šç©æ¥µçš„ãªå­¦ç¿’ç‡
            'lambda_coord': 15.0,   # åº§æ¨™å­¦ç¿’ã‚’ã•ã‚‰ã«å¼·åŒ–
        })
        suggestions['priority'] = 'high'
        suggestions['explanation'] += f' æœ€å¤§ä¿¡é ¼åº¦{max_conf:.3f}ã¯ä½ã™ãã‚‹ã€‚å­¦ç¿’å¼·åŒ–ãŒå¿…è¦ã€‚'
    
    return suggestions


# ===== ãƒ†ã‚¹ãƒˆç”¨è¨ºæ–­é–¢æ•°ï¼ˆä¿®æ­£ç‰ˆï¼‰ =====

def quick_diagnosis_test():
    """è¨ºæ–­ã‚·ã‚¹ãƒ†ãƒ ã®å‹•ä½œç¢ºèªï¼ˆä¿®æ­£ç‰ˆï¼‰"""
    print("ğŸ§ª è¨ºæ–­ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆé–‹å§‹ï¼ˆä¿®æ­£ç‰ˆï¼‰")
    
    try:
        # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã§è¨ºæ–­æ©Ÿèƒ½ã‚’ãƒ†ã‚¹ãƒˆ
        diagnostics = DiagnosticTrainer("test_diagnostic_logs")
        
        # ãƒ€ãƒŸãƒ¼äºˆæ¸¬ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
        dummy_predictions = {
            'small': torch.randn(2, 100, 20),
            'medium': torch.randn(2, 50, 20), 
            'large': torch.randn(2, 25, 20)
        }
        dummy_targets = [
            torch.tensor([[0, 0.5, 0.5, 0.1, 0.1]]),
            torch.tensor([[1, 0.3, 0.7, 0.2, 0.3]])
        ]
        dummy_images = torch.randn(2, 1, 416, 416)
        
        # ã‚¨ãƒãƒƒã‚¯1ã®ãƒ†ã‚¹ãƒˆ
        diagnostics.start_epoch_diagnosis(1)
        
        for batch_idx in range(3):  # 5 â†’ 3ï¼ˆãƒ†ã‚¹ãƒˆçŸ­ç¸®ï¼‰
            diagnostics.log_batch_diagnosis(
                batch_idx, dummy_images, dummy_targets, dummy_predictions
            )
        
        suggestions = diagnostics.end_epoch_diagnosis(1, 45.0)
        
        print(f"âœ… è¨ºæ–­ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆå®Œäº†ï¼ˆä¿®æ­£ç‰ˆï¼‰")
        print(f"   ææ¡ˆæ•°: {len(suggestions)}")
        for suggestion in suggestions:
            print(f"   {suggestion['type']}: {suggestion['issue']}")
        
        # JSONä¿å­˜ãƒ†ã‚¹ãƒˆ
        test_config = suggest_config_fixes({'epoch_stats': diagnostics.detection_stats['epoch_stats']})
        print(f"   è¨­å®šææ¡ˆ: {test_config['config_changes']}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False


# ===== ç°¡æ˜“ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ï¼ˆä¿®æ­£ç‰ˆï¼‰ =====

def calculate_simple_metrics(model, val_loader, max_batches=10):
    """ç°¡æ˜“çš„ãªæ€§èƒ½ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—ï¼ˆã‚¨ãƒ©ãƒ¼è€æ€§å¼·åŒ–ï¼‰"""
    model.eval()
    
    metrics = {
        'val_loss': [],
        'detection_rate': [],
        'confidence_stats': {
            'mean': [], 'max': [], 'above_0.5': [], 'above_0.7': [], 'perfect_count': []
        },
        'per_image_detections': []
    }
    
    print(f"ğŸ“Š ç°¡æ˜“ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—é–‹å§‹ (æœ€å¤§{max_batches}ãƒãƒƒãƒ)")
    
    with torch.no_grad():
        try:
            for batch_idx, (images, targets) in enumerate(val_loader):
                if batch_idx >= max_batches:
                    break
                
                # äºˆæ¸¬
                predictions = model(images)
                
                # ä¿¡é ¼åº¦çµ±è¨ˆï¼ˆã‚¨ãƒ©ãƒ¼è€æ€§ï¼‰
                try:
                    if isinstance(predictions, dict):
                        all_confs = []
                        for scale_preds in predictions.values():
                            confs = torch.sigmoid(scale_preds[..., 4]).cpu().numpy().flatten()
                            all_confs.extend(confs)
                    else:
                        all_confs = torch.sigmoid(predictions[..., 4]).cpu().numpy().flatten()
                    
                    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
                    metrics['confidence_stats']['mean'].append(float(np.mean(all_confs)))
                    metrics['confidence_stats']['max'].append(float(np.max(all_confs)))
                    metrics['confidence_stats']['above_0.5'].append(int(np.sum(np.array(all_confs) > 0.5)))
                    metrics['confidence_stats']['above_0.7'].append(int(np.sum(np.array(all_confs) > 0.7)))
                    metrics['confidence_stats']['perfect_count'].append(int(np.sum(np.array(all_confs) >= 0.999)))
                    
                    # ç”»åƒã‚ãŸã‚Šã®é«˜ä¿¡é ¼åº¦æ¤œå‡ºæ•°
                    high_conf_per_image = np.sum(np.array(all_confs) > 0.5) / images.shape[0]
                    metrics['per_image_detections'].append(float(high_conf_per_image))
                    
                except Exception as e:
                    print(f"   âš ï¸ ãƒãƒƒãƒ{batch_idx}ã§ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
                
                if batch_idx % 3 == 0:
                    avg_conf = np.mean(all_confs) if 'all_confs' in locals() else 0
                    high_conf_count = np.sum(np.array(all_confs) > 0.5) if 'all_confs' in locals() else 0
                    print(f"   Batch {batch_idx}: å¹³å‡ä¿¡é ¼åº¦={avg_conf:.3f}, "
                          f"é«˜ä¿¡é ¼åº¦æ¤œå‡º={high_conf_count}")
        
        except Exception as e:
            print(f"âŒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return None, None
    
    # ã‚µãƒãƒªãƒ¼çµ±è¨ˆ
    try:
        summary = {
            'avg_confidence': float(np.mean(metrics['confidence_stats']['mean'])) if metrics['confidence_stats']['mean'] else 0.0,
            'max_confidence': float(np.max(metrics['confidence_stats']['max'])) if metrics['confidence_stats']['max'] else 0.0,
            'total_high_conf': int(np.sum(metrics['confidence_stats']['above_0.5'])),
            'total_super_conf': int(np.sum(metrics['confidence_stats']['above_0.7'])),
            'total_perfect_conf': int(np.sum(metrics['confidence_stats']['perfect_count'])),
            'avg_detections_per_image': float(np.mean(metrics['per_image_detections'])) if metrics['per_image_detections'] else 0.0
        }
        
        print(f"\nğŸ“‹ ç°¡æ˜“ãƒ¡ãƒˆãƒªã‚¯ã‚¹çµæœ:")
        print(f"   å¹³å‡ä¿¡é ¼åº¦: {summary['avg_confidence']:.3f}")
        print(f"   æœ€å¤§ä¿¡é ¼åº¦: {summary['max_confidence']:.3f}")
        print(f"   é«˜ä¿¡é ¼åº¦æ¤œå‡º: {summary['total_high_conf']}")
        print(f"   è¶…é«˜ä¿¡é ¼åº¦æ¤œå‡º: {summary['total_super_conf']}")
        print(f"   å®Œç’§ä¿¡é ¼åº¦æ¤œå‡º: {summary['total_perfect_conf']}")
        print(f"   å¹³å‡æ¤œå‡ºæ•°/ç”»åƒ: {summary['avg_detections_per_image']:.1f}")
        
        # è­¦å‘Š
        if summary['total_perfect_conf'] > 100:
            print(f"   ğŸš¨ è­¦å‘Š: å®Œç’§ä¿¡é ¼åº¦æ¤œå‡ºãŒå¤šã™ãã¾ã™ï¼ˆå½æ¤œå‡ºã®å¯èƒ½æ€§ï¼‰")
        
        return summary, metrics
        
    except Exception as e:
        print(f"âŒ ã‚µãƒãƒªãƒ¼è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
        return None, None


if __name__ == "__main__":
    # è¨ºæ–­ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    success = quick_diagnosis_test()
    
    if success:
        print("ğŸ‰ è¨ºæ–­ã‚·ã‚¹ãƒ†ãƒ æº–å‚™å®Œäº†ï¼ˆä¿®æ­£ç‰ˆï¼‰!")
        print("   ä¿®æ­£ç‚¹:")
        print("     âœ… JSON serialization ã‚¨ãƒ©ãƒ¼ä¿®æ­£")
        print("     âœ… å½æ¤œå‡ºï¼ˆå®Œç’§ä¿¡é ¼åº¦ï¼‰ã®æ¤œå‡ºãƒ»å¯¾ç­–")
        print("     âœ… ã‚¨ãƒ©ãƒ¼è€æ€§å¼·åŒ–")
        print("     âœ… è¨­å®šä¿®æ­£ææ¡ˆæ©Ÿèƒ½")
        print("   æ¬¡: train.pyã«çµ±åˆã—ã¦å®‰å®šã—ãŸè¨ºæ–­ã‚’é–‹å§‹")
    else:
        print("âŒ è¨ºæ–­ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ - ä¿®æ­£ãŒå¿…è¦")