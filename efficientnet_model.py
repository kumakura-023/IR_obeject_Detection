import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision.models import efficientnet_b1

# EfficientNet-B1 Backboneï¼ˆãƒãƒ£ãƒ³ãƒãƒ«æ•°ä¿®æ­£ç‰ˆï¼‰
class EfficientNetBackbone(nn.Module):
    def __init__(self, in_channels=1, pretrained=True):
        super().__init__()
        # EfficientNet-B1ã‚’ãƒ­ãƒ¼ãƒ‰
        efficientnet = efficientnet_b1(pretrained=pretrained)
        
        # 1chå¯¾å¿œï¼šæœ€åˆã®convå±¤ã‚’å¤‰æ›´
        self.conv_stem = nn.Conv2d(in_channels, 32, kernel_size=3, stride=2, padding=1, bias=False)
        if pretrained:
            # RGBã®é‡ã¿ã‚’è¼åº¦å¤‰æ›ã§1chã«é©å¿œ
            rgb_weights = efficientnet.features[0][0].weight.data
            self.conv_stem.weight.data = (0.299 * rgb_weights[:, 0:1, :, :] + 
                                        0.587 * rgb_weights[:, 1:2, :, :] + 
                                        0.114 * rgb_weights[:, 2:3, :, :])
        
        # EfficientNetã®ç‰¹å¾´ã‚’ä¿å­˜
        self.features = efficientnet.features
        self.features[0][0] = self.conv_stem  # æœ€åˆã®å±¤ã‚’ç½®ãæ›ãˆ
        
        # EfficientNetã®æ§‹é€ ã‚’èª¿æŸ»ã—ã¦ãƒ‡ãƒãƒƒã‚°
        print(f">> EfficientNet-B1 backbone initialized")
        self._print_feature_info()

    def _print_feature_info(self):
        """å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®å‡ºåŠ›ãƒãƒ£ãƒ³ãƒãƒ«æ•°ã‚’èª¿æŸ»"""
        print("\n=== EfficientNet-B1 Layer Structure ===")
        x = torch.randn(1, 1, 640, 512)
        
        for i, layer in enumerate(self.features):
            x = layer(x)
            print(f"Layer {i}: {layer.__class__.__name__} -> output shape: {x.shape}")

    def forward(self, x):
        # print(f">> Backbone input: {x.shape}")
        
        # EfficientNet-B1ã®å®Ÿéš›ã®æ§‹é€ ã«åŸºã¥ã„ã¦ç‰¹å¾´ã‚’æŠ½å‡º
        # MBConvãƒ–ãƒ­ãƒƒã‚¯ã®å‡ºåŠ›ã‚’ä½¿ç”¨
        features = []
        
        for i, layer in enumerate(self.features):
            x = layer(x)
            
            # EfficientNet-B1ã®ç‰¹å¾´æŠ½å‡ºãƒã‚¤ãƒ³ãƒˆï¼ˆå®Ÿéš›ã®ãƒãƒ£ãƒ³ãƒãƒ«æ•°ã«åŸºã¥ãï¼‰
            if i == 1:  # æœ€åˆã®MBConvãƒ–ãƒ­ãƒƒã‚¯å¾Œï¼ˆ16ãƒãƒ£ãƒ³ãƒãƒ«ï¼‰
                feat1 = x
                # print(f">> Feature 1 (Layer {i}): {feat1.shape}")
            elif i == 3:  # ä¸­é–“ã®MBConvãƒ–ãƒ­ãƒƒã‚¯å¾Œï¼ˆ40ãƒãƒ£ãƒ³ãƒãƒ«ï¼‰
                feat2 = x  
                # print(f">> Feature 2 (Layer {i}): {feat2.shape}")
            elif i == 5:  # ã‚ˆã‚Šæ·±ã„MBConvãƒ–ãƒ­ãƒƒã‚¯å¾Œï¼ˆ80ãƒãƒ£ãƒ³ãƒãƒ«ï¼‰
                feat3 = x
                # print(f">> Feature 3 (Layer {i}): {feat3.shape}")
        
        return feat1, feat2, feat3

# å‹•çš„FPNå®Ÿè£…ï¼ˆãƒãƒ£ãƒ³ãƒãƒ«æ•°ã‚’è‡ªå‹•èª¿æ•´ï¼‰
class AdaptiveFPN(nn.Module):
    def __init__(self, out_channels=256):
        super().__init__()
        self.out_channels = out_channels
        
        # EfficientNet-B1ã®å®Ÿéš›ã®ãƒãƒ£ãƒ³ãƒãƒ«æ•°ã«å¯¾å¿œ
        # å®Ÿè¡Œæ™‚ã«å‹•çš„ã«æ±ºå®šã•ã‚Œã‚‹ãŒã€å…¸å‹çš„ãªå€¤ã‚’è¨­å®š
        self.in_channels = [16, 40, 80]  # EfficientNet-B1ã®å…¸å‹çš„ãªãƒãƒ£ãƒ³ãƒãƒ«æ•°
        
        # Lateral connections
        self.lateral_convs = nn.ModuleList([
            nn.Conv2d(in_ch, out_channels, 1) for in_ch in self.in_channels
        ])
        
        # Output convolutions
        self.output_convs = nn.ModuleList([
            nn.Conv2d(out_channels, out_channels, 3, padding=1) for _ in self.in_channels
        ])
        
        print(f">> AdaptiveFPN initialized for channels: {self.in_channels}")

    def forward(self, feat1, feat2, feat3):
        # print(f">> FPN input shapes: {feat1.shape}, {feat2.shape}, {feat3.shape}")
        
        # å®Ÿéš›ã®å…¥åŠ›ãƒãƒ£ãƒ³ãƒãƒ«æ•°ã‚’ç¢ºèª
        actual_channels = [feat1.size(1), feat2.size(1), feat3.size(1)]
        
        # æœŸå¾…å€¤ã¨ç•°ãªã‚‹å ´åˆã¯å‹•çš„ã«èª¿æ•´
        if actual_channels != self.in_channels:
            print(f">> Adjusting FPN for actual channels: {actual_channels}")
            self.in_channels = actual_channels
            
            # ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’å†ä½œæˆ
            self.lateral_convs = nn.ModuleList([
                nn.Conv2d(in_ch, self.out_channels, 1).to(feat1.device) 
                for in_ch in actual_channels
            ])
            self.output_convs = nn.ModuleList([
                nn.Conv2d(self.out_channels, self.out_channels, 3, padding=1).to(feat1.device)
                for _ in actual_channels
            ])
        
        # Lateral connections
        p1 = self.lateral_convs[0](feat1)
        p2 = self.lateral_convs[1](feat2)
        p3 = self.lateral_convs[2](feat3)
        
        # Top-down pathwayï¼ˆã‚µã‚¤ã‚ºã‚’å‹•çš„ã«èª¿æ•´ï¼‰
        h2, w2 = p2.shape[2], p2.shape[3]
        h1, w1 = p1.shape[2], p1.shape[3]
        
        # p3ã‚’p2ã®ã‚µã‚¤ã‚ºã«ãƒªã‚µã‚¤ã‚º
        p2 = p2 + F.interpolate(p3, size=(h2, w2), mode='nearest')
        
        # p2ã‚’p1ã®ã‚µã‚¤ã‚ºã«ãƒªã‚µã‚¤ã‚º  
        p1 = p1 + F.interpolate(p2, size=(h1, w1), mode='nearest')
        
        # Final output convolutions
        p1 = self.output_convs[0](p1)
        p2 = self.output_convs[1](p2)
        p3 = self.output_convs[2](p3)
        
        # print(f">> FPN output shapes: {p1.shape}, {p2.shape}, {p3.shape}")
        
        return p1, p2, p3

# Detection Headï¼ˆä¿®æ­£ç‰ˆï¼‰
class SafeDetectionHead(nn.Module):
    def __init__(self, in_channels=256, num_classes=15, num_anchors=3):
        super().__init__()
        self.num_outputs = 5 + num_classes
        self.num_anchors = num_anchors
        self.num_classes = num_classes
        
        # Shared convolutions
        self.shared_conv = nn.Sequential(
            nn.Conv2d(in_channels, in_channels, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels, in_channels, 3, padding=1),
            nn.ReLU(inplace=True),
        )
        
        # å„ã‚¹ã‚±ãƒ¼ãƒ«ç”¨ã®Headï¼ˆ1ã¤ã®Headã‚’å…±æœ‰ï¼‰
        self.detection_head = nn.Conv2d(in_channels, num_anchors * self.num_outputs, 1)
        
        # é‡ã¿åˆæœŸåŒ–
        self._initialize_weights()

    def _initialize_weights(self):
        """é‡ã¿ã¨ãƒã‚¤ã‚¢ã‚¹ã®åˆæœŸåŒ–"""
        print("ğŸ”§ Initializing SafeDetectionHead weights...")
        
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.normal_(m.weight, 0, 0.01)
                if m.bias is not None:
                    # é€šå¸¸ã®åˆæœŸåŒ–
                    nn.init.constant_(m.bias, 0)
        
        # Detection head ã®Objectness biasã‚’ç‰¹åˆ¥ã«åˆæœŸåŒ–
        if hasattr(self, 'detection_head') and self.detection_head.bias is not None:
            bias_shape = self.detection_head.bias.shape[0]
            outputs_per_anchor = self.num_outputs  # 5 + num_classes
            
            print(f"   Setting objectness bias to -4.0 for {self.num_anchors} anchors")
            
            # Objectness biasã‚’-2.0ã«å¤‰æ›´ï¼ˆsigmoid(-2.0) â‰ˆ 0.12ï¼‰
        for anchor_idx in range(self.num_anchors):
            obj_idx = anchor_idx * outputs_per_anchor + 4
            if obj_idx < bias_shape:
                self.detection_head.bias.data[obj_idx] = -2.0  # -4.0 â†’ -2.0
        
        print("âœ… Weight initialization completed")

    def _reshape_output(self, x):
        B, _, H, W = x.shape
        x = x.permute(0, 2, 3, 1).contiguous()
        return x.view(B, H * W * self.num_anchors, self.num_outputs)

    def forward(self, p1, p2, p3):
        # Shared processing
        p1 = self.shared_conv(p1)
        p2 = self.shared_conv(p2) 
        p3 = self.shared_conv(p3)
        
        # Head outputsï¼ˆåŒã˜headã‚’ä½¿ç”¨ï¼‰
        out_p1 = self._reshape_output(self.detection_head(p1))
        out_p2 = self._reshape_output(self.detection_head(p2))
        out_p3 = self._reshape_output(self.detection_head(p3))
        
        # Concatenate all outputs
        out = torch.cat([out_p1, out_p2, out_p3], dim=1)

        # â˜…â˜…â˜… æ ¹æœ¬çš„ãªä¿®æ­£ï¼šæ´»æ€§åŒ–é–¢æ•°ã‚’é©ç”¨ã›ãšã€ç”Ÿã®logitsã‚’è¿”ã™ â˜…â˜…â˜…
        # æå¤±è¨ˆç®—ã«å¿…è¦ãªã®ã¯ç”Ÿã®äºˆæ¸¬å€¤ã®ãŸã‚ã€ã“ã“ã§ã¯ä½•ã‚‚ã›ãšãã®ã¾ã¾è¿”ã™ã€‚
        return out

# EfficientNet Detection Modelï¼ˆä¿®æ­£ç‰ˆï¼‰
class EfficientNetDetectionModel(nn.Module):
    def __init__(self, in_channels=1, num_classes=15, num_anchors=3, pretrained_backbone=True):
        super().__init__()
        print("ğŸš€ Creating EfficientNetDetectionModel...")
        
        self.backbone = EfficientNetBackbone(in_channels=in_channels, pretrained=pretrained_backbone)
        self.neck = AdaptiveFPN(out_channels=256)
        self.head = SafeDetectionHead(in_channels=256, num_classes=num_classes, num_anchors=num_anchors)
        
        # Dropout for regularization
        self.dropout = nn.Dropout2d(0.1)
        
        print("âœ… EfficientNetDetectionModel created successfully")

    def forward(self, x):
        # print(f">> Model input: {x.shape}")
        
        # Backboneç‰¹å¾´æŠ½å‡º
        feat1, feat2, feat3 = self.backbone(x)
        
        # Dropouté©ç”¨
        feat1 = self.dropout(feat1)
        feat2 = self.dropout(feat2) 
        feat3 = self.dropout(feat3)
        
        # Neckå‡¦ç†
        p1, p2, p3 = self.neck(feat1, feat2, feat3)
        
        # Detection head
        output = self.head(p1, p2, p3)
        # print(f">> Model output: {output.shape}")
        
        return output

# ãƒ¢ãƒ‡ãƒ«ä½œæˆé–¢æ•°
def create_efficientnet_model(num_classes=15, pretrained=True):
    """EfficientNetãƒ™ãƒ¼ã‚¹ã®æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ"""
    print(f"ğŸ—ï¸ Creating EfficientNet model (classes={num_classes}, pretrained={pretrained})")
    
    model = EfficientNetDetectionModel(
        in_channels=1,
        num_classes=num_classes,
        num_anchors=3,
        pretrained_backbone=pretrained
    )
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ç¢ºèª
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"ğŸ“Š Model Statistics:")
    print(f"   Total parameters: {total_params:,}")
    print(f"   Trainable parameters: {trainable_params:,}")
    print(f"   vs ResNet50 (~25M): {total_params/25_000_000:.1f}x")
    
    return model

# ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ†ã‚¹ãƒˆé–¢æ•°
def debug_efficientnet_structure():
    """EfficientNetã®æ§‹é€ ã‚’è©³ç´°ã«èª¿æŸ»"""
    print("=== Debugging EfficientNet-B1 Structure ===")
    
    # EfficientNet-B1ã‚’ãƒ­ãƒ¼ãƒ‰
    efficientnet = efficientnet_b1(pretrained=False)
    
    # ãƒ†ã‚¹ãƒˆå…¥åŠ›
    x = torch.randn(1, 3, 640, 512)
    
    print("\n=== Layer-by-layer analysis ===")
    features = []
    
    # å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’é€šéã•ã›ãªãŒã‚‰å½¢çŠ¶ã‚’è¨˜éŒ²
    for i, layer in enumerate(efficientnet.features):
        x = layer(x)
        print(f"Layer {i} ({layer.__class__.__name__}): {x.shape}")
        features.append((i, x.shape))
    
    print("\n=== Suitable feature extraction points ===")
    # é©åˆ‡ãªç‰¹å¾´æŠ½å‡ºãƒã‚¤ãƒ³ãƒˆã‚’ææ¡ˆ
    for i, shape in features:
        if shape[1] in [16, 24, 40, 80, 112, 192, 320]:  # EfficientNetã®å…¸å‹çš„ãªãƒãƒ£ãƒ³ãƒãƒ«æ•°
            print(f"Layer {i}: channels={shape[1]}, spatial={shape[2]}x{shape[3]}")

# ãƒ†ã‚¹ãƒˆé–¢æ•°
def test_model_creation():
    """ãƒ¢ãƒ‡ãƒ«ä½œæˆã‚’ãƒ†ã‚¹ãƒˆ"""
    print("ğŸ§ª Testing model creation...")
    
    try:
        model = create_efficientnet_model(num_classes=15)
        
        # ãƒ†ã‚¹ãƒˆç”¨å…¥åŠ›
        x = torch.randn(2, 1, 640, 512)
        print(f"ğŸ“¥ Input shape: {x.shape}")
        
        with torch.no_grad():
            output = model(x)
            print(f"ğŸ“¤ Output shape: {output.shape}")
            print(f"âœ… Model test successful!")
            
        return model
        
    except Exception as e:
        print(f"âŒ Model test failed: {e}")
        import traceback
        traceback.print_exc()
        return None

# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨åˆ†
if __name__ == "__main__":
    print("=== Testing Corrected EfficientNet Model ===")
    
    # ã¾ãšæ§‹é€ ã‚’èª¿æŸ»
    debug_efficientnet_structure()
    
    print("\n\n=== Creating Detection Model ===")
    model = test_model_creation()
    
    if model is not None:
        print("\nğŸ‰ All tests passed!")
    else:
        print("\nğŸ’¥ Tests failed!")