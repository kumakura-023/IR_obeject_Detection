{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNdd30AvgmeEZMq23WMosgF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8tUzuXgIjHRq","executionInfo":{"status":"ok","timestamp":1749459424170,"user_tz":-540,"elapsed":9359,"user":{"displayName":"ã¿ã¯","userId":"00152872231100665214"}},"outputId":"9bb09db7-8d19-4af6-b13d-d8f687885fbd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Found checkpoints: []\n"]}],"source":["# Google Driveãƒã‚¦ãƒ³ãƒˆ\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç§»å‹•\n","import os\n","os.chdir('/content/drive/MyDrive/IR_obj_detction/new/IR_obeject_Detection')\n","\n","# ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç¢ºèª\n","import glob\n","checkpoints = glob.glob('saved_models/checkpoint_epoch_*.pth')\n","print(f\"Found checkpoints: {[os.path.basename(f) for f in checkpoints]}\")"]},{"cell_type":"code","source":["# ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ\n","!mkdir -p /content/FLIR_YOLO_local/images/train\n","!mkdir -p /content/FLIR_YOLO_local/labels/train\n","\n","# Google Driveã‹ã‚‰ãƒ­ãƒ¼ã‚«ãƒ«ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã‚³ãƒ”ãƒ¼ (æ™‚é–“ã¯ã‹ã‹ã‚Šã¾ã™)\n","# ãƒ‘ã‚¹ã¯å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«åˆã‚ã›ã¦ãã ã•ã„\n","!echo \"Copying training images...\"\n","!cp -r /content/drive/MyDrive/old/EfficientNet_Project/data/FLIR_YOLO/images/train/* /content/FLIR_YOLO_local/images/train/\n","!echo \"Copying training labels...\"\n","!cp -r /content/drive/MyDrive/old/EfficientNet_Project/data/FLIR_YOLO/labels/train/* /content/FLIR_YOLO_local/labels/train/\n","!echo \"Copy complete!\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"puxvQzu4jOSX","executionInfo":{"status":"ok","timestamp":1749460039647,"user_tz":-540,"elapsed":615468,"user":{"displayName":"ã¿ã¯","userId":"00152872231100665214"}},"outputId":"18528735-958f-4b4f-c350-8e4a004c358e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Copying training images...\n","Copying training labels...\n","Copy complete!\n"]}]},{"cell_type":"code","source":["# æœ€æ–°ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰è‡ªå‹•å†é–‹\n","exec(open('unified_training.py').read())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qtK-nEMjjPxq","outputId":"7099d945-e309-49d0-a72a-2c891b58d3d0","executionInfo":{"status":"ok","timestamp":1749460102018,"user_tz":-540,"elapsed":62370,"user":{"displayName":"ã¿ã¯","userId":"00152872231100665214"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","============================================================\n","ğŸ“‹ Unified Training System v2.6 - Version 1.0.0\n","â° Loaded: 2025-06-09 09:07:33\n","ğŸ”— File Hash: 2d7cae99\n","ğŸ“ Recent Modifications (4):\n","   1. 09:07:33 - ç©æ¥µçš„ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—æ©Ÿèƒ½è¿½åŠ \n","   2. 09:07:33 - float32çµ±ä¸€ã§AMPå®Œå…¨ç„¡åŠ¹åŒ–\n","   3. 09:07:33 - OOMã‚¨ãƒ©ãƒ¼è‡ªå‹•å›å¾©æ©Ÿèƒ½è¿½åŠ \n","============================================================\n","\n","ğŸ”§ Memory-optimized config:\n","   Batch size: 8\n","   Accumulation steps: 8\n","   Effective batch size: 64\n","   Memory check interval: 10\n","ğŸš€ Starting Unified EfficientNet Training with Comprehensive Debug\n","ğŸ“± Device: cuda\n","ğŸ¯ Target: 15 classes\n","ğŸ“ Input size: (640, 512)\n","ver 1.5-gemini - Corrected for Latest Project Structure\n","ğŸ§ª Testing model creation...\n","\n","============================================================\n","ğŸ“‹ EfficientNet Model System v1.3 - Version 1.0.0\n","â° Loaded: 2025-06-09 09:07:31\n","ğŸ”— File Hash: 0739c4bf\n","ğŸ“ Recent Modifications (3):\n","   1. 09:07:31 - AdaptiveFPN ãƒãƒ£ãƒ³ãƒãƒ«æ•°è‡ªå‹•èª¿æ•´\n","   2. 09:07:31 - SafeDetectionHead é‡ã¿åˆæœŸåŒ–æ”¹å–„\n","   3. 09:07:31 - float32æ˜ç¤ºçš„å¤‰æ›è¿½åŠ \n","============================================================\n","\n","ğŸ—ï¸ [MODEL v1.0.0] Creating EfficientNet model (classes=15, pretrained=True)\n","ğŸš€ Creating EfficientNetDetectionModel...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B1_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B1_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/efficientnet_b1_rwightman-bac287d4.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b1_rwightman-bac287d4.pth\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30.1M/30.1M [00:00<00:00, 74.5MB/s]\n"]},{"output_type":"stream","name":"stdout","text":[">> EfficientNet-B1 backbone initialized\n","\n","=== EfficientNet-B1 Layer Structure ===\n","Layer 0: Conv2dNormActivation -> output shape: torch.Size([1, 32, 320, 256])\n","Layer 1: Sequential -> output shape: torch.Size([1, 16, 320, 256])\n","Layer 2: Sequential -> output shape: torch.Size([1, 24, 160, 128])\n","Layer 3: Sequential -> output shape: torch.Size([1, 40, 80, 64])\n","Layer 4: Sequential -> output shape: torch.Size([1, 80, 40, 32])\n","Layer 5: Sequential -> output shape: torch.Size([1, 112, 40, 32])\n","Layer 6: Sequential -> output shape: torch.Size([1, 192, 20, 16])\n","Layer 7: Sequential -> output shape: torch.Size([1, 320, 20, 16])\n","Layer 8: Conv2dNormActivation -> output shape: torch.Size([1, 1280, 20, 16])\n",">> AdaptiveFPN initialized for channels: [16, 40, 80]\n","ğŸ”§ Initializing SafeDetectionHead weights...\n","   Setting objectness bias to -4.0 for 3 anchors\n","âœ… Weight initialization completed\n","âœ… EfficientNetDetectionModel created successfully\n","ğŸ“Š Model Statistics:\n","   Total parameters: 9,514,012\n","   Trainable parameters: 9,514,012\n","   vs ResNet50 (~25M): 0.4x\n","ğŸ“¥ Input shape: torch.Size([2, 1, 640, 512])\n",">> Adjusting FPN for actual channels: [16, 40, 112]\n","ğŸ“¤ Output shape: torch.Size([2, 264960, 20])\n","âœ… Model test successful!\n","âœ… ãƒ¢ãƒ‡ãƒ«ä½œæˆæˆåŠŸï¼\n","ğŸ”§ AMPï¼ˆè‡ªå‹•æ··åˆç²¾åº¦ï¼‰ã‚’å®Œå…¨ç„¡åŠ¹åŒ–ã—ã¾ã™\n","\n","============================================================\n","ğŸ“‹ EfficientNet Model System v1.3 - Version 1.0.0\n","â° Loaded: 2025-06-09 09:07:31\n","ğŸ”— File Hash: 0739c4bf\n","ğŸ“ Recent Modifications (3):\n","   1. 09:07:31 - AdaptiveFPN ãƒãƒ£ãƒ³ãƒãƒ«æ•°è‡ªå‹•èª¿æ•´\n","   2. 09:07:31 - SafeDetectionHead é‡ã¿åˆæœŸåŒ–æ”¹å–„\n","   3. 09:07:31 - float32æ˜ç¤ºçš„å¤‰æ›è¿½åŠ \n","============================================================\n","\n","ğŸ—ï¸ [MODEL v1.0.0] Creating EfficientNet model (classes=15, pretrained=True)\n","ğŸš€ Creating EfficientNetDetectionModel...\n",">> EfficientNet-B1 backbone initialized\n","\n","=== EfficientNet-B1 Layer Structure ===\n","Layer 0: Conv2dNormActivation -> output shape: torch.Size([1, 32, 320, 256])\n","Layer 1: Sequential -> output shape: torch.Size([1, 16, 320, 256])\n","Layer 2: Sequential -> output shape: torch.Size([1, 24, 160, 128])\n","Layer 3: Sequential -> output shape: torch.Size([1, 40, 80, 64])\n","Layer 4: Sequential -> output shape: torch.Size([1, 80, 40, 32])\n","Layer 5: Sequential -> output shape: torch.Size([1, 112, 40, 32])\n","Layer 6: Sequential -> output shape: torch.Size([1, 192, 20, 16])\n","Layer 7: Sequential -> output shape: torch.Size([1, 320, 20, 16])\n","Layer 8: Conv2dNormActivation -> output shape: torch.Size([1, 1280, 20, 16])\n",">> AdaptiveFPN initialized for channels: [16, 40, 80]\n","ğŸ”§ Initializing SafeDetectionHead weights...\n","   Setting objectness bias to -4.0 for 3 anchors\n","âœ… Weight initialization completed\n","âœ… EfficientNetDetectionModel created successfully\n","ğŸ“Š Model Statistics:\n","   Total parameters: 9,514,012\n","   Trainable parameters: 9,514,012\n","   vs ResNet50 (~25M): 0.4x\n","ğŸ”§ Model dtype: torch.float32\n","\n","=== ğŸ“š Dataset Loading ===\n","âœ… Dataset: 10742 samples\n","\n","=== ğŸ¯ Anchor Optimization ===\n","ğŸ“Š Analyzing dataset statistics from 1000 samples...\n","   Progress: 100/1000 (10.0%) - 0.3s\n","   Progress: 200/1000 (20.0%) - 0.5s\n","   Progress: 300/1000 (30.0%) - 0.8s\n","   Progress: 400/1000 (40.0%) - 1.0s\n","   Progress: 500/1000 (50.0%) - 1.2s\n","   Progress: 600/1000 (60.0%) - 1.4s\n","   Progress: 700/1000 (70.0%) - 1.7s\n","   Progress: 800/1000 (80.0%) - 1.9s\n","   Progress: 900/1000 (90.0%) - 2.1s\n","âœ… Analysis completed: 15220 valid boxes found in 2.4s\n","\n","ğŸ“ˆ Dataset Statistics:\n","   Total boxes: 15220\n","   Avg size: 24.7 x 29.9 pixels\n","   Size distribution: Small=11854, Medium=2824, Large=542\n","\n","ğŸ”§ Generating optimized anchors...\n","ğŸ”§ Generating 9 anchors using K-means++ (max 300 iterations)...\n","ğŸ‰ Generated optimized anchors:\n","   Level 0: [(9, 10), (14, 26), (38, 26)]\n","   Level 1: [(24, 53), (74, 52), (45, 106)]\n","   Level 2: [(128, 86), (98, 219), (203, 144)]\n","âœ… Anchor optimization completed in 1.6s\n","âœ… Anchor optimization completed\n","\n","=== ğŸ“Š Anchor Comparison ===\n","\n","ğŸ†š Comparing anchor sets...\n","\n","--- Anchor Set 1 (Default) ---\n","ğŸ“ Evaluating anchor quality with 500 samples...\n","ğŸ“ˆ Evaluating anchor performance...\n","   å¹³å‡IoU: 0.5698 (Â±0.1806)\n","   50%ã‚«ãƒãƒ¬ãƒƒã‚¸: 69.82%\n","   70%ã‚«ãƒãƒ¬ãƒƒã‚¸: 23.26%\n","   90%ã‚«ãƒãƒ¬ãƒƒã‚¸: 2.63%\n","   æœªä½¿ç”¨ã‚¢ãƒ³ã‚«ãƒ¼: 0/9\n","\n","--- Anchor Set 2 (Optimized) ---\n","ğŸ“ Evaluating anchor quality with 500 samples...\n","ğŸ“ˆ Evaluating anchor performance...\n","   å¹³å‡IoU: 0.5849 (Â±0.1693)\n","   50%ã‚«ãƒãƒ¬ãƒƒã‚¸: 70.50%\n","   70%ã‚«ãƒãƒ¬ãƒƒã‚¸: 25.36%\n","   90%ã‚«ãƒãƒ¬ãƒƒã‚¸: 2.15%\n","   æœªä½¿ç”¨ã‚¢ãƒ³ã‚«ãƒ¼: 0/9\n","\n","ğŸ“Š Improvement Summary:\n","   Mean IoU: +0.0151\n","   50% Coverage: +0.68%\n","   70% Coverage: +2.10%\n","   âš ï¸ Limited improvement. Consider different strategies.\n","ğŸ“¦ Batches per epoch: 1343\n","ğŸ“Š Progress will be shown every 10 batches\n","\n","=== ğŸ¤– Model Initialization ===\n","\n","============================================================\n","ğŸ“‹ EfficientNet Model System v1.3 - Version 1.0.0\n","â° Loaded: 2025-06-09 09:07:31\n","ğŸ”— File Hash: 0739c4bf\n","ğŸ“ Recent Modifications (3):\n","   1. 09:07:31 - AdaptiveFPN ãƒãƒ£ãƒ³ãƒãƒ«æ•°è‡ªå‹•èª¿æ•´\n","   2. 09:07:31 - SafeDetectionHead é‡ã¿åˆæœŸåŒ–æ”¹å–„\n","   3. 09:07:31 - float32æ˜ç¤ºçš„å¤‰æ›è¿½åŠ \n","============================================================\n","\n","ğŸ—ï¸ [MODEL v1.0.0] Creating EfficientNet model (classes=15, pretrained=True)\n","ğŸš€ Creating EfficientNetDetectionModel...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B1_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B1_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":[">> EfficientNet-B1 backbone initialized\n","\n","=== EfficientNet-B1 Layer Structure ===\n","Layer 0: Conv2dNormActivation -> output shape: torch.Size([1, 32, 320, 256])\n","Layer 1: Sequential -> output shape: torch.Size([1, 16, 320, 256])\n","Layer 2: Sequential -> output shape: torch.Size([1, 24, 160, 128])\n","Layer 3: Sequential -> output shape: torch.Size([1, 40, 80, 64])\n","Layer 4: Sequential -> output shape: torch.Size([1, 80, 40, 32])\n","Layer 5: Sequential -> output shape: torch.Size([1, 112, 40, 32])\n","Layer 6: Sequential -> output shape: torch.Size([1, 192, 20, 16])\n","Layer 7: Sequential -> output shape: torch.Size([1, 320, 20, 16])\n","Layer 8: Conv2dNormActivation -> output shape: torch.Size([1, 1280, 20, 16])\n",">> AdaptiveFPN initialized for channels: [16, 40, 80]\n","ğŸ”§ Initializing SafeDetectionHead weights...\n","   Setting objectness bias to -4.0 for 3 anchors\n","âœ… Weight initialization completed\n","âœ… EfficientNetDetectionModel created successfully\n","ğŸ“Š Model Statistics:\n","   Total parameters: 9,514,012\n","   Trainable parameters: 9,514,012\n","   vs ResNet50 (~25M): 0.4x\n","\n","=== ğŸ“ Anchor & Grid Setup ===\n",">> Adjusting FPN for actual channels: [16, 40, 112]\n","\n","=== ğŸ¯ Loss Function Setup ===\n","\n","============================================================\n","ğŸ“‹ Unified Loss System v2.3 - Version 1.0.0\n","â° Loaded: 2025-06-09 09:07:32\n","ğŸ”— File Hash: 2749e9b8\n","ğŸ“ Recent Modifications (3):\n","   1. 09:07:33 - SimOTAå¯¾å¿œæå¤±é–¢æ•°å®Ÿè£…\n","   2. 09:07:33 - å‹•çš„é‡ã¿èª¿æ•´æ©Ÿèƒ½è¿½åŠ \n","   3. 09:07:33 - ãƒ‡ãƒ¼ã‚¿å‹çµ±ä¸€ (float32)\n","============================================================\n","\n","ğŸ¯ Creating Enhanced Loss with strategy: balanced (weights: {'lambda_box': 5.0, 'lambda_obj': 1.0, 'lambda_cls': 1.0})\n","âœ… EnhancedDetectionLoss (SimOTA-compatible) initialized.\n","âœ… Anchor and grid info prepared - Points: torch.Size([88320, 2]), Strides: torch.Size([88320, 1])\n","\n","=== ğŸ¯ Loss Function Setup ===\n","\n","============================================================\n","ğŸ“‹ Unified Loss System v2.3 - Version 1.0.0\n","â° Loaded: 2025-06-09 09:07:32\n","ğŸ”— File Hash: 2749e9b8\n","ğŸ“ Recent Modifications (3):\n","   1. 09:07:33 - SimOTAå¯¾å¿œæå¤±é–¢æ•°å®Ÿè£…\n","   2. 09:07:33 - å‹•çš„é‡ã¿èª¿æ•´æ©Ÿèƒ½è¿½åŠ \n","   3. 09:07:33 - ãƒ‡ãƒ¼ã‚¿å‹çµ±ä¸€ (float32)\n","============================================================\n","\n","ğŸ¯ Creating Enhanced Loss with strategy: balanced (weights: {'lambda_box': 5.0, 'lambda_obj': 1.0, 'lambda_cls': 1.0})\n","âœ… EnhancedDetectionLoss (SimOTA-compatible) initialized.\n","\n","=== âš™ï¸ Optimizer Setup ===\n","\n","ğŸ¬ Training Started with Bug Fixes!\n","\n","================================================================================\n","ğŸ¯ Epoch 1/30 | Stage: Head+Neck Only\n","ğŸ“¦ Total batches: 1343 | Batch size: 8\n","âš™ï¸ Accumulation steps: 8\n","================================================================================\n","ğŸ§¹ Memory cleanup: 9 objects | GPU: 0.1GB alloc, 0.2GB reserved\n","\n","============================================================\n","ğŸ“‹ Unified Targets System v3.4-clean-fix\n","â° Clean Fix Applied: 2025-06-09 16:00\n","ğŸ“ Fixed: Variable scope, duplicate code, naming issues\n","============================================================\n","\n","ğŸ”§ [TARGETS v3.4-clean-fix] Processing: torch.Size([8, 264960, 20])\n","ğŸ”§ [v3.4-clean-fix] Batch 0: candidates = 87\n","ğŸ‰ [v3.4-clean-fix] Cost matrix computed: torch.Size([10, 87])\n","ğŸ”§ [LOSS] decode_predictions called\n","   self.anchor_info keys: ['anchors_pixel_per_level', 'strides_per_level', 'grid_sizes', 'input_size']\n","   Generating anchor_points from grid_sizes\n","   Generated anchor_points: torch.Size([264960, 2])\n","   Generated strides from strides_per_level: torch.Size([264960])\n","âœ… [LOSS] decode_predictions successful: torch.Size([8, 264960, 4])\n","   ğŸš¨ ABNORMAL OBJ LOSS DETECTED: 2291.08\n","      Box: 1.9744\n","      Cls: 10.2355\n","      Positive samples: 119\n","      â†’ Emergency LR reduction to 1.00e-04\n","   ğŸ“Š Batch    1/1343 (  0.1%)\n","                        ğŸ“‰ Loss: T=2311.1912 B=1.9744 O=2291.0835 C=10.2355\n","                        âš™ï¸ LR: 1.00e-04 | ETA: 72.5min | Mem: 0.1GB\n","                        âœ… Success: 0 | âŒ Errors: 0 | ğŸš¨ OOM: 0\n","ğŸ§¹ Memory cleanup: 0 objects | GPU: 0.2GB alloc, 0.4GB reserved\n","ğŸ”§ [TARGETS v3.4-clean-fix] Processing: torch.Size([8, 264960, 20])\n","ğŸ”§ [v3.4-clean-fix] Batch 0: candidates = 30\n","ğŸ‰ [v3.4-clean-fix] Cost matrix computed: torch.Size([5, 30])\n","ğŸ”§ [LOSS] decode_predictions called\n","   self.anchor_info keys: ['anchors_pixel_per_level', 'strides_per_level', 'grid_sizes', 'input_size']\n","   Generating anchor_points from grid_sizes\n","   Generated anchor_points: torch.Size([264960, 2])\n","   Generated strides from strides_per_level: torch.Size([264960])\n","âœ… [LOSS] decode_predictions successful: torch.Size([8, 264960, 4])\n","   ğŸš¨ ABNORMAL OBJ LOSS DETECTED: 2114.42\n","      Box: 1.9770\n","      Cls: 10.2523\n","      Positive samples: 129\n","      â†’ Emergency LR reduction to 1.00e-05\n","ğŸ”§ [TARGETS v3.4-clean-fix] Processing: torch.Size([8, 264960, 20])\n","ğŸ”§ [v3.4-clean-fix] Batch 0: candidates = 90\n","ğŸ‰ [v3.4-clean-fix] Cost matrix computed: torch.Size([14, 90])\n","ğŸ”§ [LOSS] decode_predictions called\n","   self.anchor_info keys: ['anchors_pixel_per_level', 'strides_per_level', 'grid_sizes', 'input_size']\n","   Generating anchor_points from grid_sizes\n","   Generated anchor_points: torch.Size([264960, 2])\n","   Generated strides from strides_per_level: torch.Size([264960])\n","âœ… [LOSS] decode_predictions successful: torch.Size([8, 264960, 4])\n","   ğŸš¨ ABNORMAL OBJ LOSS DETECTED: 2008.45\n","      Box: 1.9793\n","      Cls: 10.2502\n","      Positive samples: 136\n","ğŸ”§ [TARGETS v3.4-clean-fix] Processing: torch.Size([8, 264960, 20])\n","ğŸ”§ [v3.4-clean-fix] Batch 0: candidates = 120\n","ğŸ‰ [v3.4-clean-fix] Cost matrix computed: torch.Size([17, 120])\n","ğŸ”§ [LOSS] decode_predictions called\n","   self.anchor_info keys: ['anchors_pixel_per_level', 'strides_per_level', 'grid_sizes', 'input_size']\n","   Generating anchor_points from grid_sizes\n","   Generated anchor_points: torch.Size([264960, 2])\n","   Generated strides from strides_per_level: torch.Size([264960])\n","âœ… [LOSS] decode_predictions successful: torch.Size([8, 264960, 4])\n","   ğŸš¨ ABNORMAL OBJ LOSS DETECTED: 1718.33\n","      Box: 1.9769\n","      Cls: 10.2449\n","      Positive samples: 159\n","ğŸ”§ [TARGETS v3.4-clean-fix] Processing: torch.Size([8, 264960, 20])\n","ğŸ”§ [v3.4-clean-fix] Batch 0: candidates = 129\n","ğŸ‰ [v3.4-clean-fix] Cost matrix computed: torch.Size([17, 129])\n","ğŸ”§ [LOSS] decode_predictions called\n","   self.anchor_info keys: ['anchors_pixel_per_level', 'strides_per_level', 'grid_sizes', 'input_size']\n","   Generating anchor_points from grid_sizes\n","   Generated anchor_points: torch.Size([264960, 2])\n","   Generated strides from strides_per_level: torch.Size([264960])\n","âœ… [LOSS] decode_predictions successful: torch.Size([8, 264960, 4])\n","   ğŸš¨ ABNORMAL OBJ LOSS DETECTED: 2372.71\n","      Box: 1.9795\n","      Cls: 10.2346\n","      Positive samples: 115\n","ğŸ”§ [TARGETS v3.4-clean-fix] Processing: torch.Size([8, 264960, 20])\n","ğŸ”§ [v3.4-clean-fix] Batch 0: candidates = 189\n","ğŸ‰ [v3.4-clean-fix] Cost matrix computed: torch.Size([30, 189])\n","ğŸ”§ [LOSS] decode_predictions called\n","   self.anchor_info keys: ['anchors_pixel_per_level', 'strides_per_level', 'grid_sizes', 'input_size']\n","   Generating anchor_points from grid_sizes\n","   Generated anchor_points: torch.Size([264960, 2])\n","   Generated strides from strides_per_level: torch.Size([264960])\n","âœ… [LOSS] decode_predictions successful: torch.Size([8, 264960, 4])\n","   ğŸš¨ ABNORMAL OBJ LOSS DETECTED: 1748.47\n","      Box: 1.9793\n","      Cls: 10.2613\n","      Positive samples: 156\n","ğŸ”§ [TARGETS v3.4-clean-fix] Processing: torch.Size([8, 264960, 20])\n","ğŸ”§ [v3.4-clean-fix] Batch 0: candidates = 162\n","ğŸ‰ [v3.4-clean-fix] Cost matrix computed: torch.Size([22, 162])\n","ğŸ”§ [LOSS] decode_predictions called\n","   self.anchor_info keys: ['anchors_pixel_per_level', 'strides_per_level', 'grid_sizes', 'input_size']\n","   Generating anchor_points from grid_sizes\n","   Generated anchor_points: torch.Size([264960, 2])\n","   Generated strides from strides_per_level: torch.Size([264960])\n","âœ… [LOSS] decode_predictions successful: torch.Size([8, 264960, 4])\n","   ğŸš¨ ABNORMAL OBJ LOSS DETECTED: 2790.35\n","      Box: 1.9806\n","      Cls: 10.2353\n","      Positive samples: 98\n","ğŸ”§ [TARGETS v3.4-clean-fix] Processing: torch.Size([8, 264960, 20])\n","ğŸ”§ [v3.4-clean-fix] Batch 0: candidates = 72\n","ğŸ‰ [v3.4-clean-fix] Cost matrix computed: torch.Size([11, 72])\n","ğŸ”§ [LOSS] decode_predictions called\n","   self.anchor_info keys: ['anchors_pixel_per_level', 'strides_per_level', 'grid_sizes', 'input_size']\n","   Generating anchor_points from grid_sizes\n","   Generated anchor_points: torch.Size([264960, 2])\n","   Generated strides from strides_per_level: torch.Size([264960])\n","âœ… [LOSS] decode_predictions successful: torch.Size([8, 264960, 4])\n","   ğŸš¨ ABNORMAL OBJ LOSS DETECTED: 2220.26\n","      Box: 1.9796\n","      Cls: 10.2542\n","      Positive samples: 123\n","ğŸ”§ [TARGETS v3.4-clean-fix] Processing: torch.Size([8, 264960, 20])\n","ğŸ”§ [v3.4-clean-fix] Batch 0: candidates = 93\n","ğŸ‰ [v3.4-clean-fix] Cost matrix computed: torch.Size([13, 93])\n","ğŸ”§ [LOSS] decode_predictions called\n","   self.anchor_info keys: ['anchors_pixel_per_level', 'strides_per_level', 'grid_sizes', 'input_size']\n","   Generating anchor_points from grid_sizes\n","   Generated anchor_points: torch.Size([264960, 2])\n","   Generated strides from strides_per_level: torch.Size([264960])\n","âœ… [LOSS] decode_predictions successful: torch.Size([8, 264960, 4])\n","   ğŸš¨ ABNORMAL OBJ LOSS DETECTED: 2081.07\n","      Box: 1.9769\n","      Cls: 10.2510\n","      Positive samples: 129\n","ğŸ”§ [TARGETS v3.4-clean-fix] Processing: torch.Size([8, 264960, 20])\n","ğŸ”§ [v3.4-clean-fix] Batch 0: candidates = 156\n","ğŸ‰ [v3.4-clean-fix] Cost matrix computed: torch.Size([22, 156])\n","ğŸ”§ [LOSS] decode_predictions called\n","   self.anchor_info keys: ['anchors_pixel_per_level', 'strides_per_level', 'grid_sizes', 'input_size']\n","   Generating anchor_points from grid_sizes\n","   Generated anchor_points: torch.Size([264960, 2])\n","   Generated strides from strides_per_level: torch.Size([264960])\n","âœ… [LOSS] decode_predictions successful: torch.Size([8, 264960, 4])\n","   ğŸš¨ ABNORMAL OBJ LOSS DETECTED: 2081.14\n","      Box: 1.9818\n","      Cls: 10.2431\n","      Positive samples: 129\n","ğŸ§¹ Memory cleanup: 0 objects | GPU: 0.2GB alloc, 0.4GB reserved\n","ğŸ”§ [TARGETS v3.4-clean-fix] Processing: torch.Size([8, 264960, 20])\n","ğŸ”§ [v3.4-clean-fix] Batch 0: candidates = 144\n","ğŸ‰ [v3.4-clean-fix] Cost matrix computed: torch.Size([22, 144])\n","ğŸ”§ [LOSS] decode_predictions called\n","   self.anchor_info keys: ['anchors_pixel_per_level', 'strides_per_level', 'grid_sizes', 'input_size']\n","   Generating anchor_points from grid_sizes\n","   Generated anchor_points: torch.Size([264960, 2])\n","   Generated strides from strides_per_level: torch.Size([264960])\n","âœ… [LOSS] decode_predictions successful: torch.Size([8, 264960, 4])\n","   ğŸš¨ ABNORMAL OBJ LOSS DETECTED: 1607.44\n","      Box: 1.9785\n","      Cls: 10.2546\n","      Positive samples: 167\n","   ğŸ“Š Batch   11/1343 (  0.8%)\n","                        ğŸ“‰ Loss: T=1627.5892 B=1.9785 O=1607.4424 C=10.2546\n","                        âš™ï¸ LR: 4.00e-06 | ETA: 36.0min | Mem: 0.2GB\n","                        âœ… Success: 10 | âŒ Errors: 0 | ğŸš¨ OOM: 0\n","ğŸ§¹ Memory cleanup: 254 objects | GPU: 0.2GB alloc, 0.4GB reserved\n","ğŸ”§ [TARGETS v3.4-clean-fix] Processing: torch.Size([8, 264960, 20])\n","ğŸ”§ [v3.4-clean-fix] Batch 0: candidates = 36\n","ğŸ‰ [v3.4-clean-fix] Cost matrix computed: torch.Size([4, 36])\n","ğŸ”§ [LOSS] decode_predictions called\n","   self.anchor_info keys: ['anchors_pixel_per_level', 'strides_per_level', 'grid_sizes', 'input_size']\n","   Generating anchor_points from grid_sizes\n","   Generated anchor_points: torch.Size([264960, 2])\n","   Generated strides from strides_per_level: torch.Size([264960])\n","âœ… [LOSS] decode_predictions successful: torch.Size([8, 264960, 4])\n","   ğŸš¨ ABNORMAL OBJ LOSS DETECTED: 2530.72\n","      Box: 1.9780\n","      Cls: 10.2335\n","      Positive samples: 106\n","ğŸ”§ [TARGETS v3.4-clean-fix] Processing: torch.Size([8, 264960, 20])\n","ğŸ”§ [v3.4-clean-fix] Batch 0: candidates = 99\n","ğŸ‰ [v3.4-clean-fix] Cost matrix computed: torch.Size([12, 99])\n","ğŸ”§ [LOSS] decode_predictions called\n","   self.anchor_info keys: ['anchors_pixel_per_level', 'strides_per_level', 'grid_sizes', 'input_size']\n","   Generating anchor_points from grid_sizes\n","   Generated anchor_points: torch.Size([264960, 2])\n","   Generated strides from strides_per_level: torch.Size([264960])\n","âœ… [LOSS] decode_predictions successful: torch.Size([8, 264960, 4])\n","   ğŸš¨ ABNORMAL OBJ LOSS DETECTED: 2196.67\n","      Box: 1.9783\n","      Cls: 10.2372\n","      Positive samples: 122\n","ğŸ”§ [TARGETS v3.4-clean-fix] Processing: torch.Size([8, 264960, 20])\n","ğŸ”§ [v3.4-clean-fix] Batch 0: candidates = 9\n","ğŸ‰ [v3.4-clean-fix] Cost matrix computed: torch.Size([1, 9])\n","ğŸ”§ [LOSS] decode_predictions called\n","   self.anchor_info keys: ['anchors_pixel_per_level', 'strides_per_level', 'grid_sizes', 'input_size']\n","   Generating anchor_points from grid_sizes\n","   Generated anchor_points: torch.Size([264960, 2])\n","   Generated strides from strides_per_level: torch.Size([264960])\n","âœ… [LOSS] decode_predictions successful: torch.Size([8, 264960, 4])\n","   ğŸš¨ ABNORMAL OBJ LOSS DETECTED: 3160.59\n","      Box: 1.9807\n","      Cls: 10.2332\n","      Positive samples: 85\n","ğŸ”§ [TARGETS v3.4-clean-fix] Processing: torch.Size([8, 264960, 20])\n","ğŸ”§ [v3.4-clean-fix] Batch 0: candidates = 171\n","ğŸ‰ [v3.4-clean-fix] Cost matrix computed: torch.Size([24, 171])\n","ğŸ”§ [LOSS] decode_predictions called\n","   self.anchor_info keys: ['anchors_pixel_per_level', 'strides_per_level', 'grid_sizes', 'input_size']\n","   Generating anchor_points from grid_sizes\n","   Generated anchor_points: torch.Size([264960, 2])\n","   Generated strides from strides_per_level: torch.Size([264960])\n","âœ… [LOSS] decode_predictions successful: torch.Size([8, 264960, 4])\n","   ğŸš¨ ABNORMAL OBJ LOSS DETECTED: 1860.22\n","      Box: 1.9777\n","      Cls: 10.2466\n","      Positive samples: 144\n","ğŸ”§ [TARGETS v3.4-clean-fix] Processing: torch.Size([8, 264960, 20])\n","ğŸ”§ [v3.4-clean-fix] Batch 0: candidates = 213\n","ğŸ‰ [v3.4-clean-fix] Cost matrix computed: torch.Size([30, 213])\n","ğŸ”§ [LOSS] decode_predictions called\n","   self.anchor_info keys: ['anchors_pixel_per_level', 'strides_per_level', 'grid_sizes', 'input_size']\n","   Generating anchor_points from grid_sizes\n","   Generated anchor_points: torch.Size([264960, 2])\n","   Generated strides from strides_per_level: torch.Size([264960])\n","âœ… [LOSS] decode_predictions successful: torch.Size([8, 264960, 4])\n","   ğŸš¨ ABNORMAL OBJ LOSS DETECTED: 1851.19\n","      Box: 1.9762\n","      Cls: 10.2663\n","      Positive samples: 145\n","ğŸ”§ [TARGETS v3.4-clean-fix] Processing: torch.Size([8, 264960, 20])\n","ğŸ”§ [v3.4-clean-fix] Batch 0: candidates = 126\n","ğŸ‰ [v3.4-clean-fix] Cost matrix computed: torch.Size([21, 126])\n","ğŸ”§ [LOSS] decode_predictions called\n","   self.anchor_info keys: ['anchors_pixel_per_level', 'strides_per_level', 'grid_sizes', 'input_size']\n","   Generating anchor_points from grid_sizes\n","   Generated anchor_points: torch.Size([264960, 2])\n","   Generated strides from strides_per_level: torch.Size([264960])\n","âœ… [LOSS] decode_predictions successful: torch.Size([8, 264960, 4])\n","   ğŸš¨ ABNORMAL OBJ LOSS DETECTED: 1802.11\n","      Box: 1.9799\n","      Cls: 10.2434\n","      Positive samples: 148\n","ğŸ”§ [TARGETS v3.4-clean-fix] Processing: torch.Size([8, 264960, 20])\n","ğŸ”§ [v3.4-clean-fix] Batch 0: candidates = 174\n","ğŸ‰ [v3.4-clean-fix] Cost matrix computed: torch.Size([24, 174])\n","ğŸ”§ [LOSS] decode_predictions called\n","   self.anchor_info keys: ['anchors_pixel_per_level', 'strides_per_level', 'grid_sizes', 'input_size']\n","   Generating anchor_points from grid_sizes\n","   Generated anchor_points: torch.Size([264960, 2])\n","   Generated strides from strides_per_level: torch.Size([264960])\n","âœ… [LOSS] decode_predictions successful: torch.Size([8, 264960, 4])\n","   ğŸš¨ ABNORMAL OBJ LOSS DETECTED: 1902.38\n","      Box: 1.9798\n","      Cls: 10.2367\n","      Positive samples: 140\n","ğŸ”§ [TARGETS v3.4-clean-fix] Processing: torch.Size([8, 264960, 20])\n","ğŸ”§ [v3.4-clean-fix] Batch 0: candidates = 81\n","ğŸ‰ [v3.4-clean-fix] Cost matrix computed: torch.Size([11, 81])\n","ğŸ”§ [LOSS] decode_predictions called\n","   self.anchor_info keys: ['anchors_pixel_per_level', 'strides_per_level', 'grid_sizes', 'input_size']\n","   Generating anchor_points from grid_sizes\n","   Generated anchor_points: torch.Size([264960, 2])\n","   Generated strides from strides_per_level: torch.Size([264960])\n","âœ… [LOSS] decode_predictions successful: torch.Size([8, 264960, 4])\n","   ğŸš¨ ABNORMAL OBJ LOSS DETECTED: 2318.75\n","      Box: 1.9774\n","      Cls: 10.2399\n","      Positive samples: 115\n","\n","âš ï¸ Training interrupted by user\n"]}]},{"cell_type":"markdown","source":["ä»¥ä¸‹ã€geminiç‰ˆ"],"metadata":{"id":"IG2BN_oHFfI-"}},{"cell_type":"code","source":["import os\n","import numpy as np\n","\n","def verify_dataset(image_dir, label_dir):\n","    \"\"\"\n","    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ©ãƒ™ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã«ç ´æãŒãªã„ã‹ï¼ˆNaN/infãŒå«ã¾ã‚Œã¦ã„ãªã„ã‹ï¼‰ã‚’\n","    å¾¹åº•çš„ã«ãƒã‚§ãƒƒã‚¯ã™ã‚‹è¨ºæ–­ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€‚\n","    \"\"\"\n","    print(\"--- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å®Œå…¨æ€§æ¤œè¨¼ã‚’é–‹å§‹ã—ã¾ã™ ---\")\n","    corrupted_files = []\n","\n","    try:\n","        image_files = sorted([f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png', '.jpeg'))])\n","        total_files = len(image_files)\n","        print(f\"ãƒã‚§ãƒƒã‚¯å¯¾è±¡ã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«æ•°: {total_files}\")\n","    except FileNotFoundError:\n","        print(f\"âŒã‚¨ãƒ©ãƒ¼: ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {image_dir}\")\n","        print(\"TRAIN_IMG_DIRã®ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")\n","        return None\n","\n","    for i, img_name in enumerate(image_files):\n","        if i > 0 and i % 1000 == 0:\n","            print(f\"é€²æ—: {i}/{total_files}\")\n","\n","        label_filename = os.path.splitext(img_name)[0] + '.txt'\n","        label_path = os.path.join(label_dir, label_filename)\n","\n","        if os.path.exists(label_path):\n","            try:\n","                with open(label_path, 'r') as f:\n","                    content = f.read()\n","                    if content.strip() == \"\":\n","                        continue  # ç©ºã®ãƒ©ãƒ™ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã¯æ­£å¸¸ã¨ã¿ãªã™\n","\n","                    # ãƒ•ã‚¡ã‚¤ãƒ«å†…ã« 'nan' ã‚„ 'inf' ã®æ–‡å­—åˆ—ãŒãªã„ã‹ãƒã‚§ãƒƒã‚¯\n","                    if \"nan\" in content.lower() or \"inf\" in content.lower():\n","                        print(f\"ğŸš¨ã€ç ´ææ¤œçŸ¥ - æ–‡å­—åˆ—ã€‘: {label_filename}\")\n","                        corrupted_files.append(label_filename)\n","                        continue\n","\n","                    # æ•°å€¤ã¨ã—ã¦èª­ã¿è¾¼ã‚“ã§ãƒã‚§ãƒƒã‚¯\n","                    lines = content.strip().split('\\n')\n","                    labels = np.array([line.split() for line in lines], dtype=np.float32)\n","\n","                    if np.isnan(labels).any() or np.isinf(labels).any():\n","                        print(f\"ğŸš¨ã€ç ´ææ¤œçŸ¥ - æ•°å€¤ã€‘: {label_filename}\")\n","                        corrupted_files.append(label_filename)\n","\n","            except Exception as e:\n","                print(f\"ğŸš¨ã€ãƒ‘ãƒ¼ã‚¹å¤±æ•—ã€‘: {label_filename}, ã‚¨ãƒ©ãƒ¼: {e}\")\n","                corrupted_files.append(label_filename)\n","\n","    print(\"\\n--- æ¤œè¨¼å®Œäº† ---\")\n","    if corrupted_files:\n","        print(f\"ç™ºè¦‹ã•ã‚ŒãŸç ´æãƒ•ã‚¡ã‚¤ãƒ«ã®æ•°: {len(corrupted_files)}\")\n","        # é‡è¤‡ã‚’é™¤ã„ã¦è¡¨ç¤º\n","        unique_corrupted = sorted(list(set(corrupted_files)))\n","        print(f\"ç ´æã—ã¦ã„ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ{len(unique_corrupted)}ä»¶ï¼‰:\")\n","        for f in unique_corrupted:\n","            print(f\"  - {f}\")\n","    else:\n","        print(\"âœ… ç ´æãƒ•ã‚¡ã‚¤ãƒ«ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã‚¯ãƒªãƒ¼ãƒ³ã§ã™ã€‚\")\n","\n","    return corrupted_files\n","\n","if __name__ == '__main__':\n","    # Colabç’°å¢ƒã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‘ã‚¹\n","    TRAIN_IMG_DIR = \"/content/FLIR_YOLO_local/images/train\"\n","    TRAIN_LABEL_DIR = \"/content/FLIR_YOLO_local/labels/train\"\n","\n","    print(f\"ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {TRAIN_IMG_DIR}\")\n","    print(f\"ãƒ©ãƒ™ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {TRAIN_LABEL_DIR}\")\n","\n","    verify_dataset(TRAIN_IMG_DIR, TRAIN_LABEL_DIR)"],"metadata":{"id":"xjGPYWpLbMxw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Google Driveãƒã‚¦ãƒ³ãƒˆ\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç§»å‹•\n","import os\n","os.chdir('/content/drive/MyDrive/IR_obj_detction/gemini')\n","!nvidia-smi\n","\n","# ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç¢ºèª\n","import glob\n","checkpoints = glob.glob('saved_models/checkpoint_epoch_*.pth')\n","print(f\"Found checkpoints: {[os.path.basename(f) for f in checkpoints]}\")"],"metadata":{"id":"KMxtPxkpFaM0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ\n","!mkdir -p /content/FLIR_YOLO_local/images/train\n","!mkdir -p /content/FLIR_YOLO_local/labels/train\n","\n","# Google Driveã‹ã‚‰ãƒ­ãƒ¼ã‚«ãƒ«ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã‚³ãƒ”ãƒ¼ (æ™‚é–“ã¯ã‹ã‹ã‚Šã¾ã™)\n","# ãƒ‘ã‚¹ã¯å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«åˆã‚ã›ã¦ãã ã•ã„\n","!echo \"Copying training images...\"\n","!cp -r /content/drive/MyDrive/old/EfficientNet_Project/data/FLIR_YOLO/images/train/* /content/FLIR_YOLO_local/images/train/\n","!echo \"Copying training labels...\"\n","!cp -r /content/drive/MyDrive/old/EfficientNet_Project/data/FLIR_YOLO/labels/train/* /content/FLIR_YOLO_local/labels/train/\n","!echo \"Copy complete!\""],"metadata":{"id":"ayI7EA9nFcjq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# æœ€æ–°ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰è‡ªå‹•å†é–‹\n","exec(open('unified_training.py').read())"],"metadata":{"id":"cxszOY3VFeXK"},"execution_count":null,"outputs":[]}]}