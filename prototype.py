#!/usr/bin/env python3
"""
Minimal Working YOLO - ç¢ºå®Ÿã«å‹•ãæœ€å°å®Ÿè£…
ã¾ãšã¯ã“ã‚Œã§å‹•ä½œç¢ºèªã—ã¦ã‹ã‚‰æ©Ÿèƒ½è¿½åŠ ã—ã¦ã„ã
"""

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import cv2
import time

# ===== è¨­å®šï¼ˆè¶…ã‚·ãƒ³ãƒ—ãƒ«ï¼‰=====
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
BATCH_SIZE = 4  # å°ã•ãå§‹ã‚ã‚‹
IMG_SIZE = 416  # æ­£æ–¹å½¢ã§ã‚·ãƒ³ãƒ—ãƒ«ã«
NUM_CLASSES = 15
LEARNING_RATE = 1e-4
NUM_EPOCHS = 10

# ãƒ‘ã‚¹è¨­å®š
TRAIN_IMG_DIR = "/content/FLIR_YOLO_local/images/train"
TRAIN_LABEL_DIR = "/content/FLIR_YOLO_local/labels/train"
SAVE_DIR = "/content/drive/MyDrive/IR_obj_detection/minimal_yolo"

# ===== ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆæœ€å°é™ï¼‰=====
class MinimalDataset(Dataset):
    def __init__(self, img_dir, label_dir, img_size=416):
        self.img_dir = img_dir
        self.label_dir = label_dir
        self.img_size = img_size
        self.img_files = [f for f in os.listdir(img_dir) if f.endswith('.jpg')][:1000]  # æœ€åˆã¯1000æšã ã‘
        
    def __len__(self):
        return len(self.img_files)
    
    def __getitem__(self, idx):
        # ç”»åƒ
        img_path = os.path.join(self.img_dir, self.img_files[idx])
        img = cv2.imread(img_path, 0)  # ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«
        img = cv2.resize(img, (self.img_size, self.img_size))
        img = torch.from_numpy(img).float() / 255.0
        img = img.unsqueeze(0)  # [1, H, W]
        
        # ãƒ©ãƒ™ãƒ«ï¼ˆYOLOå½¢å¼ï¼‰
        label_path = os.path.join(self.label_dir, self.img_files[idx].replace('.jpg', '.txt'))
        targets = []
        if os.path.exists(label_path):
            with open(label_path, 'r') as f:
                for line in f:
                    parts = line.strip().split()
                    if len(parts) == 5:
                        cls, cx, cy, w, h = map(float, parts)
                        targets.append([cls, cx, cy, w, h])
        
        return img, torch.tensor(targets) if targets else torch.zeros((0, 5))

# ===== ãƒ¢ãƒ‡ãƒ«ï¼ˆè¶…ã‚·ãƒ³ãƒ—ãƒ«ï¼‰=====
class MinimalYOLO(nn.Module):
    def __init__(self, num_classes=15):
        super().__init__()
        self.num_classes = num_classes
        
        # è¶…ã‚·ãƒ³ãƒ—ãƒ«ãªBackboneï¼ˆãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°5å›ã§stride=32ï¼‰
        self.features = nn.Sequential(
            # 416 -> 208
            nn.Conv2d(1, 16, 3, 2, 1),
            nn.BatchNorm2d(16),
            nn.ReLU(inplace=True),
            
            # 208 -> 104
            nn.Conv2d(16, 32, 3, 2, 1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            
            # 104 -> 52
            nn.Conv2d(32, 64, 3, 2, 1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            
            # 52 -> 26
            nn.Conv2d(64, 128, 3, 2, 1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            
            # 26 -> 13
            nn.Conv2d(128, 256, 3, 2, 1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
        )
        
        # Detection headï¼ˆ1ã¤ã®ã‚¹ã‚±ãƒ¼ãƒ«ã®ã¿ã€ã‚¢ãƒ³ã‚«ãƒ¼ãªã—ï¼‰
        self.detector = nn.Conv2d(256, 5 + num_classes, 1)
        
    def forward(self, x):
        x = self.features(x)
        x = self.detector(x)
        
        # [B, C, H, W] -> [B, H*W, 5+classes]
        B, C, H, W = x.shape
        x = x.permute(0, 2, 3, 1).contiguous()
        x = x.view(B, H * W, C)
        
        return x, (H, W)  # ã‚°ãƒªãƒƒãƒ‰ã‚µã‚¤ã‚ºã‚‚è¿”ã™

# ===== æå¤±é–¢æ•°ï¼ˆè¶…ã‚·ãƒ³ãƒ—ãƒ«ï¼‰=====
class MinimalLoss(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        self.num_classes = num_classes
        
    def forward(self, predictions, targets, grid_size):
        """
        predictions: [B, H*W, 5+classes]
        targets: ãƒªã‚¹ãƒˆ of [N, 5] (class, cx, cy, w, h)
        """
        B = predictions.size(0)
        H, W = grid_size
        device = predictions.device
        
        # äºˆæ¸¬å€¤ã‚’åˆ†è§£
        pred_xy = predictions[..., :2].sigmoid()  # 0-1ã«æ­£è¦åŒ–
        pred_wh = predictions[..., 2:4].exp()     # expå¤‰æ›
        pred_conf = predictions[..., 4].sigmoid()  # objectness
        pred_cls = predictions[..., 5:].sigmoid()  # classes
        
        # æå¤±ã®åˆæœŸåŒ–
        loss_xy = 0
        loss_wh = 0
        loss_conf = 0
        loss_cls = 0
        num_targets = 0
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒã‚¹ã‚¯ï¼ˆã©ã®ã‚°ãƒªãƒƒãƒ‰ã«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒã‚ã‚‹ã‹ï¼‰
        obj_mask = torch.zeros(B, H * W, device=device)
        
        # ãƒãƒƒãƒã”ã¨ã«å‡¦ç†
        for b in range(B):
            if len(targets[b]) == 0:
                continue
                
            for t in targets[b]:
                if torch.isnan(t).any() or torch.isinf(t).any():
                    continue
                    
                cls_id, cx, cy, w, h = t
                
                # ã‚°ãƒªãƒƒãƒ‰åº§æ¨™
                gx = int(cx * W)
                gy = int(cy * H)
                
                if gx >= W or gy >= H or gx < 0 or gy < 0:
                    continue
                
                # ã‚°ãƒªãƒƒãƒ‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
                gi = gy * W + gx
                
                # ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãƒã‚¹ã‚¯
                obj_mask[b, gi] = 1
                
                # ç›¸å¯¾åº§æ¨™ï¼ˆã‚°ãƒªãƒƒãƒ‰å†…ã®ä½ç½®ï¼‰
                tx = cx * W - gx
                ty = cy * H - gy
                tw = w * W
                th = h * H
                
                # åº§æ¨™æå¤±
                loss_xy += F.mse_loss(pred_xy[b, gi], torch.tensor([tx, ty], device=device))
                loss_wh += F.mse_loss(pred_wh[b, gi], torch.tensor([tw, th], device=device))
                
                # ã‚¯ãƒ©ã‚¹æå¤±
                cls_target = torch.zeros(self.num_classes, device=device)
                cls_target[int(cls_id)] = 1
                loss_cls += F.binary_cross_entropy(pred_cls[b, gi], cls_target)
                
                num_targets += 1
        
        # Confidenceæå¤±ï¼ˆæ­£ä¾‹ã¨è² ä¾‹ï¼‰
        loss_conf = F.binary_cross_entropy(pred_conf.view(-1), obj_mask.view(-1))
        
        # åˆè¨ˆï¼ˆé‡ã¿ä»˜ã‘ï¼‰
        if num_targets > 0:
            loss_xy /= num_targets
            loss_wh /= num_targets
            loss_cls /= num_targets
        
        total_loss = loss_xy * 5 + loss_wh * 5 + loss_conf + loss_cls
        
        return {
            'total': total_loss,
            'xy': loss_xy.item() if isinstance(loss_xy, torch.Tensor) else loss_xy,
            'wh': loss_wh.item() if isinstance(loss_wh, torch.Tensor) else loss_wh,
            'conf': loss_conf.item(),
            'cls': loss_cls.item() if isinstance(loss_cls, torch.Tensor) else loss_cls
        }

# ===== ãƒ¡ã‚¤ãƒ³å‡¦ç† =====
def main():
    print("ğŸš€ Starting Minimal YOLO Training")
    print(f"Device: {DEVICE}")
    
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    os.makedirs(SAVE_DIR, exist_ok=True)
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    print("\nğŸ“Š Loading dataset...")
    dataset = MinimalDataset(TRAIN_IMG_DIR, TRAIN_LABEL_DIR, IMG_SIZE)
    print(f"Dataset size: {len(dataset)}")
    
    # ã‚«ã‚¹ã‚¿ãƒ collateé–¢æ•°
    def collate_fn(batch):
        imgs, targets = zip(*batch)
        imgs = torch.stack(imgs, 0)
        return imgs, targets
    
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, 
                          collate_fn=collate_fn, num_workers=2)
    print(f"Total batches: {len(dataloader)}")
    
    # ãƒ¢ãƒ‡ãƒ«
    print("\nğŸ¤– Creating model...")
    model = MinimalYOLO(NUM_CLASSES).to(DEVICE)
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
    total_params = sum(p.numel() for p in model.parameters())
    print(f"Total parameters: {total_params:,}")
    
    # ãƒ†ã‚¹ãƒˆ
    with torch.no_grad():
        test_input = torch.randn(2, 1, IMG_SIZE, IMG_SIZE).to(DEVICE)
        test_output, grid_size = model(test_input)
        print(f"Test output shape: {test_output.shape}, Grid size: {grid_size}")
    
    # æå¤±é–¢æ•°ã¨ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
    criterion = MinimalLoss(NUM_CLASSES)
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    
    # å­¦ç¿’ãƒ«ãƒ¼ãƒ—
    print("\nğŸ¯ Starting training...")
    model.train()
    
    for epoch in range(NUM_EPOCHS):
        epoch_loss = 0
        epoch_start = time.time()
        
        for batch_idx, (images, targets) in enumerate(dataloader):
            images = images.to(DEVICE)
            
            # Forward
            predictions, grid_size = model(images)
            
            # Loss
            losses = criterion(predictions, targets, grid_size)
            loss = losses['total']
            
            # Backward
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
            
            # é€²æ—è¡¨ç¤º
            if batch_idx % 10 == 0:
                print(f"Epoch [{epoch+1}/{NUM_EPOCHS}] Batch [{batch_idx}/{len(dataloader)}] "
                      f"Loss: {loss.item():.4f} (xy:{losses['xy']:.3f} wh:{losses['wh']:.3f} "
                      f"conf:{losses['conf']:.3f} cls:{losses['cls']:.3f})")
        
        # ã‚¨ãƒãƒƒã‚¯çµ‚äº†
        epoch_time = time.time() - epoch_start
        avg_loss = epoch_loss / len(dataloader)
        print(f"\nğŸ“ˆ Epoch {epoch+1} completed in {epoch_time:.1f}s - Avg Loss: {avg_loss:.4f}")
        
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ï¼ˆæ¯ã‚¨ãƒãƒƒã‚¯ï¼‰
        if epoch % 2 == 0 or epoch == NUM_EPOCHS - 1:
            checkpoint = {
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': avg_loss
            }
            save_path = os.path.join(SAVE_DIR, f'model_epoch_{epoch+1}.pth')
            torch.save(checkpoint, save_path)
            print(f"ğŸ’¾ Model saved: {save_path}")
    
    print("\nâœ… Training completed!")

if __name__ == "__main__":
    main()