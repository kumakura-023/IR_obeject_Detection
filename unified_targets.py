# ä¿®æ­£ç‰ˆ unified_targets.py
# Obj Lossç•°å¸¸å€¤å•é¡Œã‚’è§£æ±º

import torch
import numpy as np
from collections import defaultdict
import matplotlib.pyplot as plt
import time
from typing import List, Tuple, Dict, Optional

# ===== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•° =====
def get_default_anchors() -> List[List[Tuple[int, int]]]:
    """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¢ãƒ³ã‚«ãƒ¼ï¼ˆYOLOv3ãƒ™ãƒ¼ã‚¹ï¼‰"""
    return [
        [(10,13), (16,30), (33,23)],
        [(30,61), (62,45), (59,119)],
        [(116,90), (156,198), (373,326)]
    ]

def analyze_dataset_statistics(dataset, num_samples=1000, input_size=(640, 512)):
    """
    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±è¨ˆåˆ†æã¨ã‚¢ãƒ³ã‚«ãƒ¼æœ€é©åŒ–
    """
    print(f"ğŸ“Š Analyzing dataset statistics from {min(len(dataset), num_samples)} samples...")
    
    box_sizes_pixel = []
    aspect_ratios = []
    scale_distribution = {'small': 0, 'medium': 0, 'large': 0}
    class_distribution = defaultdict(int)
    
    start_time = time.time()
    processed = 0
    img_w, img_h = input_size
    
    for i in range(min(len(dataset), num_samples)):
        if i % 100 == 0 and i > 0:
            elapsed = time.time() - start_time
            print(f"   Progress: {i}/{min(len(dataset), num_samples)} ({i/min(len(dataset), num_samples)*100:.1f}%) - {elapsed:.1f}s")
        
        try:
            _, targets = dataset[i]
            
            if torch.is_tensor(targets):
                if targets.is_cuda:
                    targets = targets.cpu()
                if targets.dim() == 1:
                    targets = targets.unsqueeze(0)
                elif targets.size(0) == 0:
                    continue
                    
                targets_np = targets.numpy()
                
                for j in range(targets_np.shape[0]):
                    target_data = targets_np[j]
                    if len(target_data) >= 4:
                        # ç›¸å¯¾åº§æ¨™ã‹ã‚‰ãƒ”ã‚¯ã‚»ãƒ«åº§æ¨™ã¸å¤‰æ›
                        w_rel, h_rel = target_data[2], target_data[3]
                        if w_rel > 0 and h_rel > 0:
                            w_pixel = w_rel * img_w
                            h_pixel = h_rel * img_h
                            box_sizes_pixel.append((w_pixel, h_pixel))
                            
                            # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”
                            aspect_ratios.append(w_pixel / h_pixel)
                            
                            # ã‚µã‚¤ã‚ºåˆ†å¸ƒ
                            area = w_pixel * h_pixel
                            if area < 32*32:
                                scale_distribution['small'] += 1
                            elif area < 96*96:
                                scale_distribution['medium'] += 1
                            else:
                                scale_distribution['large'] += 1
                            
                            # ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ
                            if len(target_data) > 5:
                                class_scores = target_data[5:]
                                class_id = np.argmax(class_scores)
                                class_distribution[class_id] += 1
                            
                            processed += 1
            
        except Exception as e:
            if i < 5:
                print(f"âš ï¸ Sample {i} analysis failed: {e}")
            continue
    
    total_time = time.time() - start_time
    print(f"âœ… Analysis completed: {processed} valid boxes found in {total_time:.1f}s")
    
    if len(box_sizes_pixel) == 0:
        print("âŒ No valid boxes found for analysis")
        return None, defaultdict(int), {}
    
    box_sizes_np = np.array(box_sizes_pixel)
    
    # çµ±è¨ˆæƒ…å ±è¨ˆç®—
    statistics = {
        'total_boxes': len(box_sizes_np),
        'avg_width': np.mean(box_sizes_np[:, 0]),
        'avg_height': np.mean(box_sizes_np[:, 1]),
        'std_width': np.std(box_sizes_np[:, 0]),
        'std_height': np.std(box_sizes_np[:, 1]),
        'avg_aspect_ratio': np.mean(aspect_ratios),
        'scale_distribution': scale_distribution,
        'processing_time': total_time
    }
    
    # çµ±è¨ˆè¡¨ç¤º
    print(f"\nğŸ“ˆ Dataset Statistics:")
    print(f"   Total boxes: {statistics['total_boxes']}")
    print(f"   Avg size: {statistics['avg_width']:.1f} x {statistics['avg_height']:.1f} pixels")
    print(f"   Size distribution: Small={scale_distribution['small']}, Medium={scale_distribution['medium']}, Large={scale_distribution['large']}")
    
    # ã‚¢ãƒ³ã‚«ãƒ¼æœ€é©åŒ–
    print("\nğŸ”§ Generating optimized anchors...")
    anchor_start = time.time()
    optimized_anchors_pixel = generate_anchors_kmeans(box_sizes_np, num_anchors=9)
    anchor_time = time.time() - anchor_start
    print(f"âœ… Anchor optimization completed in {anchor_time:.1f}s")
    
    return optimized_anchors_pixel, dict(class_distribution), statistics


def generate_anchors_kmeans(box_sizes_pixel: np.ndarray, num_anchors: int = 9, max_iter: int = 300) -> List[List[Tuple[int, int]]]:
    """
    K-means++ã§ã‚¢ãƒ³ã‚«ãƒ¼ã‚’æœ€é©åŒ–
    """
    from sklearn.cluster import KMeans
    
    print(f"ğŸ”§ Generating {num_anchors} anchors using K-means++ (max {max_iter} iterations)...")
    
    if len(box_sizes_pixel) < num_anchors:
        print("âš ï¸ Not enough boxes, using default anchors")
        return get_default_anchors()
    
    # K-means++å®Ÿè¡Œ
    kmeans = KMeans(
        n_clusters=num_anchors, 
        init='k-means++',
        random_state=42, 
        max_iter=max_iter,
        n_init=10
    )
    clusters = kmeans.fit(box_sizes_pixel)
    
    # ã‚¯ãƒ©ã‚¹ã‚¿ä¸­å¿ƒã‚’å–å¾—
    anchors_centers = clusters.cluster_centers_
    
    # é¢ç©ã§ã‚½ãƒ¼ãƒˆ
    areas = anchors_centers[:, 0] * anchors_centers[:, 1]
    sorted_indices = np.argsort(areas)
    sorted_anchors = anchors_centers[sorted_indices]
    
    # 3ã¤ã®FPNãƒ¬ãƒ™ãƒ«ã«åˆ†å‰²
    anchors_per_level = num_anchors // 3
    optimized_anchors = []
    
    for level in range(3):
        start_idx = level * anchors_per_level
        if level == 2:  # æœ€å¾Œã®ãƒ¬ãƒ™ãƒ«ã¯æ®‹ã‚Šå…¨éƒ¨
            end_idx = num_anchors
        else:
            end_idx = (level + 1) * anchors_per_level
            
        level_anchors = []
        for idx in range(start_idx, end_idx):
            if idx < len(sorted_anchors):
                w, h = sorted_anchors[idx]
                level_anchors.append((int(w), int(h)))
        
        # ä¸è¶³åˆ†ã‚’è£œå®Œ
        while len(level_anchors) < anchors_per_level and level_anchors:
            level_anchors.append(level_anchors[-1])
        
        optimized_anchors.append(level_anchors)
    
    print("ğŸ‰ Generated optimized anchors:")
    for i, level in enumerate(optimized_anchors):
        print(f"   Level {i}: {level}")
    
    return optimized_anchors


def prepare_anchor_grid_info(anchors_pixel_per_level: List[List[Tuple[int, int]]], 
                           model_grid_sizes: List[Tuple[int, int]], 
                           input_size_wh: Tuple[int, int]) -> Dict:
    """
    ã‚¢ãƒ³ã‚«ãƒ¼ã¨ã‚°ãƒªãƒƒãƒ‰æƒ…å ±ã‚’æº–å‚™
    """
    img_w, img_h = input_size_wh
    strides_per_level = []
    
    # ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰è¨ˆç®—
    for level_idx, (grid_h, grid_w) in enumerate(model_grid_sizes):
        stride = img_h // grid_h  # é€šå¸¸ã¯æ­£æ–¹å½¢ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰
        strides_per_level.append(stride)
    
    return {
        'anchors_pixel_per_level': anchors_pixel_per_level,
        'strides_per_level': strides_per_level,
        'grid_sizes': model_grid_sizes,
        'input_size': input_size_wh
    }


def build_targets(targets: List[torch.Tensor], 
                  anchors_pixel_per_level: List[List[Tuple[int, int]]],
                  strides_per_level: List[int],
                  grid_sizes: List[Tuple[int, int]], 
                  input_size: Tuple[int, int],
                  num_classes: int, 
                  device: torch.device,
                  anchor_threshold: float = 0.2) -> Dict[str, torch.Tensor]:
    """
    ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ§‹ç¯‰ï¼ˆä¿®æ­£ç‰ˆï¼šIoUå¤‰èª¿ã‚’ç·©å’Œï¼‰
    """
    B = len(targets)
    img_w, img_h = input_size
    
    # å…¨äºˆæ¸¬æ•°ã‚’è¨ˆç®—
    total_predictions = 0
    for level_idx, (grid_h, grid_w) in enumerate(grid_sizes):
        num_anchors = len(anchors_pixel_per_level[level_idx])
        total_predictions += grid_h * grid_w * num_anchors
    
    # çµ±ä¸€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ†ãƒ³ã‚½ãƒ«åˆæœŸåŒ–
    unified_targets = torch.zeros((B, total_predictions, 5 + num_classes), 
                                 device=device, dtype=torch.float32)
    
    # å„ãƒãƒƒãƒã‚¢ã‚¤ãƒ†ãƒ ã‚’å‡¦ç†
    for b_idx, batch_targets in enumerate(targets):
        if len(batch_targets) == 0:
            continue
            
        gt_data = batch_targets.to(device) if torch.is_tensor(batch_targets) else \
                  torch.tensor(batch_targets, device=device, dtype=torch.float32)
        
        if gt_data.size(0) == 0:
            continue
        
        # GTæƒ…å ±ã‚’æŠ½å‡º
        gt_boxes_rel = gt_data[:, :4]  # (cx, cy, w, h) in 0-1
        gt_objectness = gt_data[:, 4]
        gt_classes = gt_data[:, 5:] if gt_data.size(1) > 5 else torch.zeros((gt_data.size(0), num_classes), device=device)
        
        # æœ‰åŠ¹ãªGTã®ã¿ãƒ•ã‚£ãƒ«ã‚¿
        valid_mask = (gt_boxes_rel[:, 2] > 0) & (gt_boxes_rel[:, 3] > 0) & (gt_objectness > 0)
        if not valid_mask.any():
            continue
            
        gt_boxes_rel = gt_boxes_rel[valid_mask]
        gt_objectness = gt_objectness[valid_mask]
        gt_classes = gt_classes[valid_mask]
        
        # GTãƒœãƒƒã‚¯ã‚¹ã‚’ãƒ”ã‚¯ã‚»ãƒ«å˜ä½ã«å¤‰æ›
        gt_boxes_pixel = gt_boxes_rel * torch.tensor([img_w, img_h, img_w, img_h], device=device)
        
        # å„FPNãƒ¬ãƒ™ãƒ«ã§å‡¦ç†
        current_offset = 0
        for level_idx, (grid_h, grid_w) in enumerate(grid_sizes):
            stride = strides_per_level[level_idx]
            anchors = torch.tensor(anchors_pixel_per_level[level_idx], device=device, dtype=torch.float32)
            num_anchors = len(anchors)
            
            # GTä¸­å¿ƒã®ã‚°ãƒªãƒƒãƒ‰åº§æ¨™ã‚’è¨ˆç®—
            gt_cx = gt_boxes_pixel[:, 0]
            gt_cy = gt_boxes_pixel[:, 1]
            grid_x = (gt_cx / stride).long().clamp(0, grid_w - 1)
            grid_y = (gt_cy / stride).long().clamp(0, grid_h - 1)
            
            # å„GTã«å¯¾ã—ã¦ã‚¢ãƒ³ã‚«ãƒ¼ã‚’å‰²ã‚Šå½“ã¦
            for gt_idx in range(len(gt_boxes_pixel)):
                gt_w = gt_boxes_pixel[gt_idx, 2]
                gt_h = gt_boxes_pixel[gt_idx, 3]
                
                # IoUè¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
                ious = []
                for anchor_w, anchor_h in anchors:
                    inter_w = min(gt_w, anchor_w)
                    inter_h = min(gt_h, anchor_h)
                    inter_area = inter_w * inter_h
                    union_area = gt_w * gt_h + anchor_w * anchor_h - inter_area
                    iou = inter_area / (union_area + 1e-8)
                    ious.append(iou)
                
                ious = torch.tensor(ious, device=device)
                
                # é–¾å€¤ä»¥ä¸Šã®ã‚¢ãƒ³ã‚«ãƒ¼ã¾ãŸã¯æœ€è‰¯ã®ã‚¢ãƒ³ã‚«ãƒ¼ã‚’é¸æŠ
                matched_anchors = torch.where(ious > anchor_threshold)[0]
                if len(matched_anchors) == 0:
                    matched_anchors = [ious.argmax().item()]
                
                # å„ãƒãƒƒãƒã—ãŸã‚¢ãƒ³ã‚«ãƒ¼ã«å¯¾ã—ã¦ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’è¨­å®š
                for anchor_idx in matched_anchors:
                    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨ˆç®—
                    target_idx = current_offset + \
                                anchor_idx * grid_h * grid_w + \
                                grid_y[gt_idx] * grid_w + \
                                grid_x[gt_idx]
                    
                    if target_idx >= total_predictions:
                        continue
                    
                    # ã‚ªãƒ•ã‚»ãƒƒãƒˆè¨ˆç®—
                    tx = (gt_cx[gt_idx] / stride) - grid_x[gt_idx].float()
                    ty = (gt_cy[gt_idx] / stride) - grid_y[gt_idx].float()
                    
                    # ã‚¹ã‚±ãƒ¼ãƒ«è¨ˆç®—
                    anchor_w, anchor_h = anchors[anchor_idx]
                    tw = torch.log(gt_w / anchor_w + 1e-8)
                    th = torch.log(gt_h / anchor_h + 1e-8)
                    
                    # ===== ä¿®æ­£ï¼šIoUå¤‰èª¿ã‚’ç·©å’Œ =====
                    iou_score = ious[anchor_idx].item()
                    
                    # IoUå¤‰èª¿ã‚’æ§ãˆã‚ã«ã™ã‚‹
                    if iou_score > 0.7:
                        # é«˜IoUã®å ´åˆã¯å…ƒã®å€¤ã‚’ã»ã¼ä¿æŒ
                        obj_modulation = 1.0
                        cls_modulation = 1.0
                    elif iou_score > 0.5:
                        # ä¸­IoUã®å ´åˆã¯è»½ã„å¤‰èª¿
                        obj_modulation = 0.8 + 0.2 * iou_score
                        cls_modulation = 0.9
                    elif iou_score > 0.3:
                        # ä½IoUã®å ´åˆã¯é©åº¦ãªå¤‰èª¿
                        obj_modulation = 0.6 + 0.4 * iou_score
                        cls_modulation = 0.7
                    else:
                        # æ¥µä½IoUã®å ´åˆã®ã¿å¼·ã„å¤‰èª¿
                        obj_modulation = iou_score
                        cls_modulation = iou_score * 0.5
                    
                    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨­å®š
                    unified_targets[b_idx, target_idx, 0] = tx
                    unified_targets[b_idx, target_idx, 1] = ty
                    unified_targets[b_idx, target_idx, 2] = tw
                    unified_targets[b_idx, target_idx, 3] = th
                    unified_targets[b_idx, target_idx, 4] = gt_objectness[gt_idx] * obj_modulation
                    unified_targets[b_idx, target_idx, 5:] = gt_classes[gt_idx] * cls_modulation
            
            current_offset += num_anchors * grid_h * grid_w
    
    return {
        'boxes': unified_targets[..., :4],      # tx, ty, tw, th
        'objectness': unified_targets[..., 4],  # ç·©å’Œã•ã‚ŒãŸIoUå¤‰èª¿objectness
        'classes': unified_targets[..., 5:]     # ç·©å’Œã•ã‚ŒãŸIoUå¤‰èª¿classes
    }


def evaluate_anchor_performance(box_sizes_pixel: np.ndarray, anchors_pixel_per_level: List[List[Tuple[int, int]]]) -> Dict:
    """
    ã‚¢ãƒ³ã‚«ãƒ¼æ€§èƒ½ã‚’è©³ç´°è©•ä¾¡
    """
    print("ğŸ“ˆ Evaluating anchor performance...")
    
    # å…¨ã‚¢ãƒ³ã‚«ãƒ¼ã‚’ãƒ•ãƒ©ãƒƒãƒˆåŒ–
    all_anchors = []
    for level_anchors in anchors_pixel_per_level:
        all_anchors.extend(level_anchors)
    
    ious = []
    anchor_usage = defaultdict(int)
    
    for box_w, box_h in box_sizes_pixel:
        box_area = box_w * box_h
        best_iou = 0
        best_anchor_idx = -1
        
        for anchor_idx, (anchor_w, anchor_h) in enumerate(all_anchors):
            # IoUè¨ˆç®—
            inter_w = min(box_w, anchor_w)
            inter_h = min(box_h, anchor_h)
            intersection = inter_w * inter_h
            
            anchor_area = anchor_w * anchor_h
            union = box_area + anchor_area - intersection
            iou = intersection / (union + 1e-8)
            
            if iou > best_iou:
                best_iou = iou
                best_anchor_idx = anchor_idx
        
        ious.append(best_iou)
        if best_anchor_idx >= 0:
            anchor_usage[best_anchor_idx] += 1
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™è¨ˆç®—
    ious_array = np.array(ious)
    performance = {
        'mean_iou': np.mean(ious_array),
        'std_iou': np.std(ious_array),
        'coverage_50': np.mean(ious_array > 0.5),
        'coverage_70': np.mean(ious_array > 0.7),
        'coverage_90': np.mean(ious_array > 0.9),
        'anchor_usage': dict(anchor_usage),
        'most_used_anchor': max(anchor_usage.items(), key=lambda x: x[1])[0] if anchor_usage else -1,
        'unused_anchors': len(all_anchors) - len(anchor_usage)
    }
    
    print(f"   å¹³å‡IoU: {performance['mean_iou']:.4f} (Â±{performance['std_iou']:.4f})")
    print(f"   50%ã‚«ãƒãƒ¬ãƒƒã‚¸: {performance['coverage_50']:.2%}")
    print(f"   70%ã‚«ãƒãƒ¬ãƒƒã‚¸: {performance['coverage_70']:.2%}")
    print(f"   90%ã‚«ãƒãƒ¬ãƒƒã‚¸: {performance['coverage_90']:.2%}")
    print(f"   æœªä½¿ç”¨ã‚¢ãƒ³ã‚«ãƒ¼: {performance['unused_anchors']}/{len(all_anchors)}")
    
    return performance


def evaluate_anchor_quality(dataset, anchors_pixel_per_level: List[List[Tuple[int, int]]], 
                          num_samples: int = 500, input_size: Tuple[int, int] = (640, 512)) -> Dict:
    """
    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚¢ãƒ³ã‚«ãƒ¼å“è³ªã‚’è©•ä¾¡
    """
    print(f"ğŸ“ Evaluating anchor quality with {num_samples} samples...")
    
    # ã¾ãšãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ãƒœãƒƒã‚¯ã‚¹ã‚µã‚¤ã‚ºã‚’åé›†
    box_sizes_pixel = []
    img_w, img_h = input_size
    
    for i in range(min(len(dataset), num_samples)):
        try:
            _, targets = dataset[i]
            if not torch.is_tensor(targets) or targets.size(0) == 0:
                continue
                
            targets_np = targets.cpu().numpy() if targets.is_cuda else targets.numpy()
            
            for target in targets_np:
                if len(target) >= 4:
                    w_rel, h_rel = target[2], target[3]
                    if w_rel > 0 and h_rel > 0:
                        w_pixel = w_rel * img_w
                        h_pixel = h_rel * img_h
                        box_sizes_pixel.append((w_pixel, h_pixel))
                        
        except Exception as e:
            continue
    
    if len(box_sizes_pixel) == 0:
        print("âš ï¸ No valid boxes for evaluation")
        return {"mean_iou": 0, "coverage_50": 0, "coverage_70": 0}
    
    # ã‚¢ãƒ³ã‚«ãƒ¼æ€§èƒ½ã‚’è©•ä¾¡
    box_sizes_np = np.array(box_sizes_pixel)
    performance = evaluate_anchor_performance(box_sizes_np, anchors_pixel_per_level)
    
    return performance


def compare_anchor_sets(dataset, anchors_set1: List[List[Tuple[int, int]]], 
                       anchors_set2: List[List[Tuple[int, int]]],
                       num_samples: int = 500, input_size: Tuple[int, int] = (640, 512)) -> Dict:
    """
    2ã¤ã®ã‚¢ãƒ³ã‚«ãƒ¼ã‚»ãƒƒãƒˆã‚’æ¯”è¼ƒ
    """
    print("\nğŸ†š Comparing anchor sets...")
    
    # ãã‚Œãã‚Œè©•ä¾¡
    print("\n--- Anchor Set 1 (Default) ---")
    perf1 = evaluate_anchor_quality(dataset, anchors_set1, num_samples, input_size)
    
    print("\n--- Anchor Set 2 (Optimized) ---")
    perf2 = evaluate_anchor_quality(dataset, anchors_set2, num_samples, input_size)
    
    # æ”¹å–„ã‚’è¨ˆç®—
    comparison = {
        'set1_performance': perf1,
        'set2_performance': perf2,
        'improvement': {
            'mean_iou': perf2['mean_iou'] - perf1['mean_iou'],
            'coverage_50': perf2['coverage_50'] - perf1['coverage_50'],
            'coverage_70': perf2['coverage_70'] - perf1['coverage_70']
        }
    }
    
    print("\nğŸ“Š Improvement Summary:")
    print(f"   Mean IoU: {comparison['improvement']['mean_iou']:+.4f}")
    print(f"   50% Coverage: {comparison['improvement']['coverage_50']:+.2%}")
    print(f"   70% Coverage: {comparison['improvement']['coverage_70']:+.2%}")
    
    if comparison['improvement']['mean_iou'] > 0.05:
        print("   ğŸ‰ Significant improvement! Box Loss reduction expected.")
    elif comparison['improvement']['mean_iou'] > 0.02:
        print("   âœ… Good improvement achieved.")
    else:
        print("   âš ï¸ Limited improvement. Consider different strategies.")
    
    return comparison