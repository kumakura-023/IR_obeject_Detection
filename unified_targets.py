# ä¿®æ­£ç‰ˆ unified_targets.py (v3)
# ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®ä¸æ•´åˆã«æŸ”è»Ÿã«å¯¾å¿œ

import torch
import torch.nn.functional as F
import numpy as np
from collections import defaultdict
import matplotlib.pyplot as plt
import time
from scipy.optimize import linear_sum_assignment
from typing import List, Tuple, Dict, Optional


import datetime
import hashlib

class VersionTracker:
    """ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨ä¿®æ­£å±¥æ­´ã‚’è¿½è·¡"""
    
    def __init__(self, script_name, version="1.0.0"):
        self.script_name = script_name
        self.version = version
        self.load_time = datetime.datetime.now()
        self.modifications = []
        
    def add_modification(self, description, author="AI Assistant"):
        """ä¿®æ­£å±¥æ­´ã‚’è¿½åŠ """
        timestamp = datetime.datetime.now()
        self.modifications.append({
            'timestamp': timestamp,
            'description': description,
            'author': author
        })
        
    def get_file_hash(self, filepath):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—ï¼ˆå¤‰æ›´æ¤œå‡ºç”¨ï¼‰"""
        try:
            with open(filepath, 'rb') as f:
                content = f.read()
                return hashlib.md5(content).hexdigest()[:8]
        except:
            return "unknown"
    
    def print_version_info(self):
        """ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ã‚’è¡¨ç¤º"""
        print(f"\n{'='*60}")
        print(f"ğŸ“‹ {self.script_name} - Version {self.version}")
        print(f"â° Loaded: {self.load_time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        if hasattr(self, 'file_hash'):
            print(f"ğŸ”— File Hash: {self.file_hash}")
        
        if self.modifications:
            print(f"ğŸ“ Recent Modifications ({len(self.modifications)}):")
            for i, mod in enumerate(self.modifications[-3:], 1):  # æœ€æ–°3ä»¶
                print(f"   {i}. {mod['timestamp'].strftime('%H:%M:%S')} - {mod['description']}")
        
        print(f"{'='*60}\n")

# å„ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãƒˆãƒ©ãƒƒã‚«ãƒ¼ã‚’ä½œæˆ
def create_version_tracker(script_name, filepath=None):
    """ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãƒˆãƒ©ãƒƒã‚«ãƒ¼ã‚’ä½œæˆ"""
    tracker = VersionTracker(script_name)
    
    if filepath:
        tracker.file_hash = tracker.get_file_hash(filepath)
    
    return tracker

# ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
targets_version = create_version_tracker("Unified Targets System v3.2", "unified_targets.py")
targets_version.add_modification("ã‚¢ãƒ³ã‚«ãƒ¼ãƒã‚¹ã‚¯ã‚µã‚¤ã‚ºä¸æ•´åˆä¿®æ­£ (264960 vs 88320)")
targets_version.add_modification("å€™è£œã‚¢ãƒ³ã‚«ãƒ¼é¸å®šãƒ­ã‚¸ãƒƒã‚¯æ”¹å–„")
targets_version.add_modification("float32å‹çµ±ä¸€ã§ãƒ‡ãƒ¼ã‚¿å‹ã‚¨ãƒ©ãƒ¼ä¿®æ­£")
targets_version.add_modification("æ®µéšçš„ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›è¿½åŠ ")
targets_version.add_modification("å¤‰æ•°ã‚¹ã‚³ãƒ¼ãƒ—ã‚¨ãƒ©ãƒ¼ä¿®æ­£ (num_anchors_per_grid)")

# ===== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•° =====
def get_default_anchors() -> List[List[Tuple[int, int]]]:
    """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¢ãƒ³ã‚«ãƒ¼ï¼ˆYOLOv3ãƒ™ãƒ¼ã‚¹ï¼‰"""
    return [
        [(10,13), (16,30), (33,23)],
        [(30,61), (62,45), (59,119)],
        [(116,90), (156,198), (373,326)]
    ]

def analyze_dataset_statistics(dataset, num_samples=1000, input_size=(640, 512)):
    """
    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±è¨ˆåˆ†æã¨ã‚¢ãƒ³ã‚«ãƒ¼æœ€é©åŒ–
    """
    print(f"ğŸ“Š Analyzing dataset statistics from {min(len(dataset), num_samples)} samples...")
    
    box_sizes_pixel = []
    aspect_ratios = []
    scale_distribution = {'small': 0, 'medium': 0, 'large': 0}
    class_distribution = defaultdict(int)
    
    start_time = time.time()
    processed = 0
    img_w, img_h = input_size
    
    for i in range(min(len(dataset), num_samples)):
        if i % 100 == 0 and i > 0:
            elapsed = time.time() - start_time
            print(f"   Progress: {i}/{min(len(dataset), num_samples)} ({i/min(len(dataset), num_samples)*100:.1f}%) - {elapsed:.1f}s")
        
        try:
            _, targets = dataset[i]
            
            if torch.is_tensor(targets):
                if targets.is_cuda:
                    targets = targets.cpu()
                if targets.dim() == 1:
                    targets = targets.unsqueeze(0)
                elif targets.size(0) == 0:
                    continue
                    
                targets_np = targets.numpy()
                
                for j in range(targets_np.shape[0]):
                    target_data = targets_np[j]
                    if len(target_data) >= 4:
                        # ç›¸å¯¾åº§æ¨™ã‹ã‚‰ãƒ”ã‚¯ã‚»ãƒ«åº§æ¨™ã¸å¤‰æ›
                        w_rel, h_rel = target_data[2], target_data[3]
                        if w_rel > 0 and h_rel > 0:
                            w_pixel = w_rel * img_w
                            h_pixel = h_rel * img_h
                            box_sizes_pixel.append((w_pixel, h_pixel))
                            
                            # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”
                            aspect_ratios.append(w_pixel / h_pixel)
                            
                            # ã‚µã‚¤ã‚ºåˆ†å¸ƒ
                            area = w_pixel * h_pixel
                            if area < 32*32:
                                scale_distribution['small'] += 1
                            elif area < 96*96:
                                scale_distribution['medium'] += 1
                            else:
                                scale_distribution['large'] += 1
                            
                            # ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ
                            if len(target_data) > 5:
                                class_scores = target_data[5:]
                                class_id = np.argmax(class_scores)
                                class_distribution[class_id] += 1
                            
                            processed += 1
            
        except Exception as e:
            if i < 5:
                print(f"âš ï¸ Sample {i} analysis failed: {e}")
            continue
    
    total_time = time.time() - start_time
    print(f"âœ… Analysis completed: {processed} valid boxes found in {total_time:.1f}s")
    
    if len(box_sizes_pixel) == 0:
        print("âŒ No valid boxes found for analysis")
        return None, defaultdict(int), {}
    
    box_sizes_np = np.array(box_sizes_pixel)
    
    # çµ±è¨ˆæƒ…å ±è¨ˆç®—
    statistics = {
        'total_boxes': len(box_sizes_np),
        'avg_width': np.mean(box_sizes_np[:, 0]),
        'avg_height': np.mean(box_sizes_np[:, 1]),
        'std_width': np.std(box_sizes_np[:, 0]),
        'std_height': np.std(box_sizes_np[:, 1]),
        'avg_aspect_ratio': np.mean(aspect_ratios),
        'scale_distribution': scale_distribution,
        'processing_time': total_time
    }
    
    # çµ±è¨ˆè¡¨ç¤º
    print(f"\nğŸ“ˆ Dataset Statistics:")
    print(f"   Total boxes: {statistics['total_boxes']}")
    print(f"   Avg size: {statistics['avg_width']:.1f} x {statistics['avg_height']:.1f} pixels")
    print(f"   Size distribution: Small={scale_distribution['small']}, Medium={scale_distribution['medium']}, Large={scale_distribution['large']}")
    
    # ã‚¢ãƒ³ã‚«ãƒ¼æœ€é©åŒ–
    print("\nğŸ”§ Generating optimized anchors...")
    anchor_start = time.time()
    optimized_anchors_pixel = generate_anchors_kmeans(box_sizes_np, num_anchors=9)
    anchor_time = time.time() - anchor_start
    print(f"âœ… Anchor optimization completed in {anchor_time:.1f}s")
    
    return optimized_anchors_pixel, dict(class_distribution), statistics


def generate_anchors_kmeans(box_sizes_pixel: np.ndarray, num_anchors: int = 9, max_iter: int = 300) -> List[List[Tuple[int, int]]]:
    """
    K-means++ã§ã‚¢ãƒ³ã‚«ãƒ¼ã‚’æœ€é©åŒ–
    """
    from sklearn.cluster import KMeans
    
    print(f"ğŸ”§ Generating {num_anchors} anchors using K-means++ (max {max_iter} iterations)...")
    
    if len(box_sizes_pixel) < num_anchors:
        print("âš ï¸ Not enough boxes, using default anchors")
        return get_default_anchors()
    
    # K-means++å®Ÿè¡Œ
    kmeans = KMeans(
        n_clusters=num_anchors, 
        init='k-means++',
        random_state=42, 
        max_iter=max_iter,
        n_init=10
    )
    clusters = kmeans.fit(box_sizes_pixel)
    
    # ã‚¯ãƒ©ã‚¹ã‚¿ä¸­å¿ƒã‚’å–å¾—
    anchors_centers = clusters.cluster_centers_
    
    # é¢ç©ã§ã‚½ãƒ¼ãƒˆ
    areas = anchors_centers[:, 0] * anchors_centers[:, 1]
    sorted_indices = np.argsort(areas)
    sorted_anchors = anchors_centers[sorted_indices]
    
    # 3ã¤ã®FPNãƒ¬ãƒ™ãƒ«ã«åˆ†å‰²
    anchors_per_level = num_anchors // 3
    optimized_anchors = []
    
    for level in range(3):
        start_idx = level * anchors_per_level
        if level == 2:  # æœ€å¾Œã®ãƒ¬ãƒ™ãƒ«ã¯æ®‹ã‚Šå…¨éƒ¨
            end_idx = num_anchors
        else:
            end_idx = (level + 1) * anchors_per_level
            
        level_anchors = []
        for idx in range(start_idx, end_idx):
            if idx < len(sorted_anchors):
                w, h = sorted_anchors[idx]
                level_anchors.append((int(w), int(h)))
        
        # ä¸è¶³åˆ†ã‚’è£œå®Œ
        while len(level_anchors) < anchors_per_level and level_anchors:
            level_anchors.append(level_anchors[-1])
        
        optimized_anchors.append(level_anchors)
    
    print("ğŸ‰ Generated optimized anchors:")
    for i, level in enumerate(optimized_anchors):
        print(f"   Level {i}: {level}")
    
    return optimized_anchors


def prepare_anchor_grid_info(anchors_pixel_per_level: List[List[Tuple[int, int]]], 
                           model_grid_sizes: List[Tuple[int, int]], 
                           input_size_wh: Tuple[int, int]) -> Dict:
    """
    ã‚¢ãƒ³ã‚«ãƒ¼ã¨ã‚°ãƒªãƒƒãƒ‰æƒ…å ±ã‚’æº–å‚™
    """
    img_w, img_h = input_size_wh
    strides_per_level = []
    
    # ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰è¨ˆç®—
    for level_idx, (grid_h, grid_w) in enumerate(model_grid_sizes):
        stride = img_h // grid_h  # é€šå¸¸ã¯æ­£æ–¹å½¢ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰
        strides_per_level.append(stride)
    
    return {
        'anchors_pixel_per_level': anchors_pixel_per_level,
        'strides_per_level': strides_per_level,
        'grid_sizes': model_grid_sizes,
        'input_size': input_size_wh
    }


def build_targets(
    predictions: torch.Tensor,
    targets: List[torch.Tensor],
    anchor_info: Dict,
    num_classes: int,
    topk_candidates: int = 10,
    iou_threshold: float = 0.5,
):
    # â˜…â˜…â˜… ãƒãƒ¼ã‚¸ãƒ§ãƒ³è¿½è·¡ï¼ˆä¿®æ­£ç‰ˆï¼‰ â˜…â˜…â˜…
    VERSION = "3.4-clean-fix"
    
    # ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±è¡¨ç¤ºï¼ˆåˆå›ã®ã¿ï¼‰
    if not hasattr(build_targets, '_version_printed'):
        print(f"\n{'='*60}")
        print(f"ğŸ“‹ Unified Targets System v{VERSION}")
        print(f"â° Clean Fix Applied: 2025-06-09 16:00")
        print(f"ğŸ“ Fixed: Variable scope, duplicate code, naming issues")
        print(f"{'='*60}\n")
        build_targets._version_printed = True
    
    B, N, C = predictions.shape
    device = predictions.device
    
    print(f"ğŸ”§ [TARGETS v{VERSION}] Processing: {predictions.shape}")
    
    # anchor_infoã‹ã‚‰å¿…è¦ãªæƒ…å ±ã‚’å–å¾—
    grid_sizes = anchor_info['grid_sizes']
    input_w, input_h = anchor_info['input_size']
    
    # â˜…â˜…â˜… ä¿®æ­£1: å¤‰æ•°ã‚’ä½¿ç”¨å‰ã«å®šç¾© â˜…â˜…â˜…
    # ã‚°ãƒªãƒƒãƒ‰ãƒã‚¤ãƒ³ãƒˆæ•°ã‚’è¨ˆç®—
    num_grid_points_per_level = [h * w for h, w in grid_sizes]
    total_grid_points = sum(num_grid_points_per_level)
    num_anchors_per_grid = N // total_grid_points  # è‡ªå‹•æ¤œå‡º
    
    # å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
    if num_anchors_per_grid <= 0:
        print(f"âŒ Invalid num_anchors_per_grid: {num_anchors_per_grid}")
        print(f"   N: {N}, total_grid_points: {total_grid_points}")
        num_anchors_per_grid = 3  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€¤

    # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ï¼ˆæœ€å°é™ï¼‰
    if B <= 2:
        print(f"ğŸ”§ Debug: total_grid_points = {total_grid_points}, num_anchors_per_grid = {num_anchors_per_grid}")

    # â˜…â˜…â˜… ä¿®æ­£2: ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰æƒ…å ±ã®å‡¦ç† â˜…â˜…â˜…
    if 'strides' in anchor_info:
        strides_data = anchor_info['strides']
        if isinstance(strides_data, list):
            stride_values = strides_data
        else:
            stride_values = [input_h // gs[0] for gs in grid_sizes]
    else:
        stride_values = [input_h // gs[0] for gs in grid_sizes]
    
    # â˜…â˜…â˜… ä¿®æ­£3: ã‚¢ãƒ³ã‚«ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®å‡¦ç† â˜…â˜…â˜…
    if isinstance(anchor_info['anchor_points'], list):
        anchor_points_per_level = [ap.to(device) for ap in anchor_info['anchor_points']]
    else:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šã‚°ãƒªãƒƒãƒ‰ã‹ã‚‰è¨ˆç®—
        anchor_points_per_level = []
        for h, w in grid_sizes:
            grid_y, grid_x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing='ij')
            grid = torch.stack((grid_x, grid_y), 2).view(-1, 2)
            anchor_points = (grid.float() + 0.5)
            anchor_points_per_level.append(anchor_points.to(device))
    
    # â˜…â˜…â˜… ä¿®æ­£4: ã‚¢ãƒ³ã‚«ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®æ‹¡å¼µ â˜…â˜…â˜…
    anchor_points_expanded = []
    strides_expanded = []
    
    for level_idx, (anchor_points, stride_val) in enumerate(zip(anchor_points_per_level, stride_values)):
        num_points = anchor_points.shape[0]
        
        for anchor_idx in range(num_anchors_per_grid):
            anchor_points_scaled = anchor_points * stride_val
            anchor_points_expanded.append(anchor_points_scaled)
            strides_expanded.append(torch.full((num_points,), stride_val, device=device))

    anchor_points_flat = torch.cat(anchor_points_expanded, dim=0)
    strides_flat = torch.cat(strides_expanded, dim=0)

    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ†ãƒ³ã‚½ãƒ«åˆæœŸåŒ–
    target_boxes = torch.zeros((B, N, 4), device=device)
    target_obj = torch.zeros((B, N), device=device)
    target_cls = torch.zeros((B, N, num_classes), device=device)
    
    for b_idx in range(B):
        batch_targets = targets[b_idx]
        if len(batch_targets) == 0:
            continue

        gt_boxes_rel = batch_targets[:, :4]
        gt_boxes_abs = gt_boxes_rel * torch.tensor([input_w, input_h, input_w, input_h], device=device)
        gt_classes = batch_targets[:, 5:]
        num_gt = len(gt_boxes_abs)

        # â˜…â˜…â˜… ä¿®æ­£5: å€™è£œé ˜åŸŸé¸å®š â˜…â˜…â˜…
        gt_cx, gt_cy = gt_boxes_abs[:, 0], gt_boxes_abs[:, 1]
        
        is_in_center_list = []
        start_idx = 0
        
        for level_idx, stride_val in enumerate(stride_values):
            num_points_level = num_grid_points_per_level[level_idx]
            end_idx = start_idx + num_points_level * num_anchors_per_grid
            
            level_anchor_points = anchor_points_flat[start_idx:end_idx]
            
            offset = stride_val / 2
            x_lim = gt_cx.unsqueeze(1) + torch.tensor([-offset, offset], device=device).unsqueeze(0)
            y_lim = gt_cy.unsqueeze(1) + torch.tensor([-offset, offset], device=device).unsqueeze(0)
            
            anchor_x = level_anchor_points[:, 0].unsqueeze(0)
            anchor_y = level_anchor_points[:, 1].unsqueeze(0)
            
            is_in_x = (anchor_x > x_lim[:, 0].unsqueeze(1)) & (anchor_x < x_lim[:, 1].unsqueeze(1))
            is_in_y = (anchor_y > y_lim[:, 0].unsqueeze(1)) & (anchor_y < y_lim[:, 1].unsqueeze(1))
            
            is_in_center = is_in_x & is_in_y
            is_in_center_list.append(is_in_center.T)
            
            start_idx = end_idx

        is_in_center = torch.cat(is_in_center_list, dim=0)
        candidate_mask = is_in_center.any(dim=1)
        
        # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ï¼ˆæœ€å°é™ï¼‰
        if b_idx == 0:
            print(f"ğŸ”§ [v{VERSION}] Batch {b_idx}: candidates = {candidate_mask.sum()}")

        if not candidate_mask.any():
            print(f"âš ï¸ No candidates found for batch {b_idx}")
            continue

        # â˜…â˜…â˜… ä¿®æ­£6: ãƒ†ãƒ³ã‚½ãƒ«æ“ä½œï¼ˆã‚¯ãƒªãƒ¼ãƒ³ç‰ˆï¼‰ â˜…â˜…â˜…
        try:
            # äºˆæ¸¬å€¤ã®æŠ½å‡º
            pred_logits = predictions[b_idx][candidate_mask]
            pred_boxes = pred_logits[:, :4].float()
            pred_obj = pred_logits[:, 4]
            pred_cls = pred_logits[:, 5:]
            
            # ã‚¢ãƒ³ã‚«ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®æŠ½å‡º
            candidate_anchor_points = anchor_points_flat[candidate_mask].float()
            candidate_strides = strides_flat[candidate_mask].float()
            
            # ãƒœãƒƒã‚¯ã‚¹ãƒ‡ã‚³ãƒ¼ãƒ‰
            decoded_pred_boxes = torch.cat(
                (
                    (pred_boxes[:, :2].sigmoid() * 2 - 0.5 + candidate_anchor_points) * candidate_strides.unsqueeze(1),
                    (pred_boxes[:, 2:].sigmoid() * 2)**2 * candidate_strides.unsqueeze(1).repeat(1, 2)
                ),
                dim=-1
            )
            
            # IoUè¨ˆç®—
            gt_boxes_abs_float = gt_boxes_abs.float()
            pair_wise_iou = get_ious(gt_boxes_abs_float, decoded_pred_boxes, box_format='xywh')
            iou_cost = -torch.log(pair_wise_iou + 1e-8)
            
            # ã‚¯ãƒ©ã‚¹åˆ†é¡ã‚³ã‚¹ãƒˆ
            gt_cls_matrix = F.one_hot(torch.argmax(gt_classes, dim=1), num_classes).float()
            pred_cls_sigmoid = pred_cls.sigmoid().float()
            
            pred_cls_input = pred_cls_sigmoid.unsqueeze(0).repeat(num_gt, 1, 1).to(device).float()
            gt_cls_input = gt_cls_matrix.unsqueeze(1).repeat(1, pred_cls_sigmoid.shape[0], 1).to(device).float()
            
            cls_cost = F.binary_cross_entropy(
                pred_cls_input,
                gt_cls_input,
                reduction='none'
            ).sum(-1)
            
            # ã‚³ã‚¹ãƒˆãƒãƒˆãƒªãƒƒã‚¯ã‚¹è¨ˆç®—
            is_in_center_candidates = is_in_center[candidate_mask]
            cost_matrix = cls_cost + 3.0 * iou_cost + 100000.0 * (~is_in_center_candidates.T)
            
            if b_idx == 0:
                print(f"ğŸ‰ [v{VERSION}] Cost matrix computed: {cost_matrix.shape}")
            
            # â˜…â˜…â˜… ä¿®æ­£7: Dynamic K ãƒãƒƒãƒãƒ³ã‚° â˜…â˜…â˜…
            n_candidate_k = min(topk_candidates, pred_boxes.shape[0])
            if n_candidate_k > 0:
                topk_ious, _ = torch.topk(pair_wise_iou, n_candidate_k, dim=1)
                dynamic_ks = torch.clamp(topk_ious.sum(1).int(), min=1)
                
                matching_matrix = torch.zeros_like(cost_matrix)
                for gt_i in range(num_gt):
                    if dynamic_ks[gt_i] > 0:
                        _, pos_idx = torch.topk(cost_matrix[gt_i], k=dynamic_ks[gt_i], largest=False)
                        matching_matrix[gt_i, pos_idx] = 1.0

                # ç«¶åˆè§£æ±º
                anchor_matching_gt = matching_matrix.sum(0)
                if (anchor_matching_gt > 1).any():
                    conflicting_indices = torch.where(anchor_matching_gt > 1)[0]
                    cost_matrix_conflict = cost_matrix[:, conflicting_indices]
                    _, cost_argmin = torch.min(cost_matrix_conflict, dim=0)
                    
                    matching_matrix[:, conflicting_indices] = 0.0
                    matching_matrix[cost_argmin, conflicting_indices] = 1.0
                
                # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå‰²ã‚Šå½“ã¦
                fg_mask_in_cand = (matching_matrix.sum(0) > 0)
                if fg_mask_in_cand.any():
                    matched_gt_inds = matching_matrix[:, fg_mask_in_cand].argmax(0)
                    
                    candidate_indices = torch.where(candidate_mask)[0]
                    final_pos_indices = candidate_indices[fg_mask_in_cand]
                    
                    target_obj[b_idx, final_pos_indices] = 1.0
                    target_cls[b_idx, final_pos_indices] = gt_classes[matched_gt_inds]
                    target_boxes[b_idx, final_pos_indices] = gt_boxes_abs[matched_gt_inds]
            
        except Exception as step_error:
            print(f"âŒ [v{VERSION}] Error in batch {b_idx}: {step_error}")
            print(f"   Error type: {type(step_error).__name__}")
            continue

    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒœãƒƒã‚¯ã‚¹ã‚’æ­£è¦åŒ–
    normalizer = torch.tensor([input_w, input_h, input_w, input_h], device=device).unsqueeze(0).unsqueeze(0)
    target_boxes /= (normalizer + 1e-6)

    return {
        'boxes': target_boxes,
        'objectness': target_obj,
        'classes': target_cls
    }


def evaluate_anchor_performance(box_sizes_pixel: np.ndarray, anchors_pixel_per_level: List[List[Tuple[int, int]]]) -> Dict:
    """
    ã‚¢ãƒ³ã‚«ãƒ¼æ€§èƒ½ã‚’è©³ç´°è©•ä¾¡
    """
    print("ğŸ“ˆ Evaluating anchor performance...")
    
    # å…¨ã‚¢ãƒ³ã‚«ãƒ¼ã‚’ãƒ•ãƒ©ãƒƒãƒˆåŒ–
    all_anchors = []
    for level_anchors in anchors_pixel_per_level:
        all_anchors.extend(level_anchors)
    
    ious = []
    anchor_usage = defaultdict(int)
    
    for box_w, box_h in box_sizes_pixel:
        box_area = box_w * box_h
        best_iou = 0
        best_anchor_idx = -1
        
        for anchor_idx, (anchor_w, anchor_h) in enumerate(all_anchors):
            # IoUè¨ˆç®—
            inter_w = min(box_w, anchor_w)
            inter_h = min(box_h, anchor_h)
            intersection = inter_w * inter_h
            
            anchor_area = anchor_w * anchor_h
            union = box_area + anchor_area - intersection
            iou = intersection / (union + 1e-8)
            
            if iou > best_iou:
                best_iou = iou
                best_anchor_idx = anchor_idx
        
        ious.append(best_iou)
        if best_anchor_idx >= 0:
            anchor_usage[best_anchor_idx] += 1
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™è¨ˆç®—
    ious_array = np.array(ious)
    performance = {
        'mean_iou': np.mean(ious_array),
        'std_iou': np.std(ious_array),
        'coverage_50': np.mean(ious_array > 0.5),
        'coverage_70': np.mean(ious_array > 0.7),
        'coverage_90': np.mean(ious_array > 0.9),
        'anchor_usage': dict(anchor_usage),
        'most_used_anchor': max(anchor_usage.items(), key=lambda x: x[1])[0] if anchor_usage else -1,
        'unused_anchors': len(all_anchors) - len(anchor_usage)
    }
    
    print(f"   å¹³å‡IoU: {performance['mean_iou']:.4f} (Â±{performance['std_iou']:.4f})")
    print(f"   50%ã‚«ãƒãƒ¬ãƒƒã‚¸: {performance['coverage_50']:.2%}")
    print(f"   70%ã‚«ãƒãƒ¬ãƒƒã‚¸: {performance['coverage_70']:.2%}")
    print(f"   90%ã‚«ãƒãƒ¬ãƒƒã‚¸: {performance['coverage_90']:.2%}")
    print(f"   æœªä½¿ç”¨ã‚¢ãƒ³ã‚«ãƒ¼: {performance['unused_anchors']}/{len(all_anchors)}")
    
    return performance


def evaluate_anchor_quality(dataset, anchors_pixel_per_level: List[List[Tuple[int, int]]], 
                          num_samples: int = 500, input_size: Tuple[int, int] = (640, 512)) -> Dict:
    """
    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚¢ãƒ³ã‚«ãƒ¼å“è³ªã‚’è©•ä¾¡
    """
    print(f"ğŸ“ Evaluating anchor quality with {num_samples} samples...")
    
    # ã¾ãšãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ãƒœãƒƒã‚¯ã‚¹ã‚µã‚¤ã‚ºã‚’åé›†
    box_sizes_pixel = []
    img_w, img_h = input_size
    
    for i in range(min(len(dataset), num_samples)):
        try:
            _, targets = dataset[i]
            if not torch.is_tensor(targets) or targets.size(0) == 0:
                continue
                
            targets_np = targets.cpu().numpy() if targets.is_cuda else targets.numpy()
            
            for target in targets_np:
                if len(target) >= 4:
                    w_rel, h_rel = target[2], target[3]
                    if w_rel > 0 and h_rel > 0:
                        w_pixel = w_rel * img_w
                        h_pixel = h_rel * img_h
                        box_sizes_pixel.append((w_pixel, h_pixel))
                        
        except Exception as e:
            continue
    
    if len(box_sizes_pixel) == 0:
        print("âš ï¸ No valid boxes for evaluation")
        return {"mean_iou": 0, "coverage_50": 0, "coverage_70": 0}
    
    # ã‚¢ãƒ³ã‚«ãƒ¼æ€§èƒ½ã‚’è©•ä¾¡
    box_sizes_np = np.array(box_sizes_pixel)
    performance = evaluate_anchor_performance(box_sizes_np, anchors_pixel_per_level)
    
    return performance


def compare_anchor_sets(dataset, anchors_set1: List[List[Tuple[int, int]]], 
                       anchors_set2: List[List[Tuple[int, int]]],
                       num_samples: int = 500, input_size: Tuple[int, int] = (640, 512)) -> Dict:
    """
    2ã¤ã®ã‚¢ãƒ³ã‚«ãƒ¼ã‚»ãƒƒãƒˆã‚’æ¯”è¼ƒ
    """
    print("\nğŸ†š Comparing anchor sets...")
    
    # ãã‚Œãã‚Œè©•ä¾¡
    print("\n--- Anchor Set 1 (Default) ---")
    perf1 = evaluate_anchor_quality(dataset, anchors_set1, num_samples, input_size)
    
    print("\n--- Anchor Set 2 (Optimized) ---")
    perf2 = evaluate_anchor_quality(dataset, anchors_set2, num_samples, input_size)
    
    # æ”¹å–„ã‚’è¨ˆç®—
    comparison = {
        'set1_performance': perf1,
        'set2_performance': perf2,
        'improvement': {
            'mean_iou': perf2['mean_iou'] - perf1['mean_iou'],
            'coverage_50': perf2['coverage_50'] - perf1['coverage_50'],
            'coverage_70': perf2['coverage_70'] - perf1['coverage_70']
        }
    }
    
    print("\nğŸ“Š Improvement Summary:")
    print(f"   Mean IoU: {comparison['improvement']['mean_iou']:+.4f}")
    print(f"   50% Coverage: {comparison['improvement']['coverage_50']:+.2%}")
    print(f"   70% Coverage: {comparison['improvement']['coverage_70']:+.2%}")
    
    if comparison['improvement']['mean_iou'] > 0.05:
        print("   ğŸ‰ Significant improvement! Box Loss reduction expected.")
    elif comparison['improvement']['mean_iou'] > 0.02:
        print("   âœ… Good improvement achieved.")
    else:
        print("   âš ï¸ Limited improvement. Consider different strategies.")
    
    return comparison

def xywh2xyxy(x):
    y = x.new(x.shape)
    y[..., 0] = x[..., 0] - x[..., 2] / 2
    y[..., 1] = x[..., 1] - x[..., 3] / 2
    y[..., 2] = x[..., 0] + x[..., 2] / 2
    y[..., 3] = x[..., 1] + x[..., 3] / 2
    return y

def get_ious(bboxes1, bboxes2, box_format='xyxy', iou_type='iou'):
    if bboxes1.shape[0] == 0 or bboxes2.shape[0] == 0:
        return torch.zeros(bboxes1.shape[0], bboxes2.shape[0], device=bboxes1.device)
        
    if box_format == 'xywh':
        bboxes1 = xywh2xyxy(bboxes1)
        bboxes2 = xywh2xyxy(bboxes2)

    b1_x1, b1_y1, b1_x2, b1_y2 = bboxes1.split(1, -1)
    b2_x1, b2_y1, b2_x2, b2_y2 = bboxes2.split(1, -1)
    
    inter_x1 = torch.max(b1_x1, b2_x1.transpose(0, 1))
    inter_y1 = torch.max(b1_y1, b2_y1.transpose(0, 1))
    inter_x2 = torch.min(b1_x2, b2_x2.transpose(0, 1))
    inter_y2 = torch.min(b1_y2, b2_y2.transpose(0, 1))

    inter_area = torch.clamp(inter_x2 - inter_x1, min=0) * torch.clamp(inter_y2 - inter_y1, min=0)
    
    area1 = (b1_x2 - b1_x1) * (b1_y2 - b1_y1)
    area2 = (b2_x2 - b2_x1) * (b2_y2 - b2_y1)
    union_area = area1 + area2.transpose(0, 1) - inter_area
    
    return inter_area / (union_area + 1e-6)