# anchor_generator.py - Step 1: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå›ºæœ‰ã‚¢ãƒ³ã‚«ãƒ¼ç”Ÿæˆ

import numpy as np
import torch
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from dataset import FLIRDataset
from config import Config


# â˜…â˜…â˜… å…±æœ‰VersionTrackerã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ â˜…â˜…â˜…
from version_tracker import create_version_tracker, VersionTracker

# ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
loss_version = create_version_tracker("Loss System v1.1", "anchor_generator.py")
loss_version.add_modification("ã‚¢ãƒ³ã‚«ãƒ¼ç”Ÿæˆå®Ÿè£…")

def generate_anchors_kmeans(dataset, k=9, img_size=416, sample_limit=1000):
    """
    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå›ºæœ‰ã®ã‚¢ãƒ³ã‚«ãƒ¼ã‚’K-means++ã§ç”Ÿæˆ
    
    Args:
        dataset: FLIRDataset
        k: ã‚¢ãƒ³ã‚«ãƒ¼æ•°ï¼ˆ9å€‹: 3ã‚¹ã‚±ãƒ¼ãƒ« Ã— 3ã‚¢ãƒ³ã‚«ãƒ¼ï¼‰
        img_size: ç”»åƒã‚µã‚¤ã‚º
        sample_limit: å‡¦ç†ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°ä¸Šé™ï¼ˆé«˜é€ŸåŒ–ï¼‰
    
    Returns:
        anchors_dict: ã‚¹ã‚±ãƒ¼ãƒ«åˆ¥ã‚¢ãƒ³ã‚«ãƒ¼è¾æ›¸
        stats: ç”Ÿæˆçµ±è¨ˆæƒ…å ±
    """
    print(f"ğŸ” Step 1: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå›ºæœ‰ã‚¢ãƒ³ã‚«ãƒ¼ç”Ÿæˆé–‹å§‹")
    print(f"   ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚º: {len(dataset)}")
    print(f"   ã‚µãƒ³ãƒ—ãƒ«åˆ¶é™: {min(len(dataset), sample_limit)}")
    print(f"   ã‚¢ãƒ³ã‚«ãƒ¼æ•°: {k}")
    print("-" * 50)
    
    # 1. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹åé›†
    all_boxes = []
    sample_count = 0
    valid_boxes = 0
    
    for i in range(min(len(dataset), sample_limit)):
        try:
            _, targets = dataset[i]
            
            if len(targets) > 0:
                for target in targets:
                    # target format: [class, cx, cy, w, h] (normalized)
                    if len(target) >= 5:
                        w, h = target[3].item(), target[4].item()
                        
                        # æ­£å¸¸å€¤ãƒã‚§ãƒƒã‚¯
                        if 0 < w < 1 and 0 < h < 1:
                            # å®Ÿéš›ã®ãƒ”ã‚¯ã‚»ãƒ«ã‚µã‚¤ã‚ºã«å¤‰æ›
                            w_pixels = w * img_size
                            h_pixels = h * img_size
                            
                            all_boxes.append([w_pixels, h_pixels])
                            valid_boxes += 1
            
            sample_count += 1
            
            # é€²æ—è¡¨ç¤º
            if (i + 1) % 100 == 0:
                print(f"   å‡¦ç†æ¸ˆã¿: {i + 1}/{min(len(dataset), sample_limit)}, "
                      f"æœ‰åŠ¹ãƒœãƒƒã‚¯ã‚¹: {valid_boxes}")
                
        except Exception as e:
            print(f"   è­¦å‘Š: ã‚µãƒ³ãƒ—ãƒ«{i}ã§ã‚¨ãƒ©ãƒ¼: {e}")
            continue
    
    if len(all_boxes) == 0:
        raise ValueError("æœ‰åŠ¹ãªãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    all_boxes = np.array(all_boxes)
    print(f"\nğŸ“Š åé›†çµæœ:")
    print(f"   å‡¦ç†ã‚µãƒ³ãƒ—ãƒ«æ•°: {sample_count}")
    print(f"   æœ‰åŠ¹ãƒœãƒƒã‚¯ã‚¹æ•°: {len(all_boxes)}")
    print(f"   å¹…ã®ç¯„å›²: {all_boxes[:, 0].min():.1f} - {all_boxes[:, 0].max():.1f} pixels")
    print(f"   é«˜ã•ã®ç¯„å›²: {all_boxes[:, 1].min():.1f} - {all_boxes[:, 1].max():.1f} pixels")
    
    # 2. K-means++ã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
    print(f"\nğŸ”„ K-means++ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œä¸­...")
    
    try:
        kmeans = KMeans(
            n_clusters=k, 
            init='k-means++', 
            n_init=10, 
            random_state=42,
            max_iter=300
        )
        kmeans.fit(all_boxes)
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ä¸­å¿ƒã‚’ã‚¢ãƒ³ã‚«ãƒ¼ã¨ã—ã¦ä½¿ç”¨
        anchors = kmeans.cluster_centers_
        
        print(f"   ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Œäº†")
        print(f"   åæŸ: {kmeans.n_iter_}å›åå¾©")
        print(f"   æ…£æ€§: {kmeans.inertia_:.2f}")
        
    except Exception as e:
        print(f"âŒ K-meansã‚¨ãƒ©ãƒ¼: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ‰‹å‹•ã§ã‚¢ãƒ³ã‚«ãƒ¼ç”Ÿæˆ
        print("   ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ‰‹å‹•ã‚¢ãƒ³ã‚«ãƒ¼ç”Ÿæˆ")
        anchors = generate_fallback_anchors(all_boxes, k)
    
    # 3. ã‚¢ãƒ³ã‚«ãƒ¼ã‚’é¢ç©é †ã«ã‚½ãƒ¼ãƒˆ
    areas = anchors[:, 0] * anchors[:, 1]
    sorted_indices = np.argsort(areas)
    anchors = anchors[sorted_indices]
    
    # 4. ã‚¹ã‚±ãƒ¼ãƒ«åˆ¥ã«åˆ†å‰²
    anchors_dict = organize_anchors_by_scale(anchors)
    
    # 5. çµ±è¨ˆæƒ…å ±ç”Ÿæˆ
    stats = calculate_anchor_stats(all_boxes, anchors_dict)
    
    print(f"\nâœ… ã‚¢ãƒ³ã‚«ãƒ¼ç”Ÿæˆå®Œäº†!")
    print_anchor_results(anchors_dict, stats)
    
    return anchors_dict, stats

def generate_fallback_anchors(boxes, k=9):
    """K-meanså¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒ³ã‚«ãƒ¼ç”Ÿæˆ"""
    print("   ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åˆ†ä½æ•°ãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒ³ã‚«ãƒ¼ç”Ÿæˆ")
    
    # å¹…ã¨é«˜ã•ã®åˆ†ä½æ•°ã‚’è¨ˆç®—
    widths = boxes[:, 0]
    heights = boxes[:, 1]
    
    anchors = []
    for i in range(k):
        # åˆ†ä½æ•°ãƒ™ãƒ¼ã‚¹ã§ã‚¢ãƒ³ã‚«ãƒ¼ã‚µã‚¤ã‚ºã‚’æ±ºå®š
        w_percentile = (i + 1) * (100 / (k + 1))
        h_percentile = (i + 1) * (100 / (k + 1))
        
        w = np.percentile(widths, w_percentile)
        h = np.percentile(heights, h_percentile)
        
        anchors.append([w, h])
    
    return np.array(anchors)

def organize_anchors_by_scale(anchors):
    """9å€‹ã®ã‚¢ãƒ³ã‚«ãƒ¼ã‚’3ã¤ã®ã‚¹ã‚±ãƒ¼ãƒ«ï¼ˆå°ãƒ»ä¸­ãƒ»å¤§ï¼‰ã«åˆ†å‰²"""
    
    if len(anchors) != 9:
        raise ValueError(f"ã‚¢ãƒ³ã‚«ãƒ¼æ•°ã¯9å€‹ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ç¾åœ¨: {len(anchors)}")
    
    # é¢ç©ã§3ã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†å‰²
    small_anchors = [(int(w), int(h)) for w, h in anchors[:3]]    # æœ€å°ã®3å€‹
    medium_anchors = [(int(w), int(h)) for w, h in anchors[3:6]]  # ä¸­é–“ã®3å€‹
    large_anchors = [(int(w), int(h)) for w, h in anchors[6:9]]   # æœ€å¤§ã®3å€‹
    
    organized_anchors = {
        'small': small_anchors,   # 52x52 ã‚°ãƒªãƒƒãƒ‰ç”¨
        'medium': medium_anchors, # 26x26 ã‚°ãƒªãƒƒãƒ‰ç”¨  
        'large': large_anchors    # 13x13 ã‚°ãƒªãƒƒãƒ‰ç”¨
    }
    
    return organized_anchors

def calculate_iou_wh(box1_wh, box2_wh):
    """å¹…ãƒ»é«˜ã•ã®ã¿ã§IoUè¨ˆç®—ï¼ˆä¸­å¿ƒã¯åŒã˜ã¨ä»®å®šï¼‰"""
    w1, h1 = box1_wh
    w2, h2 = box2_wh
    
    # äº¤å·®é ˜åŸŸï¼ˆä¸­å¿ƒãŒåŒã˜ãªã®ã§ã€å°ã•ã„æ–¹ã®å¹…ãƒ»é«˜ã•ï¼‰
    intersection = min(w1, w2) * min(h1, h2)
    
    # å’Œé ˜åŸŸ
    union = w1 * h1 + w2 * h2 - intersection
    
    return intersection / union if union > 0 else 0.0

def calculate_anchor_stats(boxes, anchors_dict):
    """ã‚¢ãƒ³ã‚«ãƒ¼ã®å“è³ªçµ±è¨ˆã‚’è¨ˆç®—"""
    print(f"\nğŸ“Š ã‚¢ãƒ³ã‚«ãƒ¼å“è³ªè©•ä¾¡ä¸­...")
    
    all_ious = []
    anchor_usage = {scale: [0, 0, 0] for scale in ['small', 'medium', 'large']}
    scale_list = ['small', 'medium', 'large']
    
    # ã‚µãƒ³ãƒ—ãƒ«æ•°åˆ¶é™ï¼ˆè¨ˆç®—é«˜é€ŸåŒ–ï¼‰
    sample_boxes = boxes[:min(len(boxes), 500)]
    
    for box_wh in sample_boxes:
        best_iou = 0
        best_scale = None
        best_anchor_idx = None
        
        # å…¨ã‚¢ãƒ³ã‚«ãƒ¼ã¨æ¯”è¼ƒ
        for scale in scale_list:
            for anchor_idx, anchor_wh in enumerate(anchors_dict[scale]):
                iou = calculate_iou_wh(box_wh, anchor_wh)
                
                if iou > best_iou:
                    best_iou = iou
                    best_scale = scale
                    best_anchor_idx = anchor_idx
        
        all_ious.append(best_iou)
        if best_scale and best_anchor_idx is not None:
            anchor_usage[best_scale][best_anchor_idx] += 1
    
    stats = {
        'avg_iou': np.mean(all_ious),
        'iou_50': np.mean(np.array(all_ious) > 0.5),
        'iou_70': np.mean(np.array(all_ious) > 0.7),
        'anchor_usage': anchor_usage,
        'total_boxes': len(boxes),
        'evaluated_boxes': len(sample_boxes)
    }
    
    return stats

def print_anchor_results(anchors_dict, stats):
    """ã‚¢ãƒ³ã‚«ãƒ¼ç”Ÿæˆçµæœã‚’è¡¨ç¤º"""
    print(f"\nğŸ“‹ ç”Ÿæˆã•ã‚ŒãŸã‚¢ãƒ³ã‚«ãƒ¼:")
    for scale, anchor_list in anchors_dict.items():
        print(f"   {scale:6s} (grid): {anchor_list}")
        areas = [w * h for w, h in anchor_list]
        print(f"   {' ':6s}  (area): {areas}")
    
    print(f"\nğŸ“Š å“è³ªè©•ä¾¡:")
    print(f"   å¹³å‡IoU: {stats['avg_iou']:.3f}")
    print(f"   IoU > 0.5: {stats['iou_50']:.1%}")
    print(f"   IoU > 0.7: {stats['iou_70']:.1%}")
    
    print(f"\nğŸ“ˆ ã‚¢ãƒ³ã‚«ãƒ¼ä½¿ç”¨é »åº¦:")
    for scale, usage in stats['anchor_usage'].items():
        total = sum(usage)
        if total > 0:
            percentages = [f"{u/total*100:.1f}%" for u in usage]
            print(f"   {scale:6s}: {usage} â†’ {percentages}")

def visualize_anchors(anchors_dict, save_path="step1_anchors_visualization.png"):
    """ã‚¢ãƒ³ã‚«ãƒ¼ã‚’å¯è¦–åŒ–"""
    print(f"\nğŸ“Š ã‚¢ãƒ³ã‚«ãƒ¼å¯è¦–åŒ–ä¸­...")
    
    plt.figure(figsize=(15, 5))
    
    scales = ['small', 'medium', 'large']
    grid_sizes = [52, 26, 13]
    colors = ['red', 'green', 'blue']
    
    for i, (scale, grid_size) in enumerate(zip(scales, grid_sizes)):
        plt.subplot(1, 3, i+1)
        
        anchor_set = anchors_dict[scale]
        
        # ã‚°ãƒªãƒƒãƒ‰ã‚»ãƒ«ã‚µã‚¤ã‚º
        cell_size = 416 // grid_size
        
        for j, (w, h) in enumerate(anchor_set):
            # ã‚¢ãƒ³ã‚«ãƒ¼ãƒœãƒƒã‚¯ã‚¹ã‚’æç”»
            rect = plt.Rectangle(
                (cell_size//2 - w//2, cell_size//2 - h//2), 
                w, h, 
                linewidth=2, 
                edgecolor=colors[j], 
                facecolor='none',
                label=f'Anchor {j+1}: ({w}, {h})'
            )
            plt.gca().add_patch(rect)
        
        plt.xlim(0, cell_size)
        plt.ylim(0, cell_size)
        plt.gca().invert_yaxis()
        plt.title(f'{scale.capitalize()} Scale\n({grid_size}x{grid_size} grid)')
        plt.legend()
        plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.show()
    
    print(f"   å¯è¦–åŒ–ä¿å­˜: {save_path}")

def save_anchors_config(anchors_dict, stats, save_path="step1_anchors_config.py"):
    """config.pyç”¨ã®ã‚¢ãƒ³ã‚«ãƒ¼è¨­å®šã‚’ä¿å­˜"""
    
    config_text = f'''# ===== Step 1: ç”Ÿæˆã•ã‚ŒãŸã‚¢ãƒ³ã‚«ãƒ¼è¨­å®š =====
# ç”Ÿæˆæ—¥æ™‚: {import_datetime()}

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå›ºæœ‰ã‚¢ãƒ³ã‚«ãƒ¼
use_anchors = True
anchors = {{
    'small':  {anchors_dict['small']},   # 52x52 grid (å°ç‰©ä½“ç”¨)
    'medium': {anchors_dict['medium']},  # 26x26 grid (ä¸­ç‰©ä½“ç”¨)
    'large':  {anchors_dict['large']}    # 13x13 grid (å¤§ç‰©ä½“ç”¨)
}}

# ã‚¢ãƒ³ã‚«ãƒ¼å“è³ªçµ±è¨ˆ
anchor_quality = {{
    'avg_iou': {stats['avg_iou']:.3f},
    'iou_50_percent': {stats['iou_50']:.3f},
    'iou_70_percent': {stats['iou_70']:.3f},
    'total_boxes_analyzed': {stats['total_boxes']},
    'generation_method': 'K-means++',
    'dataset_specific': True
}}

print(f"ğŸ“Š Step 1ã‚¢ãƒ³ã‚«ãƒ¼è¨­å®šèª­ã¿è¾¼ã¿å®Œäº†")
print(f"   å¹³å‡IoU: {{anchor_quality['avg_iou']}}")
print(f"   IoU>0.5: {{anchor_quality['iou_50_percent']:.1%}}")
'''
    
    with open(save_path, 'w', encoding='utf-8') as f:
        f.write(config_text)
    
    print(f"ğŸ’¾ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {save_path}")
    
    return config_text

def import_datetime():
    """ç¾åœ¨æ™‚åˆ»ã‚’å–å¾—"""
    from datetime import datetime
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

# ===== Step 1å®Ÿè¡Œé–¢æ•° =====
def run_step1():
    """Step 1ã‚’å®Ÿè¡Œ"""
    print("ğŸš€ Step 1: ã‚¢ãƒ³ã‚«ãƒ¼ç”Ÿæˆé–‹å§‹")
    print("=" * 60)
    
    try:
        # 1. è¨­å®šã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿
        cfg = Config()
        print(f"ğŸ“‹ è¨­å®šèª­ã¿è¾¼ã¿å®Œäº†")
        print(f"   ç”»åƒã‚µã‚¤ã‚º: {cfg.img_size}")
        print(f"   ã‚¯ãƒ©ã‚¹æ•°: {cfg.num_classes}")
        
        # 2. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
        dataset = FLIRDataset(
            cfg.train_img_dir, 
            cfg.train_label_dir, 
            cfg.img_size, 
            augment=False  # ã‚¢ãƒ³ã‚«ãƒ¼ç”Ÿæˆæ™‚ã¯æ‹¡å¼µãªã—
        )
        print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿å®Œäº†: {len(dataset)}æš")
        
        # 3. ã‚¢ãƒ³ã‚«ãƒ¼ç”Ÿæˆ
        anchors_dict, stats = generate_anchors_kmeans(dataset, k=9, img_size=cfg.img_size)
        
        # 4. å¯è¦–åŒ–
        visualize_anchors(anchors_dict)
        
        # 5. è¨­å®šä¿å­˜
        config_text = save_anchors_config(anchors_dict, stats)
        
        # 6. çµæœã‚µãƒãƒªãƒ¼
        print("\n" + "=" * 60)
        print("âœ… Step 1å®Œäº†!")
        print("=" * 60)
        print(f"ğŸ“Š ç”Ÿæˆçµæœã‚µãƒãƒªãƒ¼:")
        print(f"   å¹³å‡IoU: {stats['avg_iou']:.3f}")
        print(f"   IoU>0.5ç‡: {stats['iou_50']:.1%}")
        print(f"   è©•ä¾¡ãƒœãƒƒã‚¯ã‚¹æ•°: {stats['evaluated_boxes']}")
        
        if stats['avg_iou'] > 0.4:
            print("ğŸ‰ é«˜å“è³ªãªã‚¢ãƒ³ã‚«ãƒ¼ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸ!")
        elif stats['avg_iou'] > 0.3:
            print("âœ… è‰¯å¥½ãªã‚¢ãƒ³ã‚«ãƒ¼ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸ")
        else:
            print("âš ï¸ ã‚¢ãƒ³ã‚«ãƒ¼å“è³ªãŒä½ã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
        
        print(f"\nğŸ“ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        print(f"   1. å¯è¦–åŒ–çµæœã‚’ç¢ºèª")
        print(f"   2. ã‚¢ãƒ³ã‚«ãƒ¼å“è³ªãŒè‰¯å¥½ã‹è©•ä¾¡")
        print(f"   3. å•é¡Œãªã‘ã‚Œã°Step 2ã«é€²è¡Œ")
        
        return anchors_dict, stats, True
        
    except Exception as e:
        print(f"âŒ Step 1ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        import traceback
        traceback.print_exc()
        return None, None, False

# ===== ä½¿ç”¨ä¾‹ =====
if __name__ == "__main__":
    # Step 1å®Ÿè¡Œ
    anchors, stats, success = run_step1()
    
    if success:
        print("ğŸ‰ Step 1æˆåŠŸ! Step 2ã®æº–å‚™å®Œäº†")
    else:
        print("âŒ Step 1å¤±æ•— - ã‚¨ãƒ©ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
