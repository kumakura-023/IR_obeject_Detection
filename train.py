# train.py - Phase 3 EMA&æ¤œè¨¼åˆ†å‰²å®Œå…¨ç‰ˆ
import torch
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
import time
import os

from config import Config
from dataset import FLIRDataset, collate_fn
from model import SimpleYOLO
from loss import YOLOLoss

# â˜…â˜…â˜… å…±æœ‰VersionTrackerã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ â˜…â˜…â˜…
from version_tracker import (
    create_version_tracker, 
    VersionTracker, 
    show_all_project_versions,
    debug_version_status,
    get_version_count
)

# ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
training_version = create_version_tracker("Training System v1.3 - Phase 3 Complete", "train.py")
training_version.add_modification("å­¦ç¿’ãƒ«ãƒ¼ãƒ—å®Ÿè£…")
training_version.add_modification("ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒ¼ã‚¸ãƒ§ãƒ³è¡¨ç¤ºè¿½åŠ ")
training_version.add_modification("gpuæœªä½¿ç”¨åŸå› ã®è¿½ç©¶")
training_version.add_modification("Phase 3: EMA & æ¤œè¨¼åˆ†å‰²å®Ÿè£…")

# ===== Phase 3: EMAã‚¯ãƒ©ã‚¹å®Ÿè£… =====
class EMAModel:
    """Exponential Moving Average for model parameters"""
    def __init__(self, model, decay=0.9999):
        self.model = model
        self.decay = decay
        self.shadow = {}
        self.backup = {}
        self.register()

    def register(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.shadow[name] = param.data.clone()

    def update(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                assert name in self.shadow
                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]
                self.shadow[name] = new_average.clone()

    def apply_shadow(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                assert name in self.shadow
                self.backup[name] = param.data
                param.data = self.shadow[name]

    def restore(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                assert name in self.backup
                param.data = self.backup[name]
        self.backup = {}

# ===== Phase 3: ãƒ‡ãƒ¼ã‚¿åˆ†å‰²é–¢æ•° =====
def create_train_val_split(dataset, val_split=0.15):
    """è¨“ç·´ãƒ»æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²"""
    total_size = len(dataset)
    val_size = int(total_size * val_split)
    train_size = total_size - val_size
    
    train_dataset, val_dataset = random_split(
        dataset, [train_size, val_size],
        generator=torch.Generator().manual_seed(42)  # å†ç¾æ€§ã®ãŸã‚
    )
    
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å‰²å®Œäº†:")
    print(f"   Train: {train_size} images ({100*(1-val_split):.1f}%)")
    print(f"   Validation: {val_size} images ({100*val_split:.1f}%)")
    return train_dataset, val_dataset

# ===== Phase 3: æ¤œè¨¼é–¢æ•° =====
def validate_model(model, val_dataloader, criterion, device):
    """æ¤œè¨¼å®Ÿè¡Œ"""
    model.eval()
    total_val_loss = 0
    val_batches = 0
    
    with torch.no_grad():
        for images, targets in val_dataloader:
            images = images.to(device, non_blocking=True)
            
            predictions, grid_size = model(images)
            loss = criterion(predictions, targets, grid_size)
            
            total_val_loss += loss.item()
            val_batches += 1
    
    avg_val_loss = total_val_loss / val_batches if val_batches > 0 else float('inf')
    return avg_val_loss

# ===== Phase 3: ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— =====
def setup_dataloaders(cfg):
    """ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆæ¤œè¨¼åˆ†å‰²å¯¾å¿œï¼‰"""
    # å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    full_dataset = FLIRDataset(cfg.train_img_dir, cfg.train_label_dir, cfg.img_size, augment=cfg.augment)
    
    # æ¤œè¨¼åˆ†å‰²
    if cfg.validation_split > 0:
        train_dataset, val_dataset = create_train_val_split(full_dataset, cfg.validation_split)
    else:
        train_dataset = full_dataset
        val_dataset = None
    
    # DataLoaderä½œæˆ
    num_workers = 2 if cfg.device.type == 'cuda' else 0
    pin_memory = cfg.device.type == 'cuda'
    
    train_dataloader = DataLoader(
        train_dataset, 
        batch_size=cfg.batch_size, 
        shuffle=True, 
        collate_fn=collate_fn,
        num_workers=num_workers,
        pin_memory=pin_memory
    )
    
    val_dataloader = None
    if val_dataset:
        val_dataloader = DataLoader(
            val_dataset,
            batch_size=cfg.batch_size,
            shuffle=False,  # æ¤œè¨¼æ™‚ã¯ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ãªã„
            collate_fn=collate_fn,
            num_workers=num_workers,
            pin_memory=pin_memory
        )
    
    print(f"   Train batches: {len(train_dataloader)}")
    if val_dataloader:
        print(f"   Validation batches: {len(val_dataloader)}")
    
    return train_dataloader, val_dataloader

# ===== Phase 3: EMA&æ¤œè¨¼åˆ†å‰²å¯¾å¿œå­¦ç¿’ãƒ«ãƒ¼ãƒ— =====
def optimized_training_loop_with_ema_val(model, train_dataloader, val_dataloader, criterion, cfg):
    """Phase 3å®Œå…¨ç‰ˆ: EMA + æ¤œè¨¼åˆ†å‰²å¯¾å¿œå­¦ç¿’ãƒ«ãƒ¼ãƒ—"""
    
    # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶è¨­å®š
    if cfg.optimizer_type == "AdamW":
        optimizer = optim.AdamW(
            model.parameters(),
            lr=cfg.learning_rate,
            weight_decay=cfg.weight_decay,
            betas=cfg.betas,
            eps=cfg.eps
        )
    else:
        optimizer = optim.Adam(model.parameters(), lr=cfg.learning_rate)
    
    # EMAåˆæœŸåŒ–
    ema = None
    if cfg.use_ema:
        ema = EMAModel(model, decay=cfg.ema_decay)
        print(f"ğŸ”„ EMA initialized with decay {cfg.ema_decay}")
    
    # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©è¨­å®š
    scheduler = None
    if cfg.use_scheduler:
        scheduler = optim.lr_scheduler.CosineAnnealingLR(
            optimizer, 
            T_max=cfg.num_epochs,
            eta_min=cfg.min_lr
        )
    
    # Early Stoppingè¨­å®š
    best_val_loss = float('inf')
    patience_counter = 0
    
    # å­¦ç¿’çµ±è¨ˆ
    train_losses = []
    val_losses = []
    learning_rates = []
    
    print(f"ğŸš€ Phase 3 EMA&æ¤œè¨¼åˆ†å‰²å­¦ç¿’é–‹å§‹")
    print(f"   Optimizer: {cfg.optimizer_type}")
    print(f"   EMA: {'ON' if cfg.use_ema else 'OFF'}")
    print(f"   Validation: {'ON' if val_dataloader else 'OFF'}")
    
    for epoch in range(cfg.num_epochs):
        epoch_start = time.time()
        
        # ===== ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å‡¦ç† =====
        if epoch < cfg.warmup_epochs:
            warmup_lr = cfg.learning_rate * (epoch + 1) / cfg.warmup_epochs
            for param_group in optimizer.param_groups:
                param_group['lr'] = warmup_lr
            print(f"ğŸ”¥ Warmup Epoch {epoch+1}: LR = {warmup_lr:.6f}")
        
        # ===== è¨“ç·´ãƒ•ã‚§ãƒ¼ã‚º =====
        model.train()
        epoch_loss = 0
        batch_count = 0
        
        for batch_idx, (images, targets) in enumerate(train_dataloader):
            images = images.to(cfg.device, non_blocking=True)
            
            # Forward
            predictions, grid_size = model(images)
            loss = criterion(predictions, targets, grid_size)
            
            # Backward
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.gradient_clip)
            optimizer.step()
            
            # EMAæ›´æ–°
            if ema:
                ema.update()
            
            epoch_loss += loss.item()
            batch_count += 1
            
            if batch_idx % 100 == 0:
                current_lr = optimizer.param_groups[0]['lr']
                avg_loss = epoch_loss / (batch_idx + 1)
                print(f"   Batch [{batch_idx:4d}] Loss: {loss.item():8.4f} "
                      f"AvgLoss: {avg_loss:8.4f} LR: {current_lr:.6f}")
        
        avg_train_loss = epoch_loss / batch_count
        
        # ===== æ¤œè¨¼ãƒ•ã‚§ãƒ¼ã‚º =====
        val_loss = float('inf')
        if val_dataloader and epoch % cfg.validate_every == 0:
            # EMAãƒ¢ãƒ‡ãƒ«ã§æ¤œè¨¼
            if ema:
                ema.apply_shadow()
            
            val_loss = validate_model(model, val_dataloader, criterion, cfg.device)
            
            if ema:
                ema.restore()
        
        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©æ›´æ–°
        if scheduler and epoch >= cfg.warmup_epochs:
            scheduler.step()
        
        epoch_time = time.time() - epoch_start
        current_lr = optimizer.param_groups[0]['lr']
        
        # çµ±è¨ˆè¨˜éŒ²
        train_losses.append(avg_train_loss)
        val_losses.append(val_loss)
        learning_rates.append(current_lr)
        
        # ãƒ­ã‚°è¡¨ç¤º
        val_str = f"Val: {val_loss:6.4f}" if val_loss != float('inf') else "Val: ----"
        print(f"\nğŸ“ˆ Epoch [{epoch+1:2d}/{cfg.num_epochs}] "
              f"Train: {avg_train_loss:6.4f} {val_str} "
              f"Time: {epoch_time:4.1f}s LR: {current_lr:.6f}")
        
        # Early Stopping & Best Model Saving
        current_loss = val_loss if val_loss != float('inf') else avg_train_loss
        
        if current_loss < best_val_loss - cfg.min_improvement:
            best_val_loss = current_loss
            patience_counter = 0
            
            # EMAãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
            if ema:
                ema.apply_shadow()
            
            save_best_model_with_ema(model, optimizer, ema, epoch, best_val_loss, cfg)
            
            if ema:
                ema.restore()
            
            print(f"ğŸ‰ New best loss: {best_val_loss:.4f}")
        else:
            patience_counter += 1
            print(f"â³ No improvement for {patience_counter}/{cfg.patience} epochs")
            
            if patience_counter >= cfg.patience:
                print(f"ğŸ›‘ Early stopping triggered")
                break
        
        # å®šæœŸä¿å­˜
        if (epoch + 1) % cfg.save_interval == 0:
            save_checkpoint(model, optimizer, epoch, avg_train_loss, cfg)
        
        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if cfg.device.type == 'cuda':
            torch.cuda.empty_cache()
    
    print(f"\nâœ… Phase 3 EMA&æ¤œè¨¼åˆ†å‰²å­¦ç¿’å®Œäº†!")
    print(f"ğŸ† Best Loss: {best_val_loss:.4f}")
    
    return train_losses, val_losses, learning_rates, best_val_loss

# ===== æ—§å­¦ç¿’ãƒ«ãƒ¼ãƒ—ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚æ®‹ã™ï¼‰ =====
def optimized_training_loop(model, dataloader, criterion, cfg):
    """Phase 3: æœ€é©åŒ–ã•ã‚ŒãŸå­¦ç¿’ãƒ«ãƒ¼ãƒ—ï¼ˆæ—§ç‰ˆãƒ»å¾Œæ–¹äº’æ›æ€§ï¼‰"""
    
    # ===== ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶è¨­å®š =====
    if cfg.optimizer_type == "AdamW":
        optimizer = optim.AdamW(
            model.parameters(),
            lr=cfg.learning_rate,
            weight_decay=cfg.weight_decay,
            betas=cfg.betas,
            eps=cfg.eps
        )
    else:
        optimizer = optim.Adam(model.parameters(), lr=cfg.learning_rate)
    
    # ===== ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©è¨­å®š =====
    scheduler = None
    if cfg.use_scheduler:
        if cfg.scheduler_type == "cosine":
            scheduler = optim.lr_scheduler.CosineAnnealingLR(
                optimizer, 
                T_max=cfg.num_epochs,
                eta_min=cfg.min_lr
            )
        elif cfg.scheduler_type == "step":
            scheduler = optim.lr_scheduler.StepLR(
                optimizer,
                step_size=10,
                gamma=0.1
            )
    
    # ===== Early Stoppingè¨­å®š =====
    best_loss = float('inf')
    patience_counter = 0
    
    # ===== å­¦ç¿’çµ±è¨ˆ =====
    train_losses = []
    learning_rates = []
    
    print(f"ğŸš€ Phase 3 æœ€é©åŒ–å­¦ç¿’é–‹å§‹ï¼ˆæ—§ç‰ˆï¼‰")
    print(f"   Optimizer: {cfg.optimizer_type}")
    print(f"   Scheduler: {cfg.scheduler_type if cfg.use_scheduler else 'None'}")
    print(f"   Initial LR: {cfg.learning_rate}")
    
    for epoch in range(cfg.num_epochs):
        epoch_start = time.time()
        
        # ===== ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å‡¦ç† =====
        if epoch < cfg.warmup_epochs:
            warmup_lr = cfg.learning_rate * (epoch + 1) / cfg.warmup_epochs
            for param_group in optimizer.param_groups:
                param_group['lr'] = warmup_lr
            print(f"ğŸ”¥ Warmup Epoch {epoch+1}: LR = {warmup_lr:.6f}")
        
        # ===== å­¦ç¿’ã‚¨ãƒãƒƒã‚¯ =====
        model.train()
        epoch_loss = 0
        batch_count = 0
        
        for batch_idx, (images, targets) in enumerate(dataloader):
            images = images.to(cfg.device, non_blocking=True)
            
            # Forward
            predictions, grid_size = model(images)
            loss = criterion(predictions, targets, grid_size)
            
            # Backward
            optimizer.zero_grad()
            loss.backward()
            
            # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.gradient_clip)
            
            optimizer.step()
            
            epoch_loss += loss.item()
            batch_count += 1
            
            # ãƒãƒƒãƒãƒ­ã‚°ï¼ˆæ”¹å–„ï¼‰
            if batch_idx % 100 == 0:
                current_lr = optimizer.param_groups[0]['lr']
                avg_loss = epoch_loss / (batch_idx + 1)
                print(f"   Batch [{batch_idx:4d}] Loss: {loss.item():8.4f} "
                      f"AvgLoss: {avg_loss:8.4f} LR: {current_lr:.6f}")
        
        # ===== ã‚¨ãƒãƒƒã‚¯çµ‚äº†å‡¦ç† =====
        avg_epoch_loss = epoch_loss / batch_count
        epoch_time = time.time() - epoch_start
        
        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©æ›´æ–°ï¼ˆã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å¾Œï¼‰
        if scheduler and epoch >= cfg.warmup_epochs:
            scheduler.step()
        
        current_lr = optimizer.param_groups[0]['lr']
        
        # ===== çµ±è¨ˆè¨˜éŒ² =====
        train_losses.append(avg_epoch_loss)
        learning_rates.append(current_lr)
        
        # ===== ãƒ­ã‚°è¡¨ç¤º =====
        print(f"\nğŸ“ˆ Epoch [{epoch+1:2d}/{cfg.num_epochs}] "
              f"Loss: {avg_epoch_loss:8.4f} Time: {epoch_time:5.1f}s LR: {current_lr:.6f}")
        
        # ===== Early Stopping & Model Saving =====
        if avg_epoch_loss < best_loss - cfg.min_improvement:
            best_loss = avg_epoch_loss
            patience_counter = 0
            
            # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä¿å­˜
            save_optimized_model(model, optimizer, epoch, best_loss, cfg)
            print(f"ğŸ‰ New best loss: {best_loss:.4f}")
            
        else:
            patience_counter += 1
            print(f"â³ No improvement for {patience_counter}/{cfg.patience} epochs")
            
            if patience_counter >= cfg.patience:
                print(f"ğŸ›‘ Early stopping triggered")
                break
        
        # ===== å®šæœŸä¿å­˜ =====
        if (epoch + 1) % cfg.save_interval == 0:
            save_checkpoint(model, optimizer, epoch, avg_epoch_loss, cfg)
        
        # ===== ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— =====
        if cfg.device.type == 'cuda':
            torch.cuda.empty_cache()
    
    # ===== å­¦ç¿’å®Œäº† =====
    print(f"\nâœ… Phase 3 å­¦ç¿’å®Œäº†!")
    print(f"ğŸ† Best Loss: {best_loss:.4f}")
    
    return train_losses, learning_rates, best_loss

# ===== EMAå¯¾å¿œä¿å­˜é–¢æ•° =====
def save_best_model_with_ema(model, optimizer, ema, epoch, loss, cfg):
    """EMAå¯¾å¿œã®æ”¹è‰¯ç‰ˆãƒ¢ãƒ‡ãƒ«ä¿å­˜"""
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
        'ema_state_dict': ema.shadow if ema else None,
        'config': {
            'batch_size': cfg.batch_size,
            'learning_rate': cfg.learning_rate,
            'ema_decay': cfg.ema_decay if cfg.use_ema else None,
            'validation_split': cfg.validation_split
        },
        'training_stats': {
            'gpu_memory_peak': torch.cuda.max_memory_allocated(0) / 1024**3 if torch.cuda.is_available() else 0,
            'parameters': sum(p.numel() for p in model.parameters())
        },
        'version_info': VersionTracker.get_all_trackers()
    }
    
    save_path = os.path.join(cfg.save_dir, f'phase3_ema_val_loss_{loss:.4f}.pth')
    torch.save(checkpoint, save_path)
    print(f"ğŸ’¾ Phase 3 EMA model saved: {save_path}")

def save_optimized_model(model, optimizer, epoch, loss, cfg):
    """æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ä¿å­˜"""
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
        'config': {
            'optimizer_type': cfg.optimizer_type,
            'learning_rate': cfg.learning_rate,
            'batch_size': cfg.batch_size,
            'scheduler_type': cfg.scheduler_type if cfg.use_scheduler else None
        },
        'training_stats': {
            'gpu_memory_peak': torch.cuda.max_memory_allocated(0) / 1024**3 if torch.cuda.is_available() else 0,
            'parameters': sum(p.numel() for p in model.parameters())
        },
        'version_info': VersionTracker.get_all_trackers()
    }
    
    save_path = os.path.join(cfg.save_dir, f'phase3_best_model_loss_{loss:.4f}.pth')
    torch.save(checkpoint, save_path)
    print(f"ğŸ’¾ Phase 3 best model saved: {save_path}")

def save_checkpoint(model, optimizer, epoch, loss, cfg):
    """å®šæœŸãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜"""
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
        'config_info': {
            'batch_size': cfg.batch_size,
            'learning_rate': cfg.learning_rate,
            'optimizer_type': getattr(cfg, 'optimizer_type', 'Adam')
        },
        'version_info': VersionTracker.get_all_trackers()
    }
    
    save_path = os.path.join(cfg.save_dir, f'checkpoint_epoch_{epoch+1}.pth')
    torch.save(checkpoint, save_path)
    print(f"ğŸ’¾ Checkpoint saved: {save_path}")

def plot_training_progress(losses, lrs, val_losses=None, save_path="training_progress.png"):
    """å­¦ç¿’é€²æ—ã‚’å¯è¦–åŒ–ï¼ˆæ¤œè¨¼losså¯¾å¿œï¼‰"""
    try:
        import matplotlib.pyplot as plt
        
        if val_losses:
            fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 4))
        else:
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
        
        # Training Loss plot
        ax1.plot(losses, 'b-', linewidth=2, label='Train Loss')
        if val_losses:
            # Noneå€¤ã‚’é™¤å¤–ã—ã¦plot
            valid_val_losses = [loss for loss in val_losses if loss != float('inf')]
            valid_epochs = [i for i, loss in enumerate(val_losses) if loss != float('inf')]
            if valid_val_losses:
                ax1.plot(valid_epochs, valid_val_losses, 'r-', linewidth=2, label='Val Loss')
        
        ax1.set_title('Training Loss')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.grid(True, alpha=0.3)
        ax1.set_yscale('log')  # Log scale for better visualization
        if val_losses:
            ax1.legend()
        
        # Learning Rate plot
        ax2.plot(lrs, 'r-', linewidth=2)
        ax2.set_title('Learning Rate')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Learning Rate')
        ax2.grid(True, alpha=0.3)
        ax2.set_yscale('log')
        
        # Validation Loss separate plot (if available)
        if val_losses:
            valid_val_losses = [loss for loss in val_losses if loss != float('inf')]
            valid_epochs = [i for i, loss in enumerate(val_losses) if loss != float('inf')]
            if valid_val_losses:
                ax3.plot(valid_epochs, valid_val_losses, 'g-', linewidth=2)
                ax3.set_title('Validation Loss')
                ax3.set_xlabel('Epoch')
                ax3.set_ylabel('Val Loss')
                ax3.grid(True, alpha=0.3)
                ax3.set_yscale('log')
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.show()
        
        print(f"ğŸ“Š Training progress saved to {save_path}")
    except ImportError:
        print("âš ï¸ matplotlib not available - skipping visualization")

# ===== ãƒ‡ãƒãƒƒã‚°ãƒ»GPUé–¢é€£æ©Ÿèƒ½ =====
def comprehensive_gpu_check():
    """åŒ…æ‹¬çš„ãªGPUç’°å¢ƒãƒã‚§ãƒƒã‚¯"""
    print("\nğŸ” GPUç’°å¢ƒè©³ç´°ãƒã‚§ãƒƒã‚¯")
    print("="*60)
    
    # 1. CUDAå¯ç”¨æ€§
    print(f"1. CUDA available: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"   CUDA version: {torch.version.cuda}")
        print(f"   GPU count: {torch.cuda.device_count()}")
        print(f"   Current device: {torch.cuda.current_device()}")
        print(f"   Device name: {torch.cuda.get_device_name(0)}")
        
        # ãƒ¡ãƒ¢ãƒªæƒ…å ±
        memory_allocated = torch.cuda.memory_allocated(0) / 1024**3
        memory_reserved = torch.cuda.memory_reserved(0) / 1024**3
        print(f"   GPU memory - Allocated: {memory_allocated:.2f}GB, Reserved: {memory_reserved:.2f}GB")
    else:
        print("   âŒ CUDA not available!")
        return False
    
    # 2. PyTorchãƒãƒ¼ã‚¸ãƒ§ãƒ³
    print(f"2. PyTorch version: {torch.__version__}")
    
    # 3. ãƒ‡ãƒã‚¤ã‚¹è¨­å®šç¢ºèª
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"3. Selected device: {device}")
    
    return True

def test_version_tracking():
    """ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã®å‹•ä½œãƒ†ã‚¹ãƒˆ"""
    print("ğŸ§ª ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤º
    debug_version_status()
    
    # ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’ãƒã‚§ãƒƒã‚¯
    count = get_version_count()
    print(f"\nğŸ“Š ç¾åœ¨ã®ç™»éŒ²çŠ¶æ³:")
    print(f"   ç™»éŒ²æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {count}")
    print(f"   æœŸå¾…å€¤: 4ãƒ•ã‚¡ã‚¤ãƒ« (dataset, model, loss, train)")
    
    if count >= 4:
        print("âœ… ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ æ­£å¸¸å‹•ä½œ")
    else:
        print(f"âš ï¸ æœŸå¾…ã•ã‚Œã‚‹4ãƒ•ã‚¡ã‚¤ãƒ«ã®ã†ã¡{count}ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ç™»éŒ²æ¸ˆã¿")
        print("   ä»–ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒã¾ã èª­ã¿è¾¼ã¾ã‚Œã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")

# ===== ãƒ¡ã‚¤ãƒ³é–¢æ•° =====
def main():
    print("ğŸš€ Starting Modular YOLO Training - Phase 3 Complete")
    
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ã‚’è¡¨ç¤º
    print("\n" + "="*80)
    print("ğŸ“‹ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª")
    print("="*80)
    VersionTracker.print_all_versions()  # è©³ç´°ç‰ˆ

    # â˜…â˜…â˜… è¨­å®šã¨GPUç¢ºèª â˜…â˜…â˜…
    cfg = Config()
    os.makedirs(cfg.save_dir, exist_ok=True)
    
    # GPUç’°å¢ƒè©³ç´°ãƒã‚§ãƒƒã‚¯
    if not comprehensive_gpu_check():
        print("âŒ GPUä½¿ç”¨ä¸å¯ - CPUå­¦ç¿’ã«åˆ‡ã‚Šæ›¿ãˆã¾ã™")
        cfg.device = torch.device('cpu')
        cfg.batch_size = max(cfg.batch_size // 4, 1)  # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å‰Šæ¸›
        print(f"   CPUç”¨ã«ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’ {cfg.batch_size} ã«èª¿æ•´")
    
    print(f"\nğŸ“‹ å­¦ç¿’è¨­å®š:")
    print(f"   Device: {cfg.device}")
    print(f"   Batch size: {cfg.batch_size}")
    print(f"   Image size: {cfg.img_size}")
    print(f"   Classes: {cfg.num_classes}")
    print(f"   EMA: {cfg.use_ema}")
    print(f"   Validation Split: {cfg.validation_split}")
    
    # â˜…â˜…â˜… Phase 3: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ & æ¤œè¨¼åˆ†å‰² â˜…â˜…â˜…
    print("\nğŸ“Š Loading dataset with validation split...")
    train_dataloader, val_dataloader = setup_dataloaders(cfg)
    
    print(f"   Total train batches: {len(train_dataloader)}")
    if val_dataloader:
        print(f"   Total validation batches: {len(val_dataloader)}")
    
    # â˜…â˜…â˜… ãƒ¢ãƒ‡ãƒ«ï¼ˆæ˜ç¤ºçš„ã«GPUã«ç§»å‹•ï¼‰ â˜…â˜…â˜…
    print("\nğŸ¤– Creating and setting up model...")
    model = SimpleYOLO(cfg.num_classes).to(cfg.device)
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’æ˜ç¤ºçš„ã«GPUã«ç§»å‹•
    print(f"   Moving model to {cfg.device}...")
    model = model.to(cfg.device)
    
    # float32ã‚’å¼·åˆ¶ï¼ˆæ··åˆç²¾åº¦ã‚’é¿ã‘ã‚‹ï¼‰
    if cfg.device.type == 'cuda':
        model = model.float()
    
    print(f"   Model parameters: {sum(p.numel() for p in model.parameters()):,}")
    
    # ãƒ¢ãƒ‡ãƒ«ãŒGPUã«æ­£ã—ãé…ç½®ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
    model_device = next(model.parameters()).device
    print(f"   Model device confirmed: {model_device}")
    
    # â˜…â˜…â˜… æå¤±é–¢æ•° â˜…â˜…â˜…
    criterion = YOLOLoss(cfg.num_classes)
    
    # â˜…â˜…â˜… Phase 3 EMA&æ¤œè¨¼åˆ†å‰²å­¦ç¿’ ã¾ãŸã¯ å¾“æ¥å­¦ç¿’ â˜…â˜…â˜…
    if cfg.use_phase3_optimization:
        print("\nğŸš€ Phase 3 EMA&æ¤œè¨¼åˆ†å‰²å­¦ç¿’ã‚’é–‹å§‹")
        
        # Phase 3 å®Œå…¨ç‰ˆå­¦ç¿’å®Ÿè¡Œ
        train_losses, val_losses, lrs, best_loss = optimized_training_loop_with_ema_val(
            model, train_dataloader, val_dataloader, criterion, cfg
        )
        
        # çµæœå¯è¦–åŒ–
        try:
            plot_training_progress(train_losses, lrs, val_losses)
        except Exception as e:
            print(f"âš ï¸ å¯è¦–åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        
        print(f"\nğŸ¯ Phase 3 EMA&æ¤œè¨¼åˆ†å‰²å®Œäº†! Best Loss: {best_loss:.4f}")
        if best_loss < 1.0:
            print("ğŸ‰ Phase 3 ç›®æ¨™é”æˆ! (Loss < 1.0)")
        if best_loss < 0.5:
            print("ğŸ† æœ€çµ‚ç›®æ¨™é”æˆ! (Loss < 0.5)")
            
    else:
        print("\nğŸ“š å¾“æ¥å­¦ç¿’ã‚’é–‹å§‹")
        
        # å¾“æ¥å­¦ç¿’ãƒ«ãƒ¼ãƒ—ï¼ˆå˜ä¸€ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ï¼‰
        train_losses, lrs, best_loss = optimized_training_loop(model, train_dataloader, criterion, cfg)
        
        # çµæœå¯è¦–åŒ–
        try:
            plot_training_progress(train_losses, lrs)
        except Exception as e:
            print(f"âš ï¸ å¯è¦–åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        
        print(f"\nâœ… å¾“æ¥å­¦ç¿’å®Œäº†! Best Loss: {best_loss:.4f}")
    
    print("\nâœ… Training completed!")
    
    # æœ€çµ‚çµ±è¨ˆ
    if cfg.device.type == 'cuda':
        final_memory = torch.cuda.memory_allocated(0) / 1024**3
        max_memory = torch.cuda.max_memory_allocated(0) / 1024**3
        print(f"ğŸ“Š æœ€çµ‚GPUçµ±è¨ˆ:")
        print(f"   ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {final_memory:.2f}GB")
        print(f"   æœ€å¤§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {max_memory:.2f}GB")
    
    # å­¦ç¿’çµæœã‚µãƒãƒªãƒ¼
    print(f"\nğŸ¯ å­¦ç¿’çµæœã‚µãƒãƒªãƒ¼:")
    print(f"   æœ€çµ‚Loss: {best_loss:.4f}")
    print(f"   EMAä½¿ç”¨: {'Yes' if cfg.use_ema else 'No'}")
    print(f"   æ¤œè¨¼åˆ†å‰²: {'Yes' if cfg.validation_split > 0 else 'No'}")
    print(f"   ä¿å­˜å…ˆ: {cfg.save_dir}")

if __name__ == "__main__":
    main()