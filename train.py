# train.py
import torch
import torch.optim as optim
from torch.utils.data import DataLoader
import time
import os

from config import Config
from dataset import FLIRDataset, collate_fn
from model import SimpleYOLO
from loss import YOLOLoss


# â˜…â˜…â˜… å…±æœ‰VersionTrackerã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ â˜…â˜…â˜…
from version_tracker import (
    create_version_tracker, 
    VersionTracker, 
    show_all_project_versions,
    debug_version_status,
    get_version_count
)

# ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
training_version = create_version_tracker("Training System v1.1", "train.py")
training_version.add_modification("å­¦ç¿’ãƒ«ãƒ¼ãƒ—å®Ÿè£…")
training_version.add_modification("ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒ¼ã‚¸ãƒ§ãƒ³è¡¨ç¤ºè¿½åŠ ")
training_version.add_modification("gpuæœªä½¿ç”¨åŸå› ã®è¿½ç©¶")

def test_version_tracking():
    """ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã®å‹•ä½œãƒ†ã‚¹ãƒˆ"""
    print("ğŸ§ª ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤º
    debug_version_status()
    
    # ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’ãƒã‚§ãƒƒã‚¯
    count = get_version_count()
    print(f"\nğŸ“Š ç¾åœ¨ã®ç™»éŒ²çŠ¶æ³:")
    print(f"   ç™»éŒ²æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {count}")
    print(f"   æœŸå¾…å€¤: 4ãƒ•ã‚¡ã‚¤ãƒ« (dataset, model, loss, train)")
    
    if count >= 4:
        print("âœ… ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ æ­£å¸¸å‹•ä½œ")
    else:
        print(f"âš ï¸ æœŸå¾…ã•ã‚Œã‚‹4ãƒ•ã‚¡ã‚¤ãƒ«ã®ã†ã¡{count}ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ç™»éŒ²æ¸ˆã¿")
        print("   ä»–ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒã¾ã èª­ã¿è¾¼ã¾ã‚Œã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")

def train_one_epoch(model, dataloader, criterion, optimizer, device):
    model.train()
    total_loss = 0
    
    for batch_idx, (images, targets) in enumerate(dataloader):
        images = images.to(device)
        
        # Forward
        predictions, grid_size = model(images)
        loss = criterion(predictions, targets, grid_size)
        
        # Backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        
        # é€²æ—è¡¨ç¤º
        if batch_idx % Config.print_interval == 0:
            print(f"Batch [{batch_idx}/{len(dataloader)}] Loss: {loss.item():.4f}")
    
    return total_loss / len(dataloader)


import torch
import time
import psutil
import os

def comprehensive_gpu_check():
    """åŒ…æ‹¬çš„ãªGPUç’°å¢ƒãƒã‚§ãƒƒã‚¯"""
    print("\nğŸ” GPUç’°å¢ƒè©³ç´°ãƒã‚§ãƒƒã‚¯")
    print("="*60)
    
    # 1. CUDAå¯ç”¨æ€§
    print(f"1. CUDA available: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"   CUDA version: {torch.version.cuda}")
        print(f"   GPU count: {torch.cuda.device_count()}")
        print(f"   Current device: {torch.cuda.current_device()}")
        print(f"   Device name: {torch.cuda.get_device_name(0)}")
        
        # ãƒ¡ãƒ¢ãƒªæƒ…å ±
        memory_allocated = torch.cuda.memory_allocated(0) / 1024**3
        memory_reserved = torch.cuda.memory_reserved(0) / 1024**3
        print(f"   GPU memory - Allocated: {memory_allocated:.2f}GB, Reserved: {memory_reserved:.2f}GB")
    else:
        print("   âŒ CUDA not available!")
        return False
    
    # 2. PyTorchãƒãƒ¼ã‚¸ãƒ§ãƒ³
    print(f"2. PyTorch version: {torch.__version__}")
    
    # 3. ãƒ‡ãƒã‚¤ã‚¹è¨­å®šç¢ºèª
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"3. Selected device: {device}")
    
    return True

def check_model_device(model):
    """ãƒ¢ãƒ‡ãƒ«ãŒGPUã«é…ç½®ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª"""
    print("\nğŸ” ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒã‚¤ã‚¹ç¢ºèª")
    print("-"*40)
    
    model_device = next(model.parameters()).device
    print(f"Model device: {model_device}")
    
    # å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ãƒ‡ãƒã‚¤ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯
    layers_on_gpu = 0
    total_layers = 0
    
    for name, param in model.named_parameters():
        total_layers += 1
        if param.device.type == 'cuda':
            layers_on_gpu += 1
        elif total_layers <= 5:  # æœ€åˆã®5ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ã¿è¡¨ç¤º
            print(f"   âš ï¸ {name}: {param.device}")
    
    print(f"GPUä¸Šã®ãƒ¬ã‚¤ãƒ¤ãƒ¼: {layers_on_gpu}/{total_layers}")
    
    if layers_on_gpu == total_layers:
        print("âœ… ãƒ¢ãƒ‡ãƒ«å…¨ä½“ãŒGPUä¸Šã«ã‚ã‚Šã¾ã™")
        return True
    else:
        print(f"âŒ {total_layers - layers_on_gpu} ãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒCPUä¸Šã«ã‚ã‚Šã¾ã™")
        return False

def check_data_device(images, targets):
    """ãƒ‡ãƒ¼ã‚¿ãŒGPUã«é…ç½®ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª"""
    print(f"\nğŸ” ãƒ‡ãƒ¼ã‚¿ãƒ‡ãƒã‚¤ã‚¹ç¢ºèª")
    print("-"*40)
    
    print(f"Images device: {images.device}")
    print(f"Images shape: {images.shape}")
    print(f"Images dtype: {images.dtype}")
    
    if isinstance(targets, list):
        if len(targets) > 0 and torch.is_tensor(targets[0]):
            print(f"Targets[0] device: {targets[0].device}")
            print(f"Targets[0] shape: {targets[0].shape}")
    else:
        print(f"Targets device: {targets.device}")
    
    return images.device.type == 'cuda'

def gpu_utilization_test():
    """GPUä½¿ç”¨ç‡ãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ” GPUä½¿ç”¨ç‡ãƒ†ã‚¹ãƒˆ")
    print("-"*40)
    
    if not torch.cuda.is_available():
        print("âŒ CUDA not available")
        return
    
    device = torch.device('cuda')
    
    # å¤§ããªãƒ†ãƒ³ã‚½ãƒ«æ“ä½œã§GPUä½¿ç”¨ç‡ã‚’ãƒ†ã‚¹ãƒˆ
    print("å¤§ããªãƒ†ãƒ³ã‚½ãƒ«æ“ä½œã‚’å®Ÿè¡Œä¸­...")
    
    start_time = time.time()
    
    # GPUä¸Šã§é‡ã„è¨ˆç®—
    a = torch.randn(5000, 5000, device=device)
    b = torch.randn(5000, 5000, device=device)
    
    for i in range(10):
        c = torch.mm(a, b)
        if i == 0:
            print(f"First operation result device: {c.device}")
    
    end_time = time.time()
    print(f"GPUè¨ˆç®—æ™‚é–“: {end_time - start_time:.2f}ç§’")
    
    # åŒã˜è¨ˆç®—ã‚’CPUã§å®Ÿè¡Œã—ã¦æ¯”è¼ƒ
    print("åŒã˜è¨ˆç®—ã‚’CPUã§å®Ÿè¡Œä¸­...")
    start_time = time.time()
    
    a_cpu = torch.randn(5000, 5000)
    b_cpu = torch.randn(5000, 5000)
    
    for i in range(10):
        c_cpu = torch.mm(a_cpu, b_cpu)
    
    end_time = time.time()
    print(f"CPUè¨ˆç®—æ™‚é–“: {end_time - start_time:.2f}ç§’")
    
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
    memory_allocated = torch.cuda.memory_allocated(0) / 1024**3
    memory_reserved = torch.cuda.memory_reserved(0) / 1024**3
    print(f"ãƒ†ã‚¹ãƒˆå¾Œã®GPUãƒ¡ãƒ¢ãƒª - Allocated: {memory_allocated:.2f}GB, Reserved: {memory_reserved:.2f}GB")

def check_dataloader_efficiency(dataloader, device, num_batches=3):
    """DataLoaderã®åŠ¹ç‡æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
    print(f"\nğŸ” DataLoaderåŠ¹ç‡æ€§ãƒã‚§ãƒƒã‚¯")
    print("-"*40)
    
    total_load_time = 0
    total_gpu_transfer_time = 0
    
    for batch_idx, (images, targets) in enumerate(dataloader):
        if batch_idx >= num_batches:
            break
        
        load_start = time.time()
        
        # GPUè»¢é€æ™‚é–“æ¸¬å®š
        gpu_start = time.time()
        images = images.to(device, non_blocking=True)
        torch.cuda.synchronize()  # GPUè»¢é€å®Œäº†ã‚’å¾…ã¤
        gpu_end = time.time()
        
        load_end = time.time()
        
        batch_load_time = load_end - load_start
        gpu_transfer_time = gpu_end - gpu_start
        
        total_load_time += batch_load_time
        total_gpu_transfer_time += gpu_transfer_time
        
        print(f"Batch {batch_idx}: Load {batch_load_time:.3f}s, GPU transfer {gpu_transfer_time:.3f}s")
        print(f"   Images shape: {images.shape}, device: {images.device}")
    
    avg_load_time = total_load_time / num_batches
    avg_gpu_time = total_gpu_transfer_time / num_batches
    
    print(f"\nå¹³å‡èª­ã¿è¾¼ã¿æ™‚é–“: {avg_load_time:.3f}s")
    print(f"å¹³å‡GPUè»¢é€æ™‚é–“: {avg_gpu_time:.3f}s")
    
    if avg_gpu_time > avg_load_time * 0.5:
        print("âš ï¸ GPUè»¢é€ãŒé…ã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
        print("   å¯¾ç­–: pin_memory=True, non_blocking=True ã‚’è©¦ã—ã¦ãã ã•ã„")

def monitor_training_step(model, images, targets, criterion, optimizer, device):
    """1ã‚¹ãƒ†ãƒƒãƒ—ã®å­¦ç¿’ã‚’ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°"""
    print(f"\nğŸ” å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°")
    print("-"*40)
    
    # é–‹å§‹æ™‚ã®GPUãƒ¡ãƒ¢ãƒª
    start_memory = torch.cuda.memory_allocated(0) / 1024**3
    
    # Forward pass
    forward_start = time.time()
    predictions, grid_size = model(images)
    torch.cuda.synchronize()
    forward_end = time.time()
    
    forward_memory = torch.cuda.memory_allocated(0) / 1024**3
    
    # Lossè¨ˆç®—
    loss_start = time.time()
    loss = criterion(predictions, targets, grid_size)
    torch.cuda.synchronize()
    loss_end = time.time()
    
    loss_memory = torch.cuda.memory_allocated(0) / 1024**3
    
    # Backward pass
    backward_start = time.time()
    optimizer.zero_grad()
    loss.backward()
    torch.cuda.synchronize()
    backward_end = time.time()
    
    backward_memory = torch.cuda.memory_allocated(0) / 1024**3
    
    # Optimizer step
    step_start = time.time()
    optimizer.step()
    torch.cuda.synchronize()
    step_end = time.time()
    
    final_memory = torch.cuda.memory_allocated(0) / 1024**3
    
    print(f"Forward pass: {forward_end - forward_start:.3f}s (+{forward_memory - start_memory:.2f}GB)")
    print(f"Loss calculation: {loss_end - loss_start:.3f}s (+{loss_memory - forward_memory:.2f}GB)")
    print(f"Backward pass: {backward_end - backward_start:.3f}s (+{backward_memory - loss_memory:.2f}GB)")
    print(f"Optimizer step: {step_end - step_start:.3f}s (+{final_memory - backward_memory:.2f}GB)")
    print(f"Total memory used: {final_memory:.2f}GB")
    
    # äºˆæ¸¬ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ãƒ‡ãƒã‚¤ã‚¹ç¢ºèª
    print(f"Predictions device: {predictions.device}")
    print(f"Loss device: {loss.device}")

# train.py ã®mainé–¢æ•°ã«è¿½åŠ ã™ã‚‹é–¢æ•°
def debug_training_setup(model, dataloader, criterion, optimizer, device):
    """å­¦ç¿’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã®å®Œå…¨ãƒ‡ãƒãƒƒã‚°"""
    print("\nğŸš€ å­¦ç¿’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œå…¨ãƒ‡ãƒãƒƒã‚°")
    print("="*60)
    
    # 1. ç’°å¢ƒãƒã‚§ãƒƒã‚¯
    if not comprehensive_gpu_check():
        return False
    
    # 2. ãƒ¢ãƒ‡ãƒ«ãƒã‚§ãƒƒã‚¯
    if not check_model_device(model):
        print("ğŸ”§ ãƒ¢ãƒ‡ãƒ«ã‚’GPUã«ç§»å‹•ã—ã¦ã„ã¾ã™...")
        model = model.to(device)
        check_model_device(model)
    
    # 3. GPUä½¿ç”¨ç‡ãƒ†ã‚¹ãƒˆ
    gpu_utilization_test()
    
    # 4. DataLoaderãƒã‚§ãƒƒã‚¯
    check_dataloader_efficiency(dataloader, device)
    
    # 5. å®Ÿéš›ã®å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—ã‚’ãƒ†ã‚¹ãƒˆ
    print("\nğŸ” å®Ÿéš›ã®å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—ã‚’ãƒ†ã‚¹ãƒˆä¸­...")
    model.train()
    
    for batch_idx, (images, targets) in enumerate(dataloader):
        # ãƒ‡ãƒ¼ã‚¿ã‚’GPUã«ç§»å‹•
        images = images.to(device, non_blocking=True)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ‡ãƒã‚¤ã‚¹ç¢ºèª
        if not check_data_device(images, targets):
            print("ğŸ”§ ãƒ‡ãƒ¼ã‚¿ã‚’GPUã«ç§»å‹•ã—ã¦ã„ã¾ã™...")
            images = images.to(device)
        
        # 1ã‚¹ãƒ†ãƒƒãƒ—ã‚’ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°
        monitor_training_step(model, images, targets, criterion, optimizer, device)
        
        print("âœ… ãƒ‡ãƒãƒƒã‚°å®Œäº† - å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™")
        break
    
    return True


def main():
    print("ğŸš€ Starting Modular YOLO Training")
    
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ã‚’è¡¨ç¤º
    print("\n" + "="*80)
    print("ğŸ“‹ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª")
    print("="*80)
    VersionTracker.print_all_versions()  # è©³ç´°ç‰ˆ

    # â˜…â˜…â˜… è¨­å®šã¨GPUç¢ºèª â˜…â˜…â˜…
    cfg = Config()
    os.makedirs(cfg.save_dir, exist_ok=True)
    
    # GPUç’°å¢ƒè©³ç´°ãƒã‚§ãƒƒã‚¯
    if not comprehensive_gpu_check():
        print("âŒ GPUä½¿ç”¨ä¸å¯ - CPUå­¦ç¿’ã«åˆ‡ã‚Šæ›¿ãˆã¾ã™")
        cfg.device = torch.device('cpu')
        cfg.batch_size = max(cfg.batch_size // 4, 1)  # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å‰Šæ¸›
        print(f"   CPUç”¨ã«ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’ {cfg.batch_size} ã«èª¿æ•´")
    
    print(f"\nğŸ“‹ å­¦ç¿’è¨­å®š:")
    print(f"   Device: {cfg.device}")
    print(f"   Batch size: {cfg.batch_size}")
    print(f"   Image size: {cfg.img_size}")
    print(f"   Classes: {cfg.num_classes}")
    
    # â˜…â˜…â˜… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆGPUæœ€é©åŒ–ï¼‰ â˜…â˜…â˜…
    print("\nğŸ“Š Loading dataset...")
    dataset = FLIRDataset(cfg.train_img_dir, cfg.train_label_dir, cfg.img_size)
    
    # DataLoaderè¨­å®šã‚’GPUæœ€é©åŒ–
    num_workers = 2 if cfg.device.type == 'cuda' else 0
    pin_memory = cfg.device.type == 'cuda'
    
    dataloader = DataLoader(
        dataset, 
        batch_size=cfg.batch_size, 
        shuffle=True, 
        collate_fn=collate_fn, 
        num_workers=num_workers,
        pin_memory=pin_memory,
        persistent_workers=num_workers > 0
    )
    
    print(f"   Dataset: {len(dataset)} images")
    print(f"   Batches: {len(dataloader)}")
    print(f"   DataLoader: workers={num_workers}, pin_memory={pin_memory}")
    
    # â˜…â˜…â˜… ãƒ¢ãƒ‡ãƒ«ï¼ˆæ˜ç¤ºçš„ã«GPUã«ç§»å‹•ï¼‰ â˜…â˜…â˜…
    print("\nğŸ¤– Creating and setting up model...")
    model = SimpleYOLO(cfg.num_classes)
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’æ˜ç¤ºçš„ã«GPUã«ç§»å‹•
    print(f"   Moving model to {cfg.device}...")
    model = model.to(cfg.device)
    
    # float32ã‚’å¼·åˆ¶ï¼ˆæ··åˆç²¾åº¦ã‚’é¿ã‘ã‚‹ï¼‰
    if cfg.device.type == 'cuda':
        model = model.float()
    
    print(f"   Model parameters: {sum(p.numel() for p in model.parameters()):,}")
    
    # ãƒ¢ãƒ‡ãƒ«ãŒGPUã«æ­£ã—ãé…ç½®ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
    model_device = next(model.parameters()).device
    print(f"   Model device confirmed: {model_device}")
    
    # â˜…â˜…â˜… æå¤±é–¢æ•°ã¨ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ â˜…â˜…â˜…
    criterion = YOLOLoss(cfg.num_classes)
    optimizer = optim.Adam(model.parameters(), lr=cfg.learning_rate)
    
    # â˜…â˜…â˜… å­¦ç¿’é–‹å§‹å‰ã®åŒ…æ‹¬çš„ãƒ‡ãƒãƒƒã‚° â˜…â˜…â˜…
    if cfg.device.type == 'cuda':
        print("\nğŸ” GPUå­¦ç¿’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚’ãƒ‡ãƒãƒƒã‚°ä¸­...")
        debug_success = debug_training_setup(model, dataloader, criterion, optimizer, cfg.device)
        
        if not debug_success:
            print("âŒ GPUè¨­å®šã«å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚CPUå­¦ç¿’ã«åˆ‡ã‚Šæ›¿ãˆã¾ã™ã€‚")
            cfg.device = torch.device('cpu')
            model = model.to(cfg.device)
    
    # â˜…â˜…â˜… æœ€é©åŒ–ã•ã‚ŒãŸå­¦ç¿’ãƒ«ãƒ¼ãƒ— â˜…â˜…â˜…
    print(f"\nğŸ¯ Starting training on {cfg.device}...")
    best_loss = float('inf')
    
    # åˆå›GPUä½¿ç”¨ç‡ç¢ºèª
    if cfg.device.type == 'cuda':
        print("\nğŸ“Š åˆå›ãƒãƒƒãƒã§GPUä½¿ç”¨ç‡ç¢ºèª...")
    
    for epoch in range(cfg.num_epochs):
        start_time = time.time()
        
        # æœ€é©åŒ–ã•ã‚ŒãŸå­¦ç¿’ã‚¨ãƒãƒƒã‚¯
        avg_loss = train_one_epoch_optimized(
            model, dataloader, criterion, optimizer, cfg.device, epoch
        )
        
        epoch_time = time.time() - start_time
        print(f"\nEpoch [{epoch+1}/{cfg.num_epochs}] "
              f"Loss: {avg_loss:.4f} Time: {epoch_time:.1f}s")
        
        # GPUä½¿ç”¨çŠ¶æ³è¡¨ç¤ºï¼ˆæœ€åˆã®3ã‚¨ãƒãƒƒã‚¯ã®ã¿ï¼‰
        if cfg.device.type == 'cuda' and epoch < 3:
            memory_allocated = torch.cuda.memory_allocated(0) / 1024**3
            memory_reserved = torch.cuda.memory_reserved(0) / 1024**3
            print(f"   GPU Memory: {memory_allocated:.2f}GB allocated, {memory_reserved:.2f}GB reserved")
        
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        if avg_loss < best_loss:
            best_loss = avg_loss
            
            # ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ä»˜ãã§ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
            checkpoint = {
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'loss': best_loss,
                'device': str(cfg.device),
                'version_info': VersionTracker.get_all_trackers()
            }
            
            torch.save(checkpoint, os.path.join(cfg.save_dir, 'best_model.pth'))
            print(f"ğŸ’¾ Best model saved (loss: {best_loss:.4f})")
    
    print("\nâœ… Training completed!")
    
    # æœ€çµ‚GPUçµ±è¨ˆ
    if cfg.device.type == 'cuda':
        final_memory = torch.cuda.memory_allocated(0) / 1024**3
        max_memory = torch.cuda.max_memory_allocated(0) / 1024**3
        print(f"ğŸ“Š æœ€çµ‚GPUçµ±è¨ˆ:")
        print(f"   ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {final_memory:.2f}GB")
        print(f"   æœ€å¤§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {max_memory:.2f}GB")

def train_one_epoch_optimized(model, dataloader, criterion, optimizer, device, epoch):
    """GPUæœ€é©åŒ–ã•ã‚ŒãŸå­¦ç¿’ã‚¨ãƒãƒƒã‚¯"""
    model.train()
    total_loss = 0
    batch_count = 0
    
    # ã‚¨ãƒãƒƒã‚¯é–‹å§‹æ™‚ã®è¨­å®š
    if device.type == 'cuda':
        torch.cuda.empty_cache()  # GPU ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢
    
    for batch_idx, (images, targets) in enumerate(dataloader):
        batch_start_time = time.time()
        
        # â˜…â˜…â˜… ãƒ‡ãƒ¼ã‚¿ã‚’æ˜ç¤ºçš„ã«GPUã«ç§»å‹• â˜…â˜…â˜…
        images = images.to(device, non_blocking=True)
        
        # targetsãŒãƒªã‚¹ãƒˆã®å ´åˆã®å‡¦ç†
        if isinstance(targets, list):
            # ãƒªã‚¹ãƒˆã®å„è¦ç´ ã‚’GPUã«ç§»å‹•ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
            pass  # YOLO lossã§ã¯é€šå¸¸CPUã®ã¾ã¾å‡¦ç†
        else:
            targets = targets.to(device, non_blocking=True)
        
        # GPUåŒæœŸï¼ˆæœ€åˆã®ãƒãƒƒãƒã®ã¿ï¼‰
        if batch_idx == 0 and device.type == 'cuda':
            torch.cuda.synchronize()
            print(f"   First batch data moved to GPU: images {images.device}, shape {images.shape}")
        
        # Forward pass
        forward_start = time.time()
        predictions, grid_size = model(images)
        
        # æœ€åˆã®ãƒãƒƒãƒã§äºˆæ¸¬å½¢çŠ¶ç¢ºèª
        if batch_idx == 0:
            print(f"   First batch predictions: shape {predictions.shape}, device {predictions.device}")
        
        # Loss calculation
        loss = criterion(predictions, targets, grid_size)
        
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        
        # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼ˆå®‰å®šæ€§å‘ä¸Šï¼‰
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        optimizer.step()
        
        total_loss += loss.item()
        batch_count += 1
        
        # ãƒãƒƒãƒå‡¦ç†æ™‚é–“ï¼ˆæœ€åˆã®æ•°ãƒãƒƒãƒã®ã¿ï¼‰
        if batch_idx < 5:
            batch_time = time.time() - batch_start_time
            print(f"   Batch {batch_idx}: {batch_time:.3f}s, Loss: {loss.item():.4f}")
        
        # é€²æ—è¡¨ç¤º
        if batch_idx % Config.print_interval == 0:
            current_lr = optimizer.param_groups[0]['lr']
            print(f"Batch [{batch_idx}/{len(dataloader)}] Loss: {loss.item():.4f} LR: {current_lr:.6f}")
        
        # ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ãƒã‚§ãƒƒã‚¯ï¼ˆGPUä½¿ç”¨æ™‚ã®ã¿ï¼‰
        if device.type == 'cuda' and batch_idx % 100 == 0:
            current_memory = torch.cuda.memory_allocated(0) / 1024**3
            if current_memory > 10.0:  # 10GBä»¥ä¸Šã®å ´åˆ
                torch.cuda.empty_cache()
                print(f"   GPU memory cleanup at batch {batch_idx}: {current_memory:.2f}GB")
    
    return total_loss / batch_count if batch_count > 0 else 0.0

if __name__ == "__main__":
    main()