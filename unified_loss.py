import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from collections import defaultdict
from typing import Dict, Optional, List, Tuple


# â˜…â˜…â˜… ã‚¹ãƒ†ãƒƒãƒ—1: ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ã‚’ã“ã“ã«è¿½åŠ  â˜…â˜…â˜…
# ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿®æ­£ã™ã‚‹ãŸã³ã«ã€ã“ã®æ–‡å­—åˆ—ã‚’æ›´æ–°ã—ã¦ãã ã•ã„ã€‚
__loss_version__ = "v1.4 - lossãƒ†ã‚¹ãƒˆãƒãƒ¼ã‚¸ãƒ§ãƒ³"


# ===== IoU Loss Functions =====
def bbox_ciou(box1: torch.Tensor, box2: torch.Tensor, eps: float = 1e-7) -> torch.Tensor:
    """
    Complete IoU (CIoU) è¨ˆç®—
    
    Args:
        box1, box2: [N, 4] (cx, cy, w, h)
        
    Returns:
        CIoUå€¤ [N]
    """
    b1_x, b1_y, b1_w, b1_h = box1.unbind(-1)
    b2_x, b2_y, b2_w, b2_h = box2.unbind(-1)
    
    # å¹…ã¨é«˜ã•ã‚’æœ€å°å€¤ã§ã‚¯ãƒ©ãƒ³ãƒ—
    b1_w = torch.clamp(b1_w, min=1e-2)
    b1_h = torch.clamp(b1_h, min=1e-2)
    b2_w = torch.clamp(b2_w, min=1e-2)
    b2_h = torch.clamp(b2_h, min=1e-2)
    
    # ãƒœãƒƒã‚¯ã‚¹ã®åº§æ¨™è¨ˆç®—
    b1_x1, b1_x2 = b1_x - b1_w / 2, b1_x + b1_w / 2
    b1_y1, b1_y2 = b1_y - b1_h / 2, b1_y + b1_h / 2
    b2_x1, b2_x2 = b2_x - b2_w / 2, b2_x + b2_w / 2
    b2_y1, b2_y2 = b2_y - b2_h / 2, b2_y + b2_h / 2
    
    # äº¤å·®é ˜åŸŸ
    inter_x1 = torch.max(b1_x1, b2_x1)
    inter_y1 = torch.max(b1_y1, b2_y1)
    inter_x2 = torch.min(b1_x2, b2_x2)
    inter_y2 = torch.min(b1_y2, b2_y2)
    
    inter_area = (inter_x2 - inter_x1).clamp(0) * (inter_y2 - inter_y1).clamp(0)
    area1 = b1_w * b1_h
    area2 = b2_w * b2_h
    union = area1 + area2 - inter_area + eps
    
    # IoU
    iou = inter_area / union
    
    # ä¸­å¿ƒè·é›¢
    center_dist_sq = (b1_x - b2_x)**2 + (b1_y - b2_y)**2
    
    # å¤–æ¥çŸ©å½¢
    enclose_x1 = torch.min(b1_x1, b2_x1)
    enclose_y1 = torch.min(b1_y1, b2_y1)
    enclose_x2 = torch.max(b1_x2, b2_x2)
    enclose_y2 = torch.max(b1_y2, b2_y2)
    enclose_diag_sq = (enclose_x2 - enclose_x1)**2 + (enclose_y2 - enclose_y1)**2 + eps
    
    # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”é …
    v = (4 / (math.pi ** 2)) * (
        torch.atan(b1_w / (b1_h + eps)) - torch.atan(b2_w / (b2_h + eps))
    ) ** 2
    
    with torch.no_grad():
        alpha = v / (1 - iou + v + eps)
    
    ciou = iou - (center_dist_sq / enclose_diag_sq) - alpha * v
    return torch.clamp(ciou, min=-1.0, max=1.0)


def ciou_loss(pred_boxes: torch.Tensor, target_boxes: torch.Tensor) -> torch.Tensor:
    """CIoU Loss"""
    ciou = bbox_ciou(pred_boxes, target_boxes)
    return 1 - ciou


# ===== Focal Loss Variants =====
class AdaptiveFocalLoss(nn.Module):
    """é©å¿œçš„Focal Loss"""
    def __init__(self, alpha: float = 0.25, gamma: float = 2.0, 
                 reduction: str = 'mean', adaptive_gamma: bool = False):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
        self.adaptive_gamma = adaptive_gamma
        self.step_count = 0
        self.loss_history = []
    
    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
        probs = torch.sigmoid(inputs)
        pt = torch.where(targets == 1, probs, 1 - probs)
        
        # é©å¿œçš„ã‚¬ãƒ³ãƒèª¿æ•´
        if self.adaptive_gamma and self.training:
            self.step_count += 1
            current_loss = bce_loss.mean().item()
            self.loss_history.append(current_loss)
            
            if len(self.loss_history) > 100:
                recent_avg = sum(self.loss_history[-50:]) / 50
                older_avg = sum(self.loss_history[-100:-50]) / 50
                
                if recent_avg > older_avg * 1.05:
                    self.gamma = min(self.gamma * 1.01, 3.5)
                elif recent_avg < older_avg * 0.95:
                    self.gamma = max(self.gamma * 0.99, 1.5)
                
                self.loss_history.pop(0)
        
        # Focal Lossè¨ˆç®—
        alpha_t = torch.where(targets == 1, self.alpha, 1 - self.alpha)
        focal_loss = alpha_t * (1 - pt) ** self.gamma * bce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss


class LabelSmoothingBCE(nn.Module):
    """ãƒ©ãƒ™ãƒ«ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ä»˜ãBCE"""
    def __init__(self, smoothing: float = 0.1, pos_weight: Optional[torch.Tensor] = None):
        super().__init__()
        self.smoothing = smoothing
        self.pos_weight = pos_weight
    
    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        targets_smooth = targets * (1 - self.smoothing) + (self.smoothing / 2)
        
        if self.pos_weight is not None:
            return F.binary_cross_entropy_with_logits(
                inputs, targets_smooth, pos_weight=self.pos_weight, reduction='mean'
            )
        else:
            return F.binary_cross_entropy_with_logits(
                inputs, targets_smooth, reduction='mean'
            )


# ===== Main Loss Class =====
class EnhancedDetectionLoss(nn.Module):
    """çµ±åˆç‰ˆç‰©ä½“æ¤œå‡ºãƒ­ã‚¹ï¼ˆä¿®æ­£ç‰ˆï¼‰"""
    
    def __init__(self, 
                 num_classes: int = 15,
                 lambda_box: float = 5.0,
                 lambda_obj: float = 2.0,
                 lambda_cls: float = 1.0,
                 box_loss_type: str = 'ciou',
                 obj_loss_type: str = 'adaptive_focal',
                 cls_loss_type: str = 'label_smooth',
                 anchor_info: Optional[Dict] = None):
        super().__init__()
        
        self.num_classes = num_classes
        self.lambda_box = lambda_box
        self.lambda_obj = lambda_obj
        self.lambda_cls = lambda_cls
        
        # ã‚¢ãƒ³ã‚«ãƒ¼æƒ…å ±ï¼ˆãƒ‡ã‚³ãƒ¼ãƒ‰ç”¨ï¼‰
        self.anchor_info = anchor_info
        
        # Box Loss
        if box_loss_type == 'ciou':
            self.box_loss_fn = ciou_loss
        else:
            self.box_loss_fn = ciou_loss  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        
        # Objectness Loss
        if obj_loss_type == 'adaptive_focal':
            self.obj_loss_fn = AdaptiveFocalLoss(
            alpha=0.75,    # 0.25 â†’ 0.75ï¼ˆæ­£ã‚µãƒ³ãƒ—ãƒ«ã¸ã®é‡ã¿ã‚’å¢—åŠ ï¼‰
            gamma=2.0,     # 1.0 â†’ 2.0ï¼ˆeasy negativesã‚’ã‚ˆã‚Šå¼·ãæŠ‘åˆ¶ï¼‰
            adaptive_gamma=True
        )
        else:
            self.obj_loss_fn = AdaptiveFocalLoss(alpha=0.25, gamma=2.0)
        
        # Classification Loss
        if cls_loss_type == 'label_smooth':
            self.cls_loss_fn = LabelSmoothingBCE(smoothing=0.1)
        elif cls_loss_type == 'adaptive_focal':
            self.cls_loss_fn = AdaptiveFocalLoss(alpha=0.25, gamma=1.5)
        else:
            self.cls_loss_fn = LabelSmoothingBCE(smoothing=0.1)
        
        # ã‚¹ãƒ†ãƒƒãƒ—ã‚«ã‚¦ãƒ³ã‚¿ã¨ãƒ­ã‚¹å±¥æ­´
        self.register_buffer('step_count', torch.tensor(0))
        self.loss_history = defaultdict(list)
        
        print(f"ğŸ¯ Enhanced Loss initialized:")
        print(f"   Box Loss: {box_loss_type}, Obj Loss: {obj_loss_type}, Cls Loss: {cls_loss_type}")
        print(f"   Loss Weights: Box={lambda_box}, Obj={lambda_obj}, Cls={lambda_cls}")
    
    def forward(self, preds: torch.Tensor, targets: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        ãƒ­ã‚¹è¨ˆç®—ï¼ˆä¿®æ­£ç‰ˆï¼‰
        
        Args:
            preds: äºˆæ¸¬å€¤ [B, N, 5+num_classes]
            targets: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¾æ›¸
            
        Returns:
            ãƒ­ã‚¹è¾æ›¸
        """
        device = preds.device
        
        # äºˆæ¸¬å€¤ã‚’åˆ†è§£
        pred_box = preds[..., :4]      # tx, ty, tw, th
        pred_obj = preds[..., 4]       # objectness logits
        pred_cls = preds[..., 5:]      # class logits
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’å–å¾—
        target_box = targets['boxes']        # tx, ty, tw, th
        target_obj = targets['objectness']   # IoU modulated
        target_cls = targets['classes']      # IoU modulated one-hot
        
        # ===== ä¿®æ­£1: ã‚ˆã‚ŠæŸ”è»Ÿãªãƒã‚¸ãƒ†ã‚£ãƒ–ãƒã‚¹ã‚¯ =====
        # å…ƒã€…ã®IoUå¤‰èª¿å€¤ã‚’ä¿æŒã—ã¤ã¤ã€ã‚ˆã‚Šå¤šãã®æ­£ã‚µãƒ³ãƒ—ãƒ«ã‚’ç¢ºä¿
        pos_mask_strict = target_obj > 0.5    # å³å¯†ãªæ­£ã‚µãƒ³ãƒ—ãƒ«
        pos_mask_loose = target_obj > 0.1     # ç·©ã„æ­£ã‚µãƒ³ãƒ—ãƒ«
        pos_mask_any = target_obj > 0.01      # éå¸¸ã«ç·©ã„æ­£ã‚µãƒ³ãƒ—ãƒ«
        
        num_pos_strict = pos_mask_strict.sum()
        num_pos_loose = pos_mask_loose.sum()
        num_pos_any = pos_mask_any.sum()
        
        # Box Lossç”¨ã®ãƒã‚¹ã‚¯ã‚’é¸æŠï¼ˆã‚ˆã‚Šå¤šãã®ã‚µãƒ³ãƒ—ãƒ«ã§å­¦ç¿’ï¼‰
        if num_pos_strict > 10:
            pos_mask_for_box = pos_mask_strict
        elif num_pos_loose > 5:
            pos_mask_for_box = pos_mask_loose
        else:
            pos_mask_for_box = pos_mask_any
        
        num_pos = pos_mask_for_box.sum()
        
        # 1. Box Lossï¼ˆãƒã‚¸ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒ«ã®ã¿ï¼‰
        loss_box = torch.tensor(0.0, device=device)
        if num_pos > 0:
            # ã‚·ãƒ³ãƒ—ãƒ«ã«L1 Lossã‚’ä½¿ç”¨ï¼ˆCIoUã®ä»£ã‚ã‚Šã«å®‰å®šæ€§é‡è¦–ï¼‰
            loss_box = F.smooth_l1_loss(
                pred_box[pos_mask_for_box], 
                target_box[pos_mask_for_box],
                reduction='mean'
            )
        
        # ===== ä¿®æ­£2: Objectness Loss ã®æ”¹å–„ =====
        # â˜…â˜…â˜…ã€å®Ÿé¨“ã€‘å‹¾é…çˆ†ç™ºã®åŸå› åˆ‡ã‚Šåˆ†ã‘ã®ãŸã‚ã€Focal Lossã‚’ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ– â˜…â˜…â˜…
        # æœ€ã‚‚å®‰å®šã—ãŸæ¨™æº–ã®BCEæå¤±ã§è¨ˆç®—ã—ã€å‹¾é…çˆ†ç™ºãŒåã¾ã‚‹ã‹ã‚’ç¢ºèªã™ã‚‹ã€‚
        obj_target_binary = (target_obj > 0.1).float()
        loss_obj = F.binary_cross_entropy_with_logits(pred_obj, obj_target_binary, reduction='mean')

        
        # ===== ä¿®æ­£3: Classification Loss ã®æ”¹å–„ =====
        loss_cls = torch.tensor(0.0, device=device)
        if num_pos > 0:
            # ã‚¯ãƒ©ã‚¹ãƒ­ã‚¹ã‚‚ã‚ˆã‚Šå¤šãã®ã‚µãƒ³ãƒ—ãƒ«ã§è¨ˆç®—
            cls_pos_mask = pos_mask_loose if num_pos_loose > 0 else pos_mask_any
            
            if cls_pos_mask.any():
                if isinstance(self.cls_loss_fn, AdaptiveFocalLoss):
                    loss_cls = self.cls_loss_fn(pred_cls[cls_pos_mask], target_cls[cls_pos_mask])
                else:
                    loss_cls = self.cls_loss_fn(pred_cls[cls_pos_mask], target_cls[cls_pos_mask])
        
        # ===== ä¿®æ­£4: å‹•çš„é‡ã¿èª¿æ•´ã®æ”¹å–„ =====
        self.step_count += 1
        current_step = self.step_count.item()
        
        # ã‚ˆã‚Šç©ã‚„ã‹ãªé‡ã¿èª¿æ•´
        if current_step < 500:
            # åˆæœŸï¼šObjectnessã‚’é‡è¦–ã€ãŸã ã—éåº¦ã§ã¯ãªã„
            w_box = self.lambda_box * 0.8
            w_obj = self.lambda_obj * 2.0  # 3.0 â†’ 2.0ã«ä¿®æ­£
            w_cls = self.lambda_cls * 0.5
        elif current_step < 2000:
            # ä¸­æœŸï¼šãƒãƒ©ãƒ³ã‚¹é‡è¦–
            w_box = self.lambda_box * 1.0
            w_obj = self.lambda_obj * 1.5  # 2.5 â†’ 1.5ã«ä¿®æ­£
            w_cls = self.lambda_cls * 0.8
        else:
            # å¾ŒæœŸï¼šå…¨ã¦ãƒãƒ©ãƒ³ã‚¹ã‚ˆã
            w_box = self.lambda_box * 1.0
            w_obj = self.lambda_obj * 1.2  # 2.0 â†’ 1.2ã«ä¿®æ­£
            w_cls = self.lambda_cls * 1.0
        
        # ===== ä¿®æ­£5: Losså€¤ã®ç•°å¸¸æ¤œå‡ºã¨è£œæ­£ =====
        # ç•°å¸¸ã«å°ã•ã„ãƒ­ã‚¹å€¤ã‚’æ¤œå‡ºã—ã¦ãƒ¯ãƒ¼ãƒ‹ãƒ³ã‚°
        if loss_obj.item() < 1e-6 and current_step > 10:
            print(f"âš ï¸ WARNING: Objectness loss is too small ({loss_obj.item():.8f}) at step {current_step}")
            # ç·Šæ€¥æ™‚ã®è£œæ­£ï¼šæœ€å°å€¤ã‚’è¨­å®š
            loss_obj = torch.maximum(loss_obj, torch.tensor(1e-5, device=device))
        
        if loss_cls.item() < 1e-6 and current_step > 10 and num_pos > 0:
            print(f"âš ï¸ WARNING: Classification loss is too small ({loss_cls.item():.8f}) at step {current_step}")
            # ç·Šæ€¥æ™‚ã®è£œæ­£ï¼šæœ€å°å€¤ã‚’è¨­å®š
            loss_cls = torch.maximum(loss_cls, torch.tensor(1e-5, device=device))
        
        # ç·åˆãƒ­ã‚¹
        total_loss = w_box * loss_box + w_obj * loss_obj + w_cls * loss_cls
        
        # ãƒ­ã‚¹å±¥æ­´æ›´æ–°
        self.loss_history['box'].append(loss_box.item())
        self.loss_history['obj'].append(loss_obj.item())
        self.loss_history['cls'].append(loss_cls.item())
        self.loss_history['total'].append(total_loss.item())
        
        # å±¥æ­´ã‚µã‚¤ã‚ºåˆ¶é™
        for key in self.loss_history:
            if len(self.loss_history[key]) > 500:
                self.loss_history[key].pop(0)
        
        return {
            'total': total_loss,
            'box': loss_box,
            'obj': loss_obj,
            'cls': loss_cls,
            'pos_samples': num_pos.item(),
            'pos_samples_breakdown': {
                'strict': num_pos_strict.item(),
                'loose': num_pos_loose.item(),
                'any': num_pos_any.item()
            },
            'dynamic_weights': {
                'box': w_box,
                'obj': w_obj,
                'cls': w_cls
            }
        }
    
    def get_loss_statistics(self) -> Dict:
        """ãƒ­ã‚¹çµ±è¨ˆã‚’å–å¾—"""
        stats = {}
        
        for key, values in self.loss_history.items():
            if values:
                tensor_values = torch.tensor(values, dtype=torch.float32)
                stats[key] = {
                    'mean': tensor_values.mean().item(),
                    'std': tensor_values.std().item() if len(values) > 1 else 0.0,
                    'min': tensor_values.min().item(),
                    'max': tensor_values.max().item(),
                    'recent_mean': tensor_values[-100:].mean().item() if len(values) >= 100 else tensor_values.mean().item()
                }
        
        return stats


def create_enhanced_loss(num_classes: int = 15, 
                        loss_strategy: str = 'balanced',
                        anchor_info: Optional[Dict] = None) -> EnhancedDetectionLoss:
    """
    ãƒ­ã‚¹é–¢æ•°ã‚’ä½œæˆï¼ˆä¿®æ­£ç‰ˆï¼‰
    
    Args:
        num_classes: ã‚¯ãƒ©ã‚¹æ•°
        loss_strategy: ãƒ­ã‚¹æˆ¦ç•¥
        anchor_info: ã‚¢ãƒ³ã‚«ãƒ¼æƒ…å ±ï¼ˆãƒ‡ã‚³ãƒ¼ãƒ‰ç”¨ï¼‰
        
    Returns:
        EnhancedDetectionLoss
    """

    print(f"--- èª­ã¿è¾¼ã¿ä¸­ã®æå¤±ãƒ•ã‚¡ã‚¤ãƒ«: unified_loss.py ({__loss_version__}) ---")

    strategies = {
        'balanced': {
            'lambda_box': 5.0,
            'lambda_obj': 2.0,  # 1.0 â†’ 2.0 ã«èª¿æ•´
            'lambda_cls': 1.0,
            'box_loss_type': 'ciou',
            'obj_loss_type': 'adaptive_focal',
            'cls_loss_type': 'label_smooth'
        },
        'box_focused': {
            'lambda_box': 7.0,
            'lambda_obj': 1.5,  # 2.5 â†’ 1.5 ã«èª¿æ•´
            'lambda_cls': 0.8,
            'box_loss_type': 'ciou',
            'obj_loss_type': 'adaptive_focal',
            'cls_loss_type': 'label_smooth'
        },
        'obj_focused': {
            'lambda_box': 5.0,
            'lambda_obj': 3.0,
            'lambda_cls': 1.0,
            'box_loss_type': 'ciou',
            'obj_loss_type': 'adaptive_focal',
            'cls_loss_type': 'adaptive_focal'
        }
    }
    
    config = strategies.get(loss_strategy, strategies['balanced'])
    
    if loss_strategy not in strategies:
        print(f"âš ï¸ Unknown strategy '{loss_strategy}', using 'balanced'")
    
    print(f"ğŸ¯ Creating Enhanced Loss with strategy: {loss_strategy}")
    
    return EnhancedDetectionLoss(
        num_classes=num_classes,
        anchor_info=anchor_info,
        **config
    )