{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMhb2n56aBgJuo5axGFhGoV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8tUzuXgIjHRq","executionInfo":{"status":"ok","timestamp":1749198828588,"user_tz":-540,"elapsed":25933,"user":{"displayName":"みは","userId":"00152872231100665214"}},"outputId":"4d8e5a93-f5de-4250-e881-5260915eeb0f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Found checkpoints: []\n"]}],"source":["# Google Driveマウント\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# 作業ディレクトリ移動\n","import os\n","os.chdir('/content/drive/MyDrive/IR_obj_detction/new/IR_obeject_Detection')\n","\n","# チェックポイント確認\n","import glob\n","checkpoints = glob.glob('saved_models/checkpoint_epoch_*.pth')\n","print(f\"Found checkpoints: {[os.path.basename(f) for f in checkpoints]}\")"]},{"cell_type":"code","source":["# ローカルに保存用ディレクトリを作成\n","!mkdir -p /content/FLIR_YOLO_local/images/train\n","!mkdir -p /content/FLIR_YOLO_local/labels/train\n","\n","# Google Driveからローカルにデータセットをコピー (時間はかかります)\n","# パスは実際のプロジェクトに合わせてください\n","!echo \"Copying training images...\"\n","!cp -r /content/drive/MyDrive/old/EfficientNet_Project/data/FLIR_YOLO/images/train/* /content/FLIR_YOLO_local/images/train/\n","!echo \"Copying training labels...\"\n","!cp -r /content/drive/MyDrive/old/EfficientNet_Project/data/FLIR_YOLO/labels/train/* /content/FLIR_YOLO_local/labels/train/\n","!echo \"Copy complete!\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"puxvQzu4jOSX","executionInfo":{"status":"ok","timestamp":1749199495689,"user_tz":-540,"elapsed":667092,"user":{"displayName":"みは","userId":"00152872231100665214"}},"outputId":"320b679a-c0ed-4bb0-e2a2-f7a339be6223"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Copying training images...\n","Copying training labels...\n","Copy complete!\n"]}]},{"cell_type":"code","source":["# テスト用コード\n","from efficientnet_model import test_model_creation\n","\n","model = test_model_creation()\n","if model:\n","    print(\"✅ モデル作成成功！\")\n","else:\n","    print(\"❌ モデル作成失敗\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q1o98TFXp9o9","executionInfo":{"status":"ok","timestamp":1749199516767,"user_tz":-540,"elapsed":21077,"user":{"displayName":"みは","userId":"00152872231100665214"}},"outputId":"0075f513-ebdf-4b4d-c4e4-04bb2df3e3d8"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["🧪 Testing model creation...\n","🏗️ Creating EfficientNet model (classes=15, pretrained=True)\n","🚀 Creating EfficientNetDetectionModel...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B1_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B1_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/efficientnet_b1_rwightman-bac287d4.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b1_rwightman-bac287d4.pth\n","100%|██████████| 30.1M/30.1M [00:00<00:00, 170MB/s]\n"]},{"output_type":"stream","name":"stdout","text":[">> EfficientNet-B1 backbone initialized\n","\n","=== EfficientNet-B1 Layer Structure ===\n","Layer 0: Conv2dNormActivation -> output shape: torch.Size([1, 32, 320, 256])\n","Layer 1: Sequential -> output shape: torch.Size([1, 16, 320, 256])\n","Layer 2: Sequential -> output shape: torch.Size([1, 24, 160, 128])\n","Layer 3: Sequential -> output shape: torch.Size([1, 40, 80, 64])\n","Layer 4: Sequential -> output shape: torch.Size([1, 80, 40, 32])\n","Layer 5: Sequential -> output shape: torch.Size([1, 112, 40, 32])\n","Layer 6: Sequential -> output shape: torch.Size([1, 192, 20, 16])\n","Layer 7: Sequential -> output shape: torch.Size([1, 320, 20, 16])\n","Layer 8: Conv2dNormActivation -> output shape: torch.Size([1, 1280, 20, 16])\n",">> AdaptiveFPN initialized for channels: [16, 40, 80]\n","🔧 Initializing SafeDetectionHead weights...\n","   Setting objectness bias to -4.0 for 3 anchors\n","✅ Weight initialization completed\n","✅ EfficientNetDetectionModel created successfully\n","📊 Model Statistics:\n","   Total parameters: 9,514,012\n","   Trainable parameters: 9,514,012\n","   vs ResNet50 (~25M): 0.4x\n","📥 Input shape: torch.Size([2, 1, 640, 512])\n",">> Adjusting FPN for actual channels: [16, 40, 112]\n","📤 Output shape: torch.Size([2, 264960, 20])\n","✅ Model test successful!\n","✅ モデル作成成功！\n"]}]},{"cell_type":"code","source":["# 最新チェックポイントから自動再開\n","exec(open('unified_training.py').read())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qtK-nEMjjPxq","outputId":"2cceff80-53ca-47c0-806c-cba8ee91c8d2","executionInfo":{"status":"ok","timestamp":1749199910318,"user_tz":-540,"elapsed":393501,"user":{"displayName":"みは","userId":"00152872231100665214"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["🚀 Starting Unified EfficientNet Training with Comprehensive Debug\n","📱 Device: cuda\n","🎯 Target: 15 classes\n","📏 Input size: (640, 512)\n","ver 1.3-gemini - Corrected for Latest Project Structure\n","🧪 Testing model creation...\n","🏗️ Creating EfficientNet model (classes=15, pretrained=True)\n","🚀 Creating EfficientNetDetectionModel...\n",">> EfficientNet-B1 backbone initialized\n","\n","=== EfficientNet-B1 Layer Structure ===\n","Layer 0: Conv2dNormActivation -> output shape: torch.Size([1, 32, 320, 256])\n","Layer 1: Sequential -> output shape: torch.Size([1, 16, 320, 256])\n","Layer 2: Sequential -> output shape: torch.Size([1, 24, 160, 128])\n","Layer 3: Sequential -> output shape: torch.Size([1, 40, 80, 64])\n","Layer 4: Sequential -> output shape: torch.Size([1, 80, 40, 32])\n","Layer 5: Sequential -> output shape: torch.Size([1, 112, 40, 32])\n","Layer 6: Sequential -> output shape: torch.Size([1, 192, 20, 16])\n","Layer 7: Sequential -> output shape: torch.Size([1, 320, 20, 16])\n","Layer 8: Conv2dNormActivation -> output shape: torch.Size([1, 1280, 20, 16])\n",">> AdaptiveFPN initialized for channels: [16, 40, 80]\n","🔧 Initializing SafeDetectionHead weights...\n","   Setting objectness bias to -4.0 for 3 anchors\n","✅ Weight initialization completed\n","✅ EfficientNetDetectionModel created successfully\n","📊 Model Statistics:\n","   Total parameters: 9,514,012\n","   Trainable parameters: 9,514,012\n","   vs ResNet50 (~25M): 0.4x\n","📥 Input shape: torch.Size([2, 1, 640, 512])\n",">> Adjusting FPN for actual channels: [16, 40, 112]\n","📤 Output shape: torch.Size([2, 264960, 20])\n","✅ Model test successful!\n","✅ モデル作成成功！\n","--- ⚠️【診断モード】AMP（自動混合精度計算）を無効化して実行します ---\n","\n","=== 📚 Dataset Loading ===\n","✅ Dataset: 10742 samples\n","\n","=== 🎯 Anchor Optimization ===\n","📊 Analyzing dataset statistics from 1000 samples...\n","   Progress: 100/1000 (10.0%) - 0.4s\n","   Progress: 200/1000 (20.0%) - 0.8s\n","   Progress: 300/1000 (30.0%) - 1.2s\n","   Progress: 400/1000 (40.0%) - 1.6s\n","   Progress: 500/1000 (50.0%) - 1.9s\n","   Progress: 600/1000 (60.0%) - 2.3s\n","   Progress: 700/1000 (70.0%) - 2.8s\n","   Progress: 800/1000 (80.0%) - 3.2s\n","   Progress: 900/1000 (90.0%) - 3.5s\n","✅ Analysis completed: 15220 valid boxes found in 4.0s\n","\n","📈 Dataset Statistics:\n","   Total boxes: 15220\n","   Avg size: 24.7 x 29.9 pixels\n","   Size distribution: Small=11854, Medium=2824, Large=542\n","\n","🔧 Generating optimized anchors...\n","🔧 Generating 9 anchors using K-means++ (max 300 iterations)...\n","🎉 Generated optimized anchors:\n","   Level 0: [(9, 10), (14, 26), (38, 26)]\n","   Level 1: [(24, 53), (74, 52), (45, 106)]\n","   Level 2: [(128, 86), (98, 219), (203, 144)]\n","✅ Anchor optimization completed in 2.7s\n","✅ Anchor optimization completed\n","\n","=== 📊 Anchor Comparison ===\n","\n","🆚 Comparing anchor sets...\n","\n","--- Anchor Set 1 (Default) ---\n","📏 Evaluating anchor quality with 500 samples...\n","📈 Evaluating anchor performance...\n","   平均IoU: 0.5698 (±0.1806)\n","   50%カバレッジ: 69.82%\n","   70%カバレッジ: 23.26%\n","   90%カバレッジ: 2.63%\n","   未使用アンカー: 0/9\n","\n","--- Anchor Set 2 (Optimized) ---\n","📏 Evaluating anchor quality with 500 samples...\n","📈 Evaluating anchor performance...\n","   平均IoU: 0.5849 (±0.1693)\n","   50%カバレッジ: 70.50%\n","   70%カバレッジ: 25.36%\n","   90%カバレッジ: 2.15%\n","   未使用アンカー: 0/9\n","\n","📊 Improvement Summary:\n","   Mean IoU: +0.0151\n","   50% Coverage: +0.68%\n","   70% Coverage: +2.10%\n","   ⚠️ Limited improvement. Consider different strategies.\n","📦 Batches per epoch: 538\n","📊 Progress will be shown every 10 batches\n","\n","=== 🤖 Model Initialization ===\n","🏗️ Creating EfficientNet model (classes=15, pretrained=True)\n","🚀 Creating EfficientNetDetectionModel...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B1_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B1_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":[">> EfficientNet-B1 backbone initialized\n","\n","=== EfficientNet-B1 Layer Structure ===\n","Layer 0: Conv2dNormActivation -> output shape: torch.Size([1, 32, 320, 256])\n","Layer 1: Sequential -> output shape: torch.Size([1, 16, 320, 256])\n","Layer 2: Sequential -> output shape: torch.Size([1, 24, 160, 128])\n","Layer 3: Sequential -> output shape: torch.Size([1, 40, 80, 64])\n","Layer 4: Sequential -> output shape: torch.Size([1, 80, 40, 32])\n","Layer 5: Sequential -> output shape: torch.Size([1, 112, 40, 32])\n","Layer 6: Sequential -> output shape: torch.Size([1, 192, 20, 16])\n","Layer 7: Sequential -> output shape: torch.Size([1, 320, 20, 16])\n","Layer 8: Conv2dNormActivation -> output shape: torch.Size([1, 1280, 20, 16])\n",">> AdaptiveFPN initialized for channels: [16, 40, 80]\n","🔧 Initializing SafeDetectionHead weights...\n","   Setting objectness bias to -4.0 for 3 anchors\n","✅ Weight initialization completed\n","✅ EfficientNetDetectionModel created successfully\n","📊 Model Statistics:\n","   Total parameters: 9,514,012\n","   Trainable parameters: 9,514,012\n","   vs ResNet50 (~25M): 0.4x\n","\n","=== 🔍 Grid Size Detection ===\n",">> Adjusting FPN for actual channels: [16, 40, 112]\n","📐 Grid sizes: [(256, 320), (64, 80), (32, 40)]\n","\n","=== 🎯 Loss Function Setup ===\n","--- 読み込み中の損失ファイル: unified_loss.py (v1.4 - lossテストバージョン) ---\n","🎯 Creating Enhanced Loss with strategy: balanced\n","🎯 Enhanced Loss initialized:\n","   Box Loss: ciou, Obj Loss: adaptive_focal, Cls Loss: label_smooth\n","   Loss Weights: Box=5.0, Obj=2.0, Cls=1.0\n","\n","=== ⚙️ Optimizer Setup ===\n","\n","🎬 Training Started with Debug System!\n","\n","🎯 Epoch 1/30\n","🧊 Stage: Head+Neck Only\n","\n","🔬 [ROOT CAUSE INVESTIGATION] Step 0, Batch 0\n","   🔍 Model Weights Analysis:\n","     Obj weights: mean=0.000088, std=0.009685\n","     Obj biases: mean=-2.000000, std=0.000000\n","   🔍 Gradient Analysis:\n","     ⚠️ No gradients found (may be due to gradient accumulation)\n","   🔍 Activation Analysis:\n","     Obj logits: mean=-1.993164, min=-2.302734, max=-1.591797\n","   🔍 Loss Component Analysis:\n","     Loss contributions:\n","       Box: 0.387 (0.542279)\n","       Obj: 0.367 (0.513287)\n","       Cls: 0.246 (0.344140)\n","\n","🔬 [COMPREHENSIVE DEBUG] Step 0, Batch 0\n","   Target Summary:\n","     Positive (>0.01): 1132\n","     Positive (>0.1): 946\n","     Mean positive: 0.567893\n","   Prediction Summary:\n","     Sigmoid mean: 0.120056\n","     Sigmoid std: 0.006348\n","   Focal Loss State:\n","     Gamma: 2.0\n","     Alpha: 0.75\n","     Internal step: 0\n","   Loss Values:\n","     Total: 1.399706\n","     Box: 0.135570\n","     Obj: 0.128322\n","     Cls: 0.688280\n","   Manual Check:\n","     Manual focal: 0.000603\n","     Difference: 0.127719\n","--- Step 0 | Loss before backward: 0.349926 ---\n"]},{"output_type":"stream","name":"stderr","text":["\n","PYDEV DEBUGGER WARNING:\n","sys.settrace() should not be used when the debugger is being used.\n","This may cause the debugger to stop working correctly.\n","If this is needed, please check: \n","http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n","to see how to restore the debug tracing back correctly.\n","Call Location:\n","  File \"/usr/lib/python3.11/bdb.py\", line 336, in set_trace\n","    sys.settrace(self.trace_dispatch)\n","\n"]},{"name":"stdout","output_type":"stream","text":["🚨🚨🚨 NaN GRADIENT DETECTED IN ==> neck.lateral_convs.0.weight at step 0\n","🚨🚨🚨 NaN GRADIENT DETECTED IN ==> neck.lateral_convs.0.bias at step 0\n","🚨🚨🚨 NaN GRADIENT DETECTED IN ==> neck.lateral_convs.1.weight at step 0\n","🚨🚨🚨 NaN GRADIENT DETECTED IN ==> neck.lateral_convs.1.bias at step 0\n","🚨🚨🚨 NaN GRADIENT DETECTED IN ==> neck.lateral_convs.2.weight at step 0\n","🚨🚨🚨 NaN GRADIENT DETECTED IN ==> neck.lateral_convs.2.bias at step 0\n","🚨🚨🚨 NaN GRADIENT DETECTED IN ==> neck.output_convs.0.weight at step 0\n","🚨🚨🚨 NaN GRADIENT DETECTED IN ==> neck.output_convs.0.bias at step 0\n","🚨🚨🚨 NaN GRADIENT DETECTED IN ==> neck.output_convs.1.weight at step 0\n","🚨🚨🚨 NaN GRADIENT DETECTED IN ==> neck.output_convs.1.bias at step 0\n","🚨🚨🚨 NaN GRADIENT DETECTED IN ==> neck.output_convs.2.weight at step 0\n","🚨🚨🚨 NaN GRADIENT DETECTED IN ==> neck.output_convs.2.bias at step 0\n","🚨🚨🚨 NaN GRADIENT DETECTED IN ==> head.shared_conv.0.weight at step 0\n","🚨🚨🚨 NaN GRADIENT DETECTED IN ==> head.shared_conv.0.bias at step 0\n","🚨🚨🚨 NaN GRADIENT DETECTED IN ==> head.shared_conv.2.weight at step 0\n","🚨🚨🚨 NaN GRADIENT DETECTED IN ==> head.shared_conv.2.bias at step 0\n","🚨🚨🚨 NaN GRADIENT DETECTED IN ==> head.detection_head.weight at step 0\n","🚨🚨🚨 NaN GRADIENT DETECTED IN ==> head.detection_head.bias at step 0\n","> \u001b[0;32m<string>\u001b[0m(268)\u001b[0;36mcheck_for_nan_gradients\u001b[0;34m()\u001b[0m\n","\n","ipdb> \n","ipdb> \n"]},{"output_type":"stream","name":"stderr","text":["\n","PYDEV DEBUGGER WARNING:\n","sys.settrace() should not be used when the debugger is being used.\n","This may cause the debugger to stop working correctly.\n","If this is needed, please check: \n","http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n","to see how to restore the debug tracing back correctly.\n","Call Location:\n","  File \"/usr/lib/python3.11/bdb.py\", line 361, in set_quit\n","    sys.settrace(None)\n","\n","\n","PYDEV DEBUGGER WARNING:\n","sys.settrace() should not be used when the debugger is being used.\n","This may cause the debugger to stop working correctly.\n","If this is needed, please check: \n","http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n","to see how to restore the debug tracing back correctly.\n","Call Location:\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/debugger.py\", line 1075, in cmdloop\n","    sys.settrace(None)\n","\n"]},{"output_type":"stream","name":"stdout","text":["--KeyboardInterrupt--\n","\n","KeyboardInterrupt: Interrupted by user\n","!!! TRAINING HALTED DUE TO NaN GRADIENT !!!\n","\n","❌ Training failed: cannot unpack non-iterable NoneType object\n"]},{"output_type":"stream","name":"stderr","text":["Traceback (most recent call last):\n","  File \"<string>\", line 1034, in <module>\n","TypeError: cannot unpack non-iterable NoneType object\n"]}]},{"cell_type":"markdown","source":["以下、gemini版"],"metadata":{"id":"IG2BN_oHFfI-"}},{"cell_type":"code","source":["import os\n","import numpy as np\n","\n","def verify_dataset(image_dir, label_dir):\n","    \"\"\"\n","    データセットのラベルファイルに破損がないか（NaN/infが含まれていないか）を\n","    徹底的にチェックする診断スクリプト。\n","    \"\"\"\n","    print(\"--- データセットの完全性検証を開始します ---\")\n","    corrupted_files = []\n","\n","    try:\n","        image_files = sorted([f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png', '.jpeg'))])\n","        total_files = len(image_files)\n","        print(f\"チェック対象の画像ファイル数: {total_files}\")\n","    except FileNotFoundError:\n","        print(f\"❌エラー: 画像ディレクトリが見つかりません: {image_dir}\")\n","        print(\"TRAIN_IMG_DIRのパスを確認してください。\")\n","        return None\n","\n","    for i, img_name in enumerate(image_files):\n","        if i > 0 and i % 1000 == 0:\n","            print(f\"進捗: {i}/{total_files}\")\n","\n","        label_filename = os.path.splitext(img_name)[0] + '.txt'\n","        label_path = os.path.join(label_dir, label_filename)\n","\n","        if os.path.exists(label_path):\n","            try:\n","                with open(label_path, 'r') as f:\n","                    content = f.read()\n","                    if content.strip() == \"\":\n","                        continue  # 空のラベルファイルは正常とみなす\n","\n","                    # ファイル内に 'nan' や 'inf' の文字列がないかチェック\n","                    if \"nan\" in content.lower() or \"inf\" in content.lower():\n","                        print(f\"🚨【破損検知 - 文字列】: {label_filename}\")\n","                        corrupted_files.append(label_filename)\n","                        continue\n","\n","                    # 数値として読み込んでチェック\n","                    lines = content.strip().split('\\n')\n","                    labels = np.array([line.split() for line in lines], dtype=np.float32)\n","\n","                    if np.isnan(labels).any() or np.isinf(labels).any():\n","                        print(f\"🚨【破損検知 - 数値】: {label_filename}\")\n","                        corrupted_files.append(label_filename)\n","\n","            except Exception as e:\n","                print(f\"🚨【パース失敗】: {label_filename}, エラー: {e}\")\n","                corrupted_files.append(label_filename)\n","\n","    print(\"\\n--- 検証完了 ---\")\n","    if corrupted_files:\n","        print(f\"発見された破損ファイルの数: {len(corrupted_files)}\")\n","        # 重複を除いて表示\n","        unique_corrupted = sorted(list(set(corrupted_files)))\n","        print(f\"破損している可能性のあるファイル（{len(unique_corrupted)}件）:\")\n","        for f in unique_corrupted:\n","            print(f\"  - {f}\")\n","    else:\n","        print(\"✅ 破損ファイルは見つかりませんでした。データセットはクリーンです。\")\n","\n","    return corrupted_files\n","\n","if __name__ == '__main__':\n","    # Colab環境のデータセットパス\n","    TRAIN_IMG_DIR = \"/content/FLIR_YOLO_local/images/train\"\n","    TRAIN_LABEL_DIR = \"/content/FLIR_YOLO_local/labels/train\"\n","\n","    print(f\"画像ディレクトリ: {TRAIN_IMG_DIR}\")\n","    print(f\"ラベルディレクトリ: {TRAIN_LABEL_DIR}\")\n","\n","    verify_dataset(TRAIN_IMG_DIR, TRAIN_LABEL_DIR)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xjGPYWpLbMxw","executionInfo":{"status":"ok","timestamp":1749199911090,"user_tz":-540,"elapsed":379,"user":{"displayName":"みは","userId":"00152872231100665214"}},"outputId":"307ffc4a-34cf-46e5-bb8d-4c7c288cbbc7"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["画像ディレクトリ: /content/FLIR_YOLO_local/images/train\n","ラベルディレクトリ: /content/FLIR_YOLO_local/labels/train\n","--- データセットの完全性検証を開始します ---\n","チェック対象の画像ファイル数: 10742\n","進捗: 1000/10742\n","進捗: 2000/10742\n","進捗: 3000/10742\n","進捗: 4000/10742\n","進捗: 5000/10742\n","進捗: 6000/10742\n","進捗: 7000/10742\n","進捗: 8000/10742\n","進捗: 9000/10742\n","進捗: 10000/10742\n","\n","--- 検証完了 ---\n","✅ 破損ファイルは見つかりませんでした。データセットはクリーンです。\n"]}]},{"cell_type":"code","source":["# Google Driveマウント\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# 作業ディレクトリ移動\n","import os\n","os.chdir('/content/drive/MyDrive/IR_obj_detction/gemini')\n","!nvidia-smi\n","\n","# チェックポイント確認\n","import glob\n","checkpoints = glob.glob('saved_models/checkpoint_epoch_*.pth')\n","print(f\"Found checkpoints: {[os.path.basename(f) for f in checkpoints]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KMxtPxkpFaM0","executionInfo":{"status":"ok","timestamp":1749184535584,"user_tz":-540,"elapsed":2221,"user":{"displayName":"みは","userId":"00152872231100665214"}},"outputId":"cdc24a39-5851-4da5-cf0f-b0ed05cff887"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Fri Jun  6 04:35:35 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   71C    P0             30W /   70W |    7012MiB /  15360MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","+-----------------------------------------------------------------------------------------+\n","Found checkpoints: []\n"]}]},{"cell_type":"code","source":["# ローカルに保存用ディレクトリを作成\n","!mkdir -p /content/FLIR_YOLO_local/images/train\n","!mkdir -p /content/FLIR_YOLO_local/labels/train\n","\n","# Google Driveからローカルにデータセットをコピー (時間はかかります)\n","# パスは実際のプロジェクトに合わせてください\n","!echo \"Copying training images...\"\n","!cp -r /content/drive/MyDrive/old/EfficientNet_Project/data/FLIR_YOLO/images/train/* /content/FLIR_YOLO_local/images/train/\n","!echo \"Copying training labels...\"\n","!cp -r /content/drive/MyDrive/old/EfficientNet_Project/data/FLIR_YOLO/labels/train/* /content/FLIR_YOLO_local/labels/train/\n","!echo \"Copy complete!\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ayI7EA9nFcjq","executionInfo":{"status":"ok","timestamp":1749183628688,"user_tz":-540,"elapsed":463587,"user":{"displayName":"みは","userId":"00152872231100665214"}},"outputId":"41c22df5-d883-4caa-af6f-ad940444c694"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Copying training images...\n","Copying training labels...\n","Copy complete!\n"]}]},{"cell_type":"code","source":["# 最新チェックポイントから自動再開\n","exec(open('unified_training.py').read())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cxszOY3VFeXK","executionInfo":{"status":"ok","timestamp":1749184545290,"user_tz":-540,"elapsed":8670,"user":{"displayName":"みは","userId":"00152872231100665214"}},"outputId":"7d277b96-e84d-45a2-9ca1-c62c660f3ef0"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["🚀 Starting Unified EfficientNet Training with Warmup (v2.3)\n","📱 Device: cuda\n","🔥 Warmup enabled for 500 steps.\n","test\n","\n","=== 📚 Dataset Loading ===\n","✅ Dataset: 10742 samples\n","\n","=== 🎯 Anchor Optimization ===\n","📊 Analyzing dataset statistics from 1000 samples...\n","   Progress: 100/1000 (10.0%) - 0.2s\n","   Progress: 200/1000 (20.0%) - 0.4s\n","   Progress: 300/1000 (30.0%) - 0.7s\n","   Progress: 400/1000 (40.0%) - 0.9s\n","   Progress: 500/1000 (50.0%) - 1.1s\n","   Progress: 600/1000 (60.0%) - 1.3s\n","   Progress: 700/1000 (70.0%) - 1.6s\n","   Progress: 800/1000 (80.0%) - 1.8s\n","   Progress: 900/1000 (90.0%) - 2.0s\n","✅ Analysis completed: 15220 valid boxes found in 2.3s\n","\n","📈 Dataset Statistics:\n","   Total boxes: 15220\n","   Avg size: 24.7 x 29.9 pixels\n","   Size distribution: Small=11854, Medium=2824, Large=542\n","\n","🔧 Generating optimized anchors...\n","🔧 Generating 9 anchors using K-means++ (max 300 iterations)...\n","🎉 Generated optimized anchors:\n","   Level 0: [(9, 10), (14, 26), (38, 26)]\n","   Level 1: [(24, 53), (74, 52), (45, 106)]\n","   Level 2: [(128, 86), (98, 219), (203, 144)]\n","✅ Anchor optimization completed in 0.3s\n","✅ Anchor optimization completed\n","\n","🆚 Comparing anchor sets...\n","\n","--- Anchor Set 1 (Default) ---\n","📏 Evaluating anchor quality with 500 samples...\n","📈 Evaluating anchor performance...\n","   平均IoU: 0.5698 (±0.1806)\n","   50%カバレッジ: 69.82%\n","   70%カバレッジ: 23.26%\n","   90%カバレッジ: 2.63%\n","   未使用アンカー: 0/9\n","\n","--- Anchor Set 2 (Optimized) ---\n","📏 Evaluating anchor quality with 500 samples...\n","📈 Evaluating anchor performance...\n","   平均IoU: 0.5849 (±0.1693)\n","   50%カバレッジ: 70.50%\n","   70%カバレッジ: 25.36%\n","   90%カバレッジ: 2.15%\n","   未使用アンカー: 0/9\n","\n","📊 Improvement Summary:\n","   Mean IoU: +0.0151\n","   50% Coverage: +0.68%\n","   70% Coverage: +2.10%\n","   ⚠️ Limited improvement. Consider different strategies.\n","📦 Batches per epoch: 538\n","\n","=== 🤖 Model Initialization ===\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B1_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B1_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":[">> EfficientNet-B1 backbone initialized\n","\n","=== EfficientNet-B1 Layer Structure ===\n","Layer 0: Conv2dNormActivation -> output shape: torch.Size([1, 32, 320, 256])\n","Layer 1: Sequential -> output shape: torch.Size([1, 16, 320, 256])\n","Layer 2: Sequential -> output shape: torch.Size([1, 24, 160, 128])\n","Layer 3: Sequential -> output shape: torch.Size([1, 40, 80, 64])\n","Layer 4: Sequential -> output shape: torch.Size([1, 80, 40, 32])\n","Layer 5: Sequential -> output shape: torch.Size([1, 112, 40, 32])\n","Layer 6: Sequential -> output shape: torch.Size([1, 192, 20, 16])\n","Layer 7: Sequential -> output shape: torch.Size([1, 320, 20, 16])\n","Layer 8: Conv2dNormActivation -> output shape: torch.Size([1, 1280, 20, 16])\n",">> AdaptiveFPN initialized for channels: [16, 40, 80]\n","\n","=== 🔍 Grid Size Detection ===\n",">> Adjusting FPN for actual channels: [16, 40, 112]\n","📐 Grid sizes: [(256, 320), (64, 80), (32, 40)]\n","\n","=== 🎯 Loss Function Setup ===\n","🎯 Creating Enhanced Loss with strategy: balanced\n","🎯 Enhanced Loss initialized:\n","   Box Loss: ciou, Obj Loss: adaptive_focal, Cls Loss: label_smooth\n","   Loss Weights: Box=5.0, Obj=2.0, Cls=1.0\n","\n","=== ⚙️ Optimizer Setup ===\n","\n","🎬 Training Started!\n","\n","🎯 Epoch 1/30 | 🧊 Stage: Head+Neck Only\n","\n","❌ Training failed: name 'cls_loss' is not defined\n"]},{"output_type":"stream","name":"stderr","text":["Traceback (most recent call last):\n","  File \"<string>\", line 812, in <module>\n","  File \"<string>\", line 749, in main\n","  File \"<string>\", line 452, in debug_loss_calculation_comprehensive\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/IR_obj_detction/gemini/unified_loss.py\", line 294, in forward\n","NameError: name 'cls_loss' is not defined\n"]}]}]}