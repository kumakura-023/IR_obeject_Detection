# anchor_optimizer.py - FLIRãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨ã‚¢ãƒ³ã‚«ãƒ¼æœ€é©åŒ–
# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã«é…ç½®

import torch
import numpy as np
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from collections import defaultdict
import os
import pickle


import datetime
import hashlib

class VersionTracker:
    """ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨ä¿®æ­£å±¥æ­´ã‚’è¿½è·¡"""
    
    def __init__(self, script_name, version="1.0.0"):
        self.script_name = script_name
        self.version = version
        self.load_time = datetime.datetime.now()
        self.modifications = []
        
    def add_modification(self, description, author="AI Assistant"):
        """ä¿®æ­£å±¥æ­´ã‚’è¿½åŠ """
        timestamp = datetime.datetime.now()
        self.modifications.append({
            'timestamp': timestamp,
            'description': description,
            'author': author
        })
        
    def get_file_hash(self, filepath):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—ï¼ˆå¤‰æ›´æ¤œå‡ºç”¨ï¼‰"""
        try:
            with open(filepath, 'rb') as f:
                content = f.read()
                return hashlib.md5(content).hexdigest()[:8]
        except:
            return "unknown"
    
    def print_version_info(self):
        """ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ã‚’è¡¨ç¤º"""
        print(f"\n{'='*60}")
        print(f"ğŸ“‹ {self.script_name} - Version {self.version}")
        print(f"â° Loaded: {self.load_time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        if hasattr(self, 'file_hash'):
            print(f"ğŸ”— File Hash: {self.file_hash}")
        
        if self.modifications:
            print(f"ğŸ“ Recent Modifications ({len(self.modifications)}):")
            for i, mod in enumerate(self.modifications[-3:], 1):  # æœ€æ–°3ä»¶
                print(f"   {i}. {mod['timestamp'].strftime('%H:%M:%S')} - {mod['description']}")
        
        print(f"{'='*60}\n")

# å„ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãƒˆãƒ©ãƒƒã‚«ãƒ¼ã‚’ä½œæˆ
def create_version_tracker(script_name, filepath=None):
    """ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãƒˆãƒ©ãƒƒã‚«ãƒ¼ã‚’ä½œæˆ"""
    tracker = VersionTracker(script_name)
    
    if filepath:
        tracker.file_hash = tracker.get_file_hash(filepath)
    
    return tracker

def analyze_flir_dataset_for_anchors(dataset, num_samples=1000, save_results=True):
    """
    FLIRãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒœãƒƒã‚¯ã‚¹åˆ†å¸ƒåˆ†æ
    """
    print(f"ğŸ” FLIRãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ†æé–‹å§‹ ({num_samples}ã‚µãƒ³ãƒ—ãƒ«)...")
    
    box_sizes_pixel = []
    class_distribution = defaultdict(int)
    
    for i in range(min(len(dataset), num_samples)):
        if i % 100 == 0:
            print(f"   é€²æ—: {i}/{num_samples}")
            
        try:
            _, targets = dataset[i]
            if torch.is_tensor(targets) and targets.size(0) > 0:
                targets_np = targets.cpu().numpy()
                
                for target in targets_np:
                    if len(target) >= 4:
                        w_rel, h_rel = target[2], target[3]
                        if w_rel > 0 and h_rel > 0:
                            # FLIR: 640x512
                            w_pixel = w_rel * 640
                            h_pixel = h_rel * 512
                            box_sizes_pixel.append((w_pixel, h_pixel))
                            
                            # ã‚¯ãƒ©ã‚¹åˆ†æ
                            if len(target) > 5:
                                class_scores = target[5:]
                                class_id = np.argmax(class_scores)
                                class_distribution[class_id] += 1
                                
        except Exception as e:
            if i < 5:
                print(f"âš ï¸ Sample {i} ã‚¨ãƒ©ãƒ¼: {e}")
            continue
    
    box_array = np.array(box_sizes_pixel)
    
    # çµ±è¨ˆè¨ˆç®—
    stats = {
        'total_boxes': len(box_sizes_pixel),
        'width_stats': {
            'min': box_array[:, 0].min(),
            'max': box_array[:, 0].max(), 
            'mean': box_array[:, 0].mean(),
            'std': box_array[:, 0].std()
        },
        'height_stats': {
            'min': box_array[:, 1].min(),
            'max': box_array[:, 1].max(),
            'mean': box_array[:, 1].mean(), 
            'std': box_array[:, 1].std()
        },
        'class_distribution': dict(class_distribution)
    }
    
    # é¢ç©ãƒ™ãƒ¼ã‚¹åˆ†é¡
    areas = box_array[:, 0] * box_array[:, 1]
    small_count = np.sum(areas < 32*32)
    medium_count = np.sum((areas >= 32*32) & (areas < 96*96))
    large_count = np.sum(areas >= 96*96)
    
    stats['size_distribution'] = {
        'small': {'count': small_count, 'ratio': small_count/len(areas)},
        'medium': {'count': medium_count, 'ratio': medium_count/len(areas)},
        'large': {'count': large_count, 'ratio': large_count/len(areas)}
    }
    
    print(f"\nğŸ“Š åˆ†æçµæœ:")
    print(f"ç·ãƒœãƒƒã‚¯ã‚¹æ•°: {stats['total_boxes']}")
    print(f"å¹…: {stats['width_stats']['min']:.1f} - {stats['width_stats']['max']:.1f} (å¹³å‡: {stats['width_stats']['mean']:.1f})")
    print(f"é«˜ã•: {stats['height_stats']['min']:.1f} - {stats['height_stats']['max']:.1f} (å¹³å‡: {stats['height_stats']['mean']:.1f})")
    print(f"Small(<32Â²): {small_count} ({small_count/len(areas)*100:.1f}%)")
    print(f"Medium(32Â²-96Â²): {medium_count} ({medium_count/len(areas)*100:.1f}%)")
    print(f"Large(>96Â²): {large_count} ({large_count/len(areas)*100:.1f}%)")
    
    if save_results:
        # çµæœä¿å­˜
        with open('flir_analysis_results.pkl', 'wb') as f:
            pickle.dump({'box_sizes': box_array, 'stats': stats}, f)
        print("âœ… åˆ†æçµæœã‚’ flir_analysis_results.pkl ã«ä¿å­˜")
    
    return box_array, stats

def optimize_anchors_for_flir(box_sizes, num_anchors=9, max_iter=300):
    """
    K-means++ã§ã‚¢ãƒ³ã‚«ãƒ¼æœ€é©åŒ–
    """
    print(f"ğŸ¯ ã‚¢ãƒ³ã‚«ãƒ¼æœ€é©åŒ–é–‹å§‹ (K={num_anchors})...")
    
    if len(box_sizes) < num_anchors:
        print("âš ï¸ ãƒœãƒƒã‚¯ã‚¹æ•°ä¸è¶³ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä½¿ç”¨")
        return get_default_anchors()
    
    # K-meanså®Ÿè¡Œ
    kmeans = KMeans(n_clusters=num_anchors, random_state=42, max_iter=max_iter, n_init=10)
    clusters = kmeans.fit(box_sizes)
    
    anchors_centers = clusters.cluster_centers_
    
    # é¢ç©ã§ã‚½ãƒ¼ãƒˆã—ã¦3ãƒ¬ãƒ™ãƒ«ã«åˆ†å‰²
    areas = anchors_centers[:, 0] * anchors_centers[:, 1]
    sorted_indices = np.argsort(areas)
    sorted_anchors = anchors_centers[sorted_indices]
    
    # 3ã¤ã®FPNãƒ¬ãƒ™ãƒ«ã«åˆ†å‰²
    anchors_per_level = num_anchors // 3
    optimized_anchors = []
    
    for level in range(3):
        start_idx = level * anchors_per_level
        if level == 2:  # æœ€å¾Œã®ãƒ¬ãƒ™ãƒ«ã¯æ®‹ã‚Šå…¨éƒ¨
            end_idx = num_anchors
        else:
            end_idx = (level + 1) * anchors_per_level
            
        level_anchors = []
        for idx in range(start_idx, end_idx):
            if idx < len(sorted_anchors):
                w, h = sorted_anchors[idx]
                level_anchors.append((int(w), int(h)))
        
        # ä¸è¶³åˆ†ã‚’è£œå®Œ
        while len(level_anchors) < anchors_per_level and level_anchors:
            level_anchors.append(level_anchors[-1])
        
        optimized_anchors.append(level_anchors)
    
    print(f"âœ… ã‚¢ãƒ³ã‚«ãƒ¼æœ€é©åŒ–å®Œäº†:")
    for i, level in enumerate(optimized_anchors):
        print(f"  Level {i}: {level}")
    
    return optimized_anchors

def evaluate_anchor_performance(box_sizes, anchors_per_level):
    """
    ã‚¢ãƒ³ã‚«ãƒ¼æ€§èƒ½è©•ä¾¡
    """
    print("ğŸ“ˆ ã‚¢ãƒ³ã‚«ãƒ¼æ€§èƒ½è©•ä¾¡ä¸­...")
    
    all_anchors = []
    for level in anchors_per_level:
        all_anchors.extend(level)
    
    ious = []
    anchor_usage = defaultdict(int)
    
    for box_w, box_h in box_sizes:
        box_area = box_w * box_h
        best_iou = 0
        best_anchor_idx = -1
        
        for anchor_idx, (anchor_w, anchor_h) in enumerate(all_anchors):
            # IoUè¨ˆç®—
            inter_w = min(box_w, anchor_w)
            inter_h = min(box_h, anchor_h)
            intersection = inter_w * inter_h
            
            anchor_area = anchor_w * anchor_h
            union = box_area + anchor_area - intersection
            iou = intersection / (union + 1e-8)
            
            if iou > best_iou:
                best_iou = iou
                best_anchor_idx = anchor_idx
        
        ious.append(best_iou)
        if best_anchor_idx >= 0:
            anchor_usage[best_anchor_idx] += 1
    
    performance = {
        'mean_iou': np.mean(ious),
        'coverage_50': np.mean(np.array(ious) > 0.5),
        'coverage_70': np.mean(np.array(ious) > 0.7),
        'anchor_usage': dict(anchor_usage)
    }
    
    print(f"å¹³å‡IoU: {performance['mean_iou']:.4f}")
    print(f"50%ã‚«ãƒãƒ¬ãƒƒã‚¸: {performance['coverage_50']:.2%}")
    print(f"70%ã‚«ãƒãƒ¬ãƒƒã‚¸: {performance['coverage_70']:.2%}")
    
    return performance

def get_default_anchors():
    """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¢ãƒ³ã‚«ãƒ¼"""
    return [
        [(10,13), (16,30), (33,23)],
        [(30,61), (62,45), (59,119)],
        [(116,90), (156,198), (373,326)]
    ]

def save_optimized_anchors(anchors, filename="optimized_anchors.py"):
    """
    æœ€é©åŒ–ã‚¢ãƒ³ã‚«ãƒ¼ã‚’Pythonãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
    """
    content = f'''# FLIRãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨æœ€é©åŒ–ã‚¢ãƒ³ã‚«ãƒ¼
# è‡ªå‹•ç”Ÿæˆ: anchor_optimizer.py

OPTIMIZED_ANCHORS = {anchors}

def get_flir_anchors():
    """FLIRãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨æœ€é©åŒ–ã‚¢ãƒ³ã‚«ãƒ¼ã‚’å–å¾—"""
    return OPTIMIZED_ANCHORS

if __name__ == "__main__":
    print("FLIRæœ€é©åŒ–ã‚¢ãƒ³ã‚«ãƒ¼:")
    for i, level in enumerate(OPTIMIZED_ANCHORS):
        print(f"Level {{i}}: {{level}}")
'''
    
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(content)
    
    print(f"âœ… æœ€é©åŒ–ã‚¢ãƒ³ã‚«ãƒ¼ã‚’ {filename} ã«ä¿å­˜")

def compare_with_default(box_sizes):
    """
    ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¢ãƒ³ã‚«ãƒ¼ã¨ã®æ¯”è¼ƒ
    """
    print("\nğŸ†š ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¢ãƒ³ã‚«ãƒ¼ã¨ã®æ¯”è¼ƒ:")
    
    default_anchors = get_default_anchors()
    optimized_anchors = optimize_anchors_for_flir(box_sizes)
    
    print("\n--- ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¢ãƒ³ã‚«ãƒ¼æ€§èƒ½ ---")
    default_perf = evaluate_anchor_performance(box_sizes, default_anchors)
    
    print("\n--- æœ€é©åŒ–ã‚¢ãƒ³ã‚«ãƒ¼æ€§èƒ½ ---")
    optimized_perf = evaluate_anchor_performance(box_sizes, optimized_anchors)
    
    print("\nğŸ“Š æ”¹å–„åŠ¹æœ:")
    iou_improvement = optimized_perf['mean_iou'] - default_perf['mean_iou']
    coverage_50_improvement = optimized_perf['coverage_50'] - default_perf['coverage_50']
    coverage_70_improvement = optimized_perf['coverage_70'] - default_perf['coverage_70']
    
    print(f"å¹³å‡IoUæ”¹å–„: {iou_improvement:+.4f}")
    print(f"50%ã‚«ãƒãƒ¬ãƒƒã‚¸æ”¹å–„: {coverage_50_improvement:+.2%}")
    print(f"70%ã‚«ãƒãƒ¬ãƒƒã‚¸æ”¹å–„: {coverage_70_improvement:+.2%}")
    
    if iou_improvement > 0.05:
        print("ğŸ‰ å¤§å¹…æ”¹å–„! Box Losså‘ä¸ŠãŒæœŸå¾…ã§ãã¾ã™")
    elif iou_improvement > 0.02:
        print("âœ… æ”¹å–„åŠ¹æœã‚ã‚Š")
    else:
        print("âš ï¸ æ”¹å–„åŠ¹æœã¯é™å®šçš„")
    
    return optimized_anchors, {'default': default_perf, 'optimized': optimized_perf}

def main():
    """
    ã‚¢ãƒ³ã‚«ãƒ¼æœ€é©åŒ–ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
    """
    print("ğŸš€ FLIRãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨ã‚¢ãƒ³ã‚«ãƒ¼æœ€é©åŒ–")
    print("="*50)
    
    try:
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿
        from dataset import YoloInfraredDataset
        
        dataset = YoloInfraredDataset(
            image_dir="/content/FLIR_YOLO_local/images/train",
            label_dir="/content/FLIR_YOLO_local/labels/train",
            input_size=(640, 512),
            num_classes=15
        )
        
        print(f"ğŸ“ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿: {len(dataset)}æš")
        
        # 1. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ†æ
        box_sizes, stats = analyze_flir_dataset_for_anchors(dataset, num_samples=1000)
        
        if len(box_sizes) == 0:
            print("âŒ æœ‰åŠ¹ãªãƒœãƒƒã‚¯ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        # 2. ã‚¢ãƒ³ã‚«ãƒ¼æœ€é©åŒ–ã¨æ¯”è¼ƒ
        optimized_anchors, comparison = compare_with_default(box_sizes)
        
        # 3. çµæœä¿å­˜
        save_optimized_anchors(optimized_anchors, "optimized_anchors.py")
        
        print(f"\nğŸ¯ æœ€é©åŒ–å®Œäº†!")
        print(f"æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        print(f"1. enhanced_training_warm_scheduler_gemini.py ã§ã‚¢ãƒ³ã‚«ãƒ¼ã‚’ä½¿ç”¨")
        print(f"2. from optimized_anchors import get_flir_anchors")
        print(f"3. optimized_anchors_pixel = get_flir_anchors()")
        
        return optimized_anchors, stats, comparison
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None

if __name__ == "__main__":
    main()